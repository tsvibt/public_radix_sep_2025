


word = Word('boop', 'en')
Word_Parsed(word)


240429    22:09:13    
def old_WordWordDirection_InspectionHTML(word, word2, direction):
   inspection_list = WordWord_Inspectionlists(word, word2)
   result = ''
   inspections = {True:[], False:[]}
   [inspections[inspection[1:] == [word, word2]].append(inspection[0]) for inspection in inspection_list]
   for direction_truth, these_inspections in inspections.items():
      if len(these_inspections)>0:
         pair = [word, word2] if direction_truth else [word2, word]
         pair = [Word_Element(this_word, {'kind':'inspection'}) for this_word in pair]
         result += f'sources saying {pair[0]} points {direction} to {pair[1]}:<br>'
         result += '; '.join(Sense_Element(sense, {'kind':'inspection'}) for sense in these_inspections)
   return result + Paragraph('expanded reasons:') + ''.join(map(Inspection_ElaborationHTML, inspection_list))

def Inspection_ElaborationHTML(inspection):
   source_sense, past_word, future_word = inspection
   results = set()
   for inference in Sense_Inferences(source_sense):
      if any({past_word,future_word} == {from_word,to_word} for from_word,to_word in Reflinks_Wordlinks(inference['reflinks'])):
         header_text = Tokens_Text(inference['header tokens'])
         match_text = Tokens_Text(inference['match tokens'])
         new_match_text = match_text
         for token in inference['match tokens']:
            if token['kind'] == 'reference':
               if any(word in Ref_Words(token['content']) for word in {past_word, future_word}):
                  token_text = token['content']['text']
                  new_match_text = new_match_text.replace(token_text, Color((160,190,255), token_text))
         text = header_text.replace(match_text, Color((255,90,90), new_match_text))
         results.add(''.join(map(Paragraph, [Color(Legend_color, inference['header']), inference['name'], text])))

   inspection_row = list(Inspection_HTMLrow(inspection))
   return Paragraph('<br>') + Paragraph(' '.join(['source:', inspection_row[0], 'conclusion:',] +  inspection_row[1:])) + ''.join(results)

240501    08:41:02    

#def WordRefslinks_Alllinks(from_word, refslinks):
#   return ([(from_word, to_word, direction) for direction in ['pastward', 'futureward'] 
#         for to_word in refslinks['ref'][direction]] + 
#         [(link_from_word, link_to_word, direction) for direction in ['pastward', 'futureward'] 
#         for link_from_word,link_to_word in refslinks['link'][direction]])

#def WordSenserefslinks_Alllinks(from_word, sense_refslinks):
#   return DictFun_Mapflatten(sense_refslinks, lambda _,refslinks: WordRefslinks_Alllinks(from_word, refslinks))

def FUN(sense, future_wordlinks):
   REGISTER_LINKS(sense.word)
   [Register_FutureLink(link) for link in future_wordlinks]
#   [Register_FutureLink(link) for link in WordSenserefslinks_Alllinks(word, sense_refslinks)]
   return []


240501    08:51:46    
#todo: replace with my_dict.setdefault(key, value)
def DictKey_MaybeAdd(d, x, default=False):
   if x not in d.keys():
      d[x] = default


for i in range(2):
      REGISTER_LINKS(link[i])
      LINKS[link[i]][].add(link[1-i])



240501    18:09:14    

GWordtitle_Redirect = {}
if not globals().get(G_loading_to_xmlparse, False):
   if not os.path.exists(Wiktionary_database):
      raise Exception("Wiktionary_database doesn't exist: " + Wiktionary_database)
   GWordtitle_Redirect = Load_Wordtitle_Redirects()


240502    18:20:12    
Global_comment_regex = re.compile("<comment>.*</comment>")


240502    22:03:43    
query = "SELECT name FROM sqlite_master WHERE type='table';"
Global_db_cursor.execute(query)
tables = Global_db_cursor.fetchall()
for table in tables:
    print(table[0])



240502    23:15:14    
def ProcessArticle(text):
   try:
      title_section, body = text.split('</title>')
      title = title_section.split('<title>')[1]
      if not excludeTitle(title) and not '<redirect title="' in body and has_included_lang(body):
         body = body.replace('wplink==', 'wplink=xx').replace('fake====', 'fake=xxx').replace('fake===', 'fake=xx').replace('fake==', 'fake=xx')
         body = '<comment> </comment>'.join([x.split('</comment>')[-1] for x in body.split('<comment>')])
         print(title, end=' ')
         return [(title, body)]
      else:
         return []
   except:
      print('something went wrong with this text:')
      print(text)
      Log('something went wrong with this text:\n')
      Log(text)
      Log('END TEXT')
      return 0


#to_do_1 = {'generator source':GWordtitle_Redirect.items(), 'function':lambda x,y: [(x,y)], 'target tables':[SQL_Wordtitle_Redirect], }

#to_do_2 = {'generator source':[(all_pastmosts, )], 'function':lambda x:[(x, )], 'target tables':[SQL_AllPastmosts], 'message': 'writing '}

#to_do_1 = {'generator source':[(x,) for x in SQL_AllPastmosts.Unstore(SQL_AllPastmosts.RowIterator().fetchone()[0])], 'function':FUN, 'target tables':[SQL_WORD_FuturePoset], }


240506    11:34:43    
   ##### for kern
#   to_profile = False

#@profile_func
#@line_profile

#line_profile.print_stats
#@profile

#from line_profiler import profile as line_profile
#@line_profile


#python -m kernprof -lv --builtin src/precomputing/5_wordpastmosts.py
#python -m line_profiler src/precomputing/5_wordpastmosts.py


#python -m kernprof -lvr --filename src/precomputing/5_wordpastmosts.py
#python -m kernprof -lvr ./src/precomputing/5_wordpastmosts.py
#python kernprof -lvr src/precomputing/5_wordpastmosts.py
#python -m kernprof -lvr myscript.py
#python  src/precomputing/5_wordpastmosts.py
# kernprof -l --globals src/precomputing/5_wordpastmosts.py



G_func_profiler_results = pstats.Stats()

#@profile_func
def profile_func(func):
   @functools.wraps(func)
   def wrapper(*args, **kwargs):
      profiler = cProfile.Profile(subcalls=True)
      profiler.enable()
      result = func(*args, **kwargs)
      profiler.disable()
      G_func_profiler_results.add(profiler)
      return result
   return wrapper

G_func_profile_stats_file = RadixRootdir + 'aux/func_prof_aux.stats'
ensure_file_exists(G_func_profile_stats_file)

   if G_func_profiler_results.stats:
      G_func_profiler_results.dump_stats(G_func_profile_stats_file)
      result += capture_output(lambda: printprofile(G_func_profile_stats_file, num=num, callers=callers))[1]

240506    11:57:58    
?? 40k?
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   118
   119
   120     20253  490603000.0  24223.7      3.9
   121     20253   15812000.0    780.7      0.1
   122     40506   59035000.0   1457.4      0.5
   123     20253 1244584000.0  61451.8      9.8
   124     20253   30415000.0   1501.8      0.2
   125     20253  422461000.0  20859.2      3.3
   126     20253  455568000.0  22493.9      3.6
   127    112841  136114000.0   1206.2      1.1
   128     92588   32086000.0    346.5      0.3
   129     92588  103354000.0   1116.3      0.8
   130    457369  228468000.0    499.5      1.8
   131    364781 4246366000.0  11640.9     33.4
   132    196984  410475000.0   2083.8      3.2
   133    393968 2245666000.0   5700.1     17.6
   134    196984  265048000.0   1345.5      2.1
   135    394776  390376000.0    988.9      3.1
   136    197792  294418000.0   1488.5      2.3
   137    196984 1447275000.0   7347.2     11.4
   138    196984   67719000.0    343.8      0.5
   139     92588   33498000.0    361.8      0.3
   140       331     140000.0    423.0      0.0
   141
   142       331     190000.0    574.0      0.0
   143       331   66535000.0 201012.1      0.5
   144       331     191000.0    577.0      0.0
   145     20253   33731000.0   1665.5      0.3
   146     20253    9419000.0    465.1      0.1

value subuniverse:
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   118
   119
   120    191308        1e+10  53475.7      3.8
   121    191308  288790000.0   1509.6      0.1
   122    382616 1004294000.0   2624.8      0.4
   123    191308        3e+10 131557.7      9.4
   124    191308  596173000.0   3116.3      0.2
   125    191308 8786974000.0  45931.0      3.3
   126    191308 9383090000.0  49047.0      3.5
   127   1076157 2528935000.0   2350.0      0.9
   128    884849  584242000.0    660.3      0.2
   129    884849 1984942000.0   2243.3      0.7
   130   4866303 4669545000.0    959.6      1.8
   131   3981454        9e+10  21897.8     32.7
   132   2160407 8561881000.0   3963.1      3.2
   133   4320814        5e+10  10835.0     17.6
   134   2160407 5537708000.0   2563.3      2.1
   135   4335390 8223393000.0   1896.8      3.1
   136   2174983 6022874000.0   2769.2      2.3
   137   2160407        3e+10  14038.8     11.4
   138   2160407 1411196000.0    653.2      0.5
   139    884849  602558000.0    681.0      0.2
   140      4204    3722000.0    885.3      0.0
   141
   142      4204    4470000.0   1063.3      0.0
   143      4204 5635393000.0    1e+06      2.1
   144      4204    4807000.0   1143.4      0.0
   145    191308  624115000.0   3262.4      0.2
   146    191308  159715000.0    834.9      0.1




sing subuniverse:
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   118
   119
   120     20253  525495000.0  25946.5      3.9
   121     20253   16699000.0    824.5      0.1
   122     40506   56143000.0   1386.0      0.4
   123     20253 1320024000.0  65176.7      9.8
   124     20253   31239000.0   1542.4      0.2
   125     20253  457345000.0  22581.6      3.4
   126     20253  484257000.0  23910.4      3.6
   127    115561  138475000.0   1198.3      1.0
   128     95308   34186000.0    358.7      0.3
   129     95308  110541000.0   1159.8      0.8
   130    460435  245952000.0    534.2      1.8
   131    365127 4514020000.0  12362.9     33.4
   132    196984  433811000.0   2202.3      3.2
   133    393968 2393130000.0   6074.4     17.7
   134    196984  282102000.0   1432.1      2.1
   135    394776  413779000.0   1048.1      3.1
   136    197792  303703000.0   1535.5      2.2
   137    196984 1550562000.0   7871.5     11.5
   138    196984   71139000.0    361.1      0.5
   139     95308   35402000.0    371.4      0.3
   140       331     140000.0    423.0      0.0
   141
   142       331     185000.0    558.9      0.0
   143       331   67927000.0 205217.5      0.5
   144       331     172000.0    519.6      0.0
   145     20253   33747000.0   1666.3      0.2
   146     20253    9504000.0    469.3      0.1

sing
   118
   119
   120     20253  541213000.0  26722.6      3.9
   121     20253   17806000.0    879.2      0.1
   122     40506   65269000.0   1611.3      0.5
   123     20253 1319981000.0  65174.6      9.5
   124     20253   36850000.0   1819.5      0.3
   125     20253  456652000.0  22547.4      3.3
   126     20253  492134000.0  24299.3      3.5
   127    112052  151524000.0   1352.3      1.1
   128     91799   34814000.0    379.2      0.3
   129     91799  110904000.0   1208.1      0.8
   130    457473  252677000.0    552.3      1.8
   131    365674 2297014000.0   6281.6     16.5
   132    365674 2438523000.0   6668.6     17.5
   133    196984  440347000.0   2235.4      3.2
   134    393968 2422936000.0   6150.1     17.4
   135    196984  280458000.0   1423.8      2.0
   136    394776  421749000.0   1068.3      3.0
   137    197792  310570000.0   1570.2      2.2
   138    196984 1598217000.0   8113.4     11.5
   139    196984   72474000.0    367.9      0.5
   140     91799   37458000.0    408.0      0.3
   141       331     127000.0    383.7      0.0
   142
   143       331     160000.0    483.4      0.0
   144       331   63381000.0 191483.4      0.5
   145       331     250000.0    755.3      0.0
   146     20253   37938000.0   1873.2      0.3
   147     20253    9445000.0    466.4      0.1

   sing

@line_profile
def ItemsetItems_Shortunionfold(itemset, items):
   if len(items) == 1:
      return itemset[list(items)[0]]
   else: 
      return unionfold(itemset[item] for item in items)

#NONSTRICT
@line_profile
def UlteriorsPriors_NonstrictTransitiveClosure_Equivalents(Item_Givenulteriors, Item_Givenpriors):
   Item_Equivalents = {item:{item} for item in Item_Givenulteriors.keys()}
   cycle_bound = 1
   for i in range(1,cycle_bound+1):
      UlteriorsEquivalentsK_MarkKcycles(Item_Givenulteriors, Item_Equivalents, i)
   mprint("taking transitive closure")
   remaining_domain, upcoming_candidates = set(Item_Givenulteriors.keys()), set(Item_Givenulteriors.keys())
   Item_AllNonstrictUlteriors = {item:{item} for item in Item_Givenulteriors.keys()}
   while len(remaining_domain)>0:
      madeprogress = False
      current_candidates, upcoming_candidates = upcoming_candidates & remaining_domain, set()
      for x in current_candidates:
#         equivalents_ulteriors = (Item_Givenulteriors[x] if len(Item_Equivalents[x])==1 else 
#                              unionfold(Item_Givenulteriors[equivalent] for equivalent in Item_Equivalents[x]))
         equivalents_ulteriors = ItemsetItems_Shortunionfold(Item_Givenulteriors, Item_Equivalents[x])
         if not any(y in remaining_domain and y not in Item_Equivalents[x] for y in equivalents_ulteriors): 
            remaining_domain.difference_update(Item_Equivalents[x])
            ALL_ULTERIORS = unionfold(Item_AllNonstrictUlteriors[y] 
                      for equivalent in Item_Equivalents[x] for y in ({equivalent}|Item_Givenulteriors[equivalent]))
            for equivalent in Item_Equivalents[x]:
               Item_AllNonstrictUlteriors[equivalent] = ALL_ULTERIORS
            upcoming_candidates.update(unionfold(Item_Givenpriors[equivalent] for equivalent in Item_Equivalents[x]) - Item_Equivalents[x])
            madeprogress = True
      if not madeprogress:
         if cycle_bound>50:
            raise Exception('cycle bound exceeded 50')
         cycle_bound += 1
         UlteriorsEquivalentsK_MarkKcycles(Item_Givenulteriors, Item_Equivalents, cycle_bound)
         upcoming_candidates = remaining_domain
   mprint("done taking transitive closure")
   return Item_AllNonstrictUlteriors, Item_Equivalents


Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   118
   119
   120     20253  531472000.0  26241.6      4.1
   121     20253   17568000.0    867.4      0.1
   122     40506   67791000.0   1673.6      0.5
   123     20253 1338993000.0  66113.3     10.4
   124     20253   35016000.0   1728.9      0.3
   125     20253  454953000.0  22463.5      3.5
   126     20253  493319000.0  24357.8      3.8
   127    114645  147035000.0   1282.5      1.1
   128     94392   36167000.0    383.2      0.3
   129     94392  114742000.0   1215.6      0.9
   130    461494  254364000.0    551.2      2.0
   131    367820 1232710000.0   3351.4      9.5
   132       718    6482000.0   9027.9      0.1
   133    367102 2327523000.0   6340.3     18.0
   134    196984  451607000.0   2292.6      3.5
   135    393968 2532599000.0   6428.4     19.6
   136    196984  282794000.0   1435.6      2.2
   137    394776  427584000.0   1083.1      3.3
   138    197792  311846000.0   1576.6      2.4
   139    196984 1632850000.0   8289.3     12.6
   140    196984   72884000.0    370.0      0.6
   141     94392   37520000.0    397.5      0.3
   142       331     137000.0    413.9      0.0
   143
   144       331     173000.0    522.7      0.0
   145       331   66436000.0 200713.0      0.5
   146       331     197000.0    595.2      0.0
   147     20253   39813000.0   1965.8      0.3
   148     20253    9784000.0    483.1      0.1

sing
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   121
   122
   123     20253  555669000.0  27436.4      3.9
   124     20253   18211000.0    899.2      0.1
   125     40506   66965000.0   1653.2      0.5
   126     20253 1408878000.0  69563.9      9.8
   127     20253   34314000.0   1694.3      0.2
   128     20253  470904000.0  23251.1      3.3
   129     20253  511802000.0  25270.4      3.6
   130    121305  159702000.0   1316.5      1.1
   131    101052   37714000.0    373.2      0.3
   132    101052  127091000.0   1257.7      0.9
   133    483962  279280000.0    577.1      2.0
   134
   135
   136    382910 1841577000.0   4809.4     12.9
   137    382910 2687821000.0   7019.5     18.8
   138    196984  462246000.0   2346.6      3.2
   139    393968 2665402000.0   6765.5     18.6
   140    196984  290515000.0   1474.8      2.0
   141    394776  443592000.0   1123.7      3.1
   142    197792  319078000.0   1613.2      2.2
   143    196984 1694955000.0   8604.5     11.8
   144    196984   74125000.0    376.3      0.5
   145    101052   41179000.0    407.5      0.3
   146       331     184000.0    555.9      0.0
   147
   148       331     197000.0    595.2      0.0
   149       331   76151000.0 230063.4      0.5
   150       331     200000.0    604.2      0.0
   151     20253   36952000.0   1824.5      0.3
   152     20253    9689000.0    478.4      0.1

sing

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   117
   118
   119    362058  281412000.0    777.3     29.0
   120    361340  682650000.0   1889.2     70.4
   121
   122       718    5810000.0   8091.9      0.6

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   125
   126
   127     20253  486556000.0  24023.9      3.5
   128     20253   16704000.0    824.8      0.1
   129     40506   57270000.0   1413.9      0.4
   130     20253 1222032000.0  60338.3      8.9
   131     20253   31861000.0   1573.1      0.2
   132     20253  408026000.0  20146.4      3.0
   133     20253  442382000.0  21842.8      3.2
   134    112305  125204000.0   1114.9      0.9
   135     92052   29984000.0    325.7      0.2
   136     92052  101627000.0   1104.0      0.7
   137    454110  225786000.0    497.2      1.6
   138
   139
   140    362058 3201551000.0   8842.6     23.3
   141    362058 2167220000.0   5985.8     15.8
   142    196984  407491000.0   2068.7      3.0
   143    393968 2251988000.0   5716.2     16.4
   144    196984  253941000.0   1289.1      1.8
   145    394776  378026000.0    957.6      2.7
   146    197792  279580000.0   1413.5      2.0
   147    196984 1472938000.0   7477.4     10.7
   148    196984   58652000.0    297.8      0.4
   149     92052   33162000.0    360.3      0.2
   150       331     127000.0    383.7      0.0
   151
   152       331     144000.0    435.0      0.0
   153       331   63761000.0 192631.4      0.5
   154       331     158000.0    477.3      0.0
   155     20253   30870000.0   1524.2      0.2
   156     20253    9161000.0    452.3      0.1
   sing
with inline
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   118
   119
   120     20253  526544000.0  25998.3      4.2
   121     20253   16750000.0    827.0      0.1
   122     40506   56787000.0   1401.9      0.5
   123     20253 1288489000.0  63619.7     10.3
   124     20253   32048000.0   1582.4      0.3
   125     20253  448420000.0  22140.9      3.6
   126     20253  488316000.0  24110.8      3.9
   127    119895  135789000.0   1132.6      1.1
   128     99642   34913000.0    350.4      0.3
   129     99642  112800000.0   1132.1      0.9
   130    480874  245565000.0    510.7      2.0
   131    381950 1218463000.0   3190.1      9.7
   132       718    5800000.0   8078.0      0.0
   133    381232 2331425000.0   6115.5     18.6
   134    196984  424168000.0   2153.3      3.4
   135    393968 2420073000.0   6142.8     19.3
   136    196984  274596000.0   1394.0      2.2
   137    394776  401022000.0   1015.8      3.2
   138    197792  297284000.0   1503.0      2.4
   139    196984 1550340000.0   7870.4     12.4
   140    196984   62283000.0    316.2      0.5
   141     99642   38363000.0    385.0      0.3
   142       331     145000.0    438.1      0.0
   143
   144       331     169000.0    510.6      0.0
   145       331   61678000.0 186338.4      0.5
   146       331     196000.0    592.1      0.0
   147     20253   34327000.0   1694.9      0.3
   148     20253    8636000.0    426.4      0.1

sing
   Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   130
   131
   132     20253  581181000.0  28696.0      4.1
   133     20253   18746000.0    925.6      0.1
   134     40506   71399000.0   1762.7      0.5
   135     20253 1444973000.0  71346.1     10.2
   136     20253   37564000.0   1854.7      0.3
   137     20253  492578000.0  24321.2      3.5
   138     20253  527428000.0  26042.0      3.7
   139    113110  156535000.0   1383.9      1.1
   140     92857   34539000.0    372.0      0.2
   141     92857  121646000.0   1310.0      0.9
   142    455674  266005000.0    583.8      1.9
   143
   144
   145    362817 1601481000.0   4414.0     11.4
   146    362817 2481554000.0   6839.7     17.6
   147    196984  486800000.0   2471.3      3.5
   148    393968 2704305000.0   6864.3     19.2
   149    196984  305665000.0   1551.7      2.2
   150    394776  456469000.0   1156.3      3.2
   151    197792  334378000.0   1690.6      2.4
   152    196984 1734121000.0   8803.4     12.3
   153    196984   81367000.0    413.1      0.6
   154     92857   38722000.0    417.0      0.3
   155       331     167000.0    504.5      0.0
   156
   157       331     199000.0    601.2      0.0
   158       331   77866000.0 235244.7      0.6
   159       331     252000.0    761.3      0.0
   160     20253   39494000.0   1950.0      0.3
   161     20253   10402000.0    513.6      0.1

240506    14:55:40    
# assumes item in item_keys
#@line_profile
def ItemsetItemsetItem_Shortunionfold(item_set, item_keys, item):
   return item_set[item] if len(item_keys[item])==1 else unionfold(item_set[z] for z in item_keys[item])
#   if len(items) == 1:
#      return itemset[list(items)[0]]
#   else: 
#      return unionfold(itemset[item] for item in items)

#         equivalents_ulteriors = (Item_Givenulteriors[x] if len(Item_Equivalents[x])==1 else 
#                              unionfold(Item_Givenulteriors[equivalent] for equivalent in Item_Equivalents[x]))
         equivalents_ulteriors = ItemsetItemsetItem_Shortunionfold(Item_Givenulteriors, Item_Equivalents, x)



#            upcoming_candidates.update(unionfold(Item_Givenpriors[equivalent] for equivalent in Item_Equivalents[x]) - Item_Equivalents[x])

sing

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   126
   127
   128     20253  513429000.0  25350.8      4.3
   129     20253   15529000.0    766.8      0.1
   130     40506   59670000.0   1473.1      0.5
   131     20253 1267076000.0  62562.4     10.6
   132     20253   30659000.0   1513.8      0.3
   133     20253  437232000.0  21588.5      3.7
   134     20253  472555000.0  23332.6      4.0
   135    112237  126587000.0   1127.9      1.1
   136     91984   31519000.0    342.7      0.3
   137     91984  103176000.0   1121.7      0.9
   138    456579  233217000.0    510.8      2.0
   139    364595 1372991000.0   3765.8     11.5
   140    364595 2202410000.0   6040.7     18.4
   141    196984  420172000.0   2133.0      3.5
   142    393968 2378414000.0   6037.1     19.9
   143    196984  268407000.0   1362.6      2.2
   144    394776  405853000.0   1028.1      3.4
   145    197792  295754000.0   1495.3      2.5
   146
   147
   148    196984 1114143000.0   5656.0      9.3
   149
   150    196984   57862000.0    293.7      0.5
   151     91984   32645000.0    354.9      0.3
   152       331     148000.0    447.1      0.0
   153
   154       331     136000.0    410.9      0.0
   155       331   69008000.0 208483.4      0.6
   156       331     297000.0    897.3      0.0
   157     20253   33054000.0   1632.1      0.3
   158     20253    8422000.0    415.8      0.1

#            upcoming_candidates.update(unionfold(Item_Givenpriors[equivalent] for equivalent in Item_Equivalents[x]) - Item_Equivalents[x])
            upcoming_candidates.update(ItemsetItemsetItem_Shortunionfold(Item_Givenpriors, Item_Equivalents, x) - Item_Equivalents[x])



Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   126
   127
   128     20253  635505000.0  31378.3      4.2
   129     20253   20000000.0    987.5      0.1
   130     40506   80202000.0   1980.0      0.5
   131     20253 1537458000.0  75912.6     10.2
   132     20253   45322000.0   2237.8      0.3
   133     20253  538976000.0  26612.2      3.6
   134     20253  574346000.0  28358.6      3.8
   135    116446  173192000.0   1487.3      1.1
   136     96193   38576000.0    401.0      0.3
   137     96193  142364000.0   1480.0      0.9
   138    465797  299893000.0    643.8      2.0
   139    369604 1723811000.0   4663.9     11.4
   140    369604 2787158000.0   7540.9     18.5
   141    196984  512508000.0   2601.8      3.4
   142    393968 2984792000.0   7576.2     19.8
   143    196984  330566000.0   1678.1      2.2
   144    394776  509684000.0   1291.1      3.4
   145    197792  364997000.0   1845.4      2.4
   146
   147    196984 1064622000.0   5404.6      7.1
   148    196984  467845000.0   2375.0      3.1
   149    196984   71724000.0    364.1      0.5
   150     96193   40503000.0    421.1      0.3
   151       331     171000.0    516.6      0.0
   152
   153       331     272000.0    821.8      0.0
   154       331   87772000.0 265172.2      0.6
   155       331     271000.0    818.7      0.0
   156     20253   45118000.0   2227.7      0.3
   157     20253   10958000.0    541.1      0.1

sing

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   126
   127
   128     20253  542653000.0  26793.7      4.8
   129     20253   15780000.0    779.1      0.1
   130
   131
   132     20253   33462000.0   1652.2      0.3
   133     20253  456553000.0  22542.5      4.0
   134     20253  486205000.0  24006.6      4.3
   135    114184  136447000.0   1195.0      1.2
   136     93931   33532000.0    357.0      0.3
   137     93931  109733000.0   1168.2      1.0
   138    461085  244149000.0    529.5      2.1
   139    367154 1485134000.0   4045.0     13.0
   140    367154 2359282000.0   6425.9     20.7
   141    196984  430990000.0   2187.9      3.8
   142    393968 2521111000.0   6399.3     22.1
   143    196984  286264000.0   1453.2      2.5
   144    394776  429786000.0   1088.7      3.8
   145    197792  315431000.0   1594.8      2.8
   146
   147    196984  917362000.0   4657.0      8.0
   148    196984  398512000.0   2023.1      3.5
   149    196984   63561000.0    322.7      0.6
   150     93931   34854000.0    371.1      0.3
   151       331     150000.0    453.2      0.0
   152
   153       331     171000.0    516.6      0.0
   154       331   70238000.0 212199.4      0.6
   155       331     165000.0    498.5      0.0
   156     20253   36021000.0   1778.6      0.3
   157     20253    9380000.0    463.1      0.1

sing
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   126
   127
   128     20253  527753000.0  26058.0      4.8
   129     20253   16943000.0    836.6      0.2
   130
   131
   132     20253   36396000.0   1797.1      0.3
   133     20253  447539000.0  22097.4      4.1
   134     20253  473353000.0  23372.0      4.3
   135    108038  131486000.0   1217.0      1.2
   136     87785   30360000.0    345.8      0.3
   137     87785  107663000.0   1226.4      1.0
   138    433669  236295000.0    544.9      2.1
   139    345884 1371837000.0   3966.2     12.4
   140    345884 2160115000.0   6245.2     19.6
   141    196984  433312000.0   2199.7      3.9
   142    393968 2507198000.0   6364.0     22.7
   143    196984  282553000.0   1434.4      2.6
   144    394776  423586000.0   1073.0      3.8
   145    197792  309448000.0   1564.5      2.8
   146
   147    196984  906770000.0   4603.3      8.2
   148    196984  406081000.0   2061.5      3.7
   149    196984   66340000.0    336.8      0.6
   150     87785   32340000.0    368.4      0.3
   151       331     152000.0    459.2      0.0
   152
   153       331     200000.0    604.2      0.0
   154       331   69187000.0 209024.2      0.6
   155       331     245000.0    740.2      0.0
   156     20253   35195000.0   1737.8      0.3
   157     20253    9608000.0    474.4      0.1

   sing
   Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   126
   127
   128     20253  561350000.0  27716.9      4.8
   129     20253   18289000.0    903.0      0.2
   130
   131
   132     20253   36132000.0   1784.0      0.3
   133     20253  471542000.0  23282.6      4.1
   134     20253  510320000.0  25197.3      4.4
   135    116257  142716000.0   1227.6      1.2
   136     96004   36381000.0    379.0      0.3
   137     96004  124158000.0   1293.3      1.1
   138    468884  266801000.0    569.0      2.3
   139
   140    373598 1264558000.0   3384.8     10.9
   141       718    7194000.0  10019.5      0.1
   142
   143    372880 2434878000.0   6529.9     21.0
   144    196984  454124000.0   2305.4      3.9
   145    393968 2604968000.0   6612.1     22.4
   146    196984  298940000.0   1517.6      2.6
   147    394776  442331000.0   1120.5      3.8
   148    197792  321475000.0   1625.3      2.8
   149
   150    196984  972972000.0   4939.3      8.4
   151    196984  413651000.0   2099.9      3.6
   152    196984   66521000.0    337.7      0.6
   153     96004   38074000.0    396.6      0.3
   154       331     198000.0    598.2      0.0
   155
   156       331     195000.0    589.1      0.0
   157       331   76888000.0 232290.0      0.7
   158       331     202000.0    610.3      0.0
   159     20253   38832000.0   1917.3      0.3
   160     20253    9847000.0    486.2      0.1

   sing
   Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   126
   127
   128     20253  532690000.0  26301.8      4.8
   129     20253   16987000.0    838.7      0.2
   130
   131
   132     20253   36006000.0   1777.8      0.3
   133     20253  450375000.0  22237.4      4.1
   134     20253  481479000.0  23773.2      4.3
   135    113623  138348000.0   1217.6      1.2
   136     93370   34892000.0    373.7      0.3
   137     93370  115288000.0   1234.7      1.0
   138    455665  251663000.0    552.3      2.3
   139
   140    363013 1183555000.0   3260.4     10.7
   141       718    8519000.0  11864.9      0.1
   142
   143    362295 2343555000.0   6468.6     21.2
   144    196984  421739000.0   2141.0      3.8
   145    393968 2478495000.0   6291.1     22.4
   146    196984  278357000.0   1413.1      2.5
   147    394776  417145000.0   1056.7      3.8
   148    197792  309010000.0   1562.3      2.8
   149
   150    196984  949298000.0   4819.2      8.6
   151    196984  392325000.0   1991.7      3.5
   152    196984   64195000.0    325.9      0.6
   153     93370   36056000.0    386.2      0.3
   154       331     209000.0    631.4      0.0
   155
   156       331     221000.0    667.7      0.0
   157       331   87591000.0 264625.4      0.8
   158       331     298000.0    900.3      0.0
   159     20253   36827000.0   1818.3      0.3
   160     20253    9453000.0    466.7      0.1


#
sing
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   127
   128
   129     20253   28912000.0   1427.5      0.3
   130
   131
   132     20253  540554000.0  26690.1      4.8
   133     20253    8164000.0    403.1      0.1
   134
   135
   136     20253   34917000.0   1724.0      0.3
   137     20253  451301000.0  22283.2      4.0
   138     20253  483699000.0  23882.8      4.3
   139    115178  141084000.0   1224.9      1.2
   140     94925   33717000.0    355.2      0.3
   141     94925  118180000.0   1245.0      1.0
   142    466014  260833000.0    559.7      2.3
   143
   144    371807 1235699000.0   3323.5     10.9
   145       718    7220000.0  10055.7      0.1
   146
   147    371089 2435228000.0   6562.4     21.4
   148    196984  437091000.0   2218.9      3.8
   149    393968 2566699000.0   6515.0     22.6
   150    196984  285377000.0   1448.7      2.5
   151    394776  427644000.0   1083.3      3.8
   152    197792  313891000.0   1587.0      2.8
   153
   154    196984  940077000.0   4772.4      8.3
   155    196984  400722000.0   2034.3      3.5
   156    196984   68031000.0    345.4      0.6
   157     94925   35847000.0    377.6      0.3
   158       331     199000.0    601.2      0.0
   159
   160       331     215000.0    649.5      0.0
   161       331   76787000.0 231984.9      0.7
   162       331     217000.0    655.6      0.0
   163     20253   38097000.0   1881.1      0.3
   164     20253    9659000.0    476.9      0.1




apparently manaully inlining is a bit faster than just having the function; using the  inliner; or putting the functoino def inside the UlteriorsPriors... function
#from inliner import inline
## assumes item in item_keys
##@line_profile
#@inline

   def ItemsetItemsetItem_Shortunionfold(item_set, item_keys, item):
      return item_set[item] if len(item_keys[item])==1 else unionfold(item_set[z] for z in item_keys[item])


#         equivalents_ulteriors = ItemsetItemsetItem_Shortunionfold(Item_Givenulteriors, Item_Equivalents, x)
         equivalents_ulteriors = (Item_Givenulteriors[x] if len(Item_Equivalents[x])==1 else 
                              unionfold(Item_Givenulteriors[equivalent] for equivalent in Item_Equivalents[x]))


sing

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   122
   123
   124
   125
   126     20253  498316000.0  24604.6      5.0
   127     20253   15544000.0    767.5      0.2
   128
   129
   130     20253   30313000.0   1496.7      0.3
   131     20253  422773000.0  20874.6      4.2
   132     20253  445474000.0  21995.5      4.4
   133    116046  124017000.0   1068.7      1.2
   134     95793   32847000.0    342.9      0.3
   135     95793  106965000.0   1116.6      1.1
   136    467482  229511000.0    491.0      2.3
   137    372407 1098497000.0   2949.7     10.9
   138       718    5923000.0   8249.3      0.1
   139
   140    371689 2217214000.0   5965.2     22.1
   141    196984  385208000.0   1955.5      3.8
   142    393968 2257990000.0   5731.4     22.5
   143    196984  256307000.0   1301.2      2.6
   144    394776  378502000.0    958.8      3.8
   145    197792  279368000.0   1412.4      2.8
   146
   147    197648  692424000.0   3503.3      6.9
   148       664    5149000.0   7754.5      0.1
   149    196984  357234000.0   1813.5      3.6
   150    196984   60493000.0    307.1      0.6
   151     95793   33330000.0    347.9      0.3
   152       331     144000.0    435.0      0.0
   153
   154       331     177000.0    534.7      0.0
   155       331   65266000.0 197178.2      0.7
   156       331     177000.0    534.7      0.0
   157     20253   31262000.0   1543.6      0.3
   158     20253    8345000.0    412.0      0.1



#def ItemsetItemsetItem_Shortunionfold(itemset, items):
#   return itemset[list(items)[0]] if len(items) == 1 else unionfold(itemset[item] for item in items)

#   for i in range(1,cycle_bound+1):
#      UlteriorsEquivalentsK_MarkKcycles(Item_Givenulteriors, Item_Equivalents, i)

         equivalents_ulteriors = unionfold(Item_Givenulteriors[equivalent] for equivalent in Item_Equivalents[x])

         
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   118
   119
   120     20253  482216000.0  23809.6      4.9
   121     20253   15152000.0    748.1      0.2
   122     20253   32150000.0   1587.4      0.3
   123     20253  402969000.0  19896.8      4.1
   124     20253  429763000.0  21219.7      4.4
   125    113286  118148000.0   1042.9      1.2
   126     93033   30301000.0    325.7      0.3
   127     93033  100272000.0   1077.8      1.0
   128    464904  233884000.0    503.1      2.4
   129    372589 1073793000.0   2882.0     10.9
   130       718    6551000.0   9124.0      0.1
   131    371871 2161150000.0   5811.6     21.9
   132    196984  380527000.0   1931.8      3.9
   133    393968 2265962000.0   5751.6     23.0
   134    196984  250028000.0   1269.3      2.5
   135    394776  376501000.0    953.7      3.8
   136    197792  275969000.0   1395.2      2.8
   137    197648  673634000.0   3408.3      6.8
   138       664    5230000.0   7876.5      0.1
   139    196984  350707000.0   1780.4      3.6
   140    196984   60429000.0    306.8      0.6
   141     93033   31998000.0    343.9      0.3
   142       331     144000.0    435.0      0.0
   143
   144       331     170000.0    513.6      0.0
   145       331   64352000.0 194416.9      0.7
   146       331     243000.0    734.1      0.0
   147     20253   32588000.0   1609.0      0.3
   148     20253    8972000.0    443.0      0.1



240506    20:55:39    
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   193
   194
   195     20253  278507000.0  13751.4      1.2
   196     20253  435831000.0  21519.3      1.9
   197     20253 1543072000.0  76189.8      6.8
   198     20253  303298000.0  14975.5      1.3
   199    217227 1166710000.0   5370.9      5.1
   200    196974 3974739000.0  20179.0     17.5
   201    196974  634145000.0   3219.4      2.8
   202    399398 1787200000.0   4474.7      7.9
   203    202424 6415777000.0  31694.7     28.2
   204    176721 1200796000.0   6794.9      5.3
   205    176721 4991215000.0  28243.5     21.9
   206     20253   17553000.0    866.7      0.1


240507    15:58:30    

#### copied code

'''

WITH PREVIOUS THINGY. ordinary word_pastward poset
../log/precompute_LOG24.05.06-18_09_05.txt


ok i think what we wrant is...
in for new_word in Word_Directionwardset[word_exploring]:....
we take Word_Directionwardset[word_exploring]
first of all, we filter away guys who are already in Liveword_Explored; they have already been inserted + dealt with.
second, we find all guys who have already been pastmosted. 
then i think we sort them by future to past .  (or maybe this doesn't matter??)
then in order we check if they've *now* been done (because a previous one might cover a nxet one ) 
   if not, then we get their all_pastwards (somehow). 
      for each of those, we ... 
         put it in the liveword_expli thing. 
         if not in Word_directionwoard set, 
            we get its *immeadiate* pastmosts (global ram cache?), and put  that in directwoard set.
some other stuff i forget...



'''


240508    04:31:09    

#@line_profile
def special_update(Liveword_Explored, word_exploring, Word_Directionwardset, direction):
#   mprint('exploring word', word_exploring)
#      Word_Directionwardset[word_exploring] = WordDirection_Set(word_exploring, direction)
   if word_exploring in RAM_word_immediate_pastward:
      Word_Directionwardset[word_exploring] = RAM_word_immediate_pastward[word_exploring] 
   else:
      Word_Directionwardset[word_exploring] = WordDirection_Set(word_exploring, direction)
   for new_word in Word_Directionwardset[word_exploring]:
      Liveword_Explored.setdefault(new_word, False)

def special_WordsDirection_UlteriorclosureGivens(words, direction):
   mprint('\nclosing set of words', direction, 'from', words)

   Word_Directionwardset = {}
   update = lambda Liveword_Explored, word_exploring: special_update(Liveword_Explored, word_exploring, Word_Directionwardset, direction)

   FIX({word: False for word in words}, update)
   return Word_Directionwardset



240511    21:06:25    

@line_profile
def GivensStrictsEquivalents_Immediates(given_ulteriors, self.strict_ulteriors, self.item_equivalents):
   mprint('computing immediate ulteriors')
#   if slow, can maybe speed up a lot by just checking each given ulterior whether it's in the ulteriors of the others.... idk. 
   # wait ..... NEDEED to makes ure to include equivalents of given guys which might NOT BE given!!!!
   result = {item: ulteriors - unionfold(Item_Allulteriors[ulterior] for ulterior in ulteriors)
                          for item, ulteriors in Item_Allulteriors.items()}
   mprint('done computing immediate ulteriors')
   return result
#the followoing is WRONG. would need to exclude Equivalents of item. but this could be faster. ... and it fails to get children of equivalents of the guy
#   return {item: ulteriors - unionfold(Item_Allulteriors[ulterior] for ulterior in ulteriors)
#                          for item, ulteriors in Item_Givenulteriors.items()}



240511    21:50:07    

newer but slower:

Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    52
    53
    54     44700        203.5      0.0      1.3
    55     44700         20.4      0.0      0.1
    56     44700         96.9      0.0      0.6
    57     24449          7.8      0.0      0.1
    58
    59     20251      14286.7      0.7     93.2
    60
    61
    62     20251        133.3      0.0      0.9
    63
    64
    65     20251        129.5      0.0      0.8
    66     20251         42.5      0.0      0.3
    67     42601        175.4      0.0      1.1
    68     22350         54.8      0.0      0.4
    69     22350         15.5      0.0      0.1
    70     22350         19.6      0.0      0.1
    71     20251        136.2      0.0      0.9
    72     20251          6.4      0.0      0.0

Total time: 2.46813 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   158
   159
   160     20252         13.1      0.0      0.5
   161    216892        102.2      0.0      4.1
   162    196640        376.9      0.0     15.3
   163    196640        200.3      0.0      8.1
   164    196640         90.0      0.0      3.6
   165    196640         68.0      0.0      2.8
   166    398921        144.9      0.0      5.9
   167    202281        919.5      0.0     37.3
   168    201062        183.9      0.0      7.5
   169    202281        191.3      0.0      7.8
   170    196640        171.8      0.0      7.0
   171     20252          6.2      0.0      0.3


newest:


   
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    52
    53
    54     44700        282.2      0.0      1.6
    55     44700         26.5      0.0      0.1
    56     44700        119.3      0.0      0.7
    57     24449         10.6      0.0      0.1
    58
    59     20251      16343.6      0.8     92.4
    60
    61
    62     20251        177.8      0.0      1.0
    63
    64
    65     20251        172.4      0.0      1.0
    66     20251         53.4      0.0      0.3
    67     42601        212.1      0.0      1.2
    68     22350         68.9      0.0      0.4
    69     22350         20.5      0.0      0.1
    70     22350         25.4      0.0      0.1
    71     20251        158.2      0.0      0.9
    72     20251          7.9      0.0      0.0

Total time: 2.26922 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   167
   168
   169     20252         30.8      0.0      1.4
   170    216892        129.7      0.0      5.7
   171    196640        523.9      0.0     23.1
   172    196640        243.2      0.0     10.7
   173    196640        708.4      0.0     31.2
   174    196640        625.4      0.0     27.6
   175     20252          7.8      0.0      0.3



older version that ought to be slower:

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    52
    53
    54     44700        204.4      0.0      1.7
    55     44700         21.9      0.0      0.2
    56     44700         90.6      0.0      0.7
    57     24449          8.1      0.0      0.1
    58
    59     20251      11214.2      0.6     92.2
    60
    61
    62     20251         80.2      0.0      0.7
    63
    64
    65     20251        129.1      0.0      1.1
    66     20251         23.9      0.0      0.2
    67     42601        171.3      0.0      1.4
    68     22350         51.4      0.0      0.4
    69     22350         14.7      0.0      0.1
    70     22350         18.7      0.0      0.2
    71     20251        134.4      0.0      1.1
    72     20251          6.3      0.0      0.1

Total time: 0.902099 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195     20252        902.1      0.0    100.0

now with drive db:

this is much slower???
@line_profile
def new_GivensStrictsEquivalents_Immediates(item_givens, item_stricts, item_equivalents):
   result = {}
   for item, givens in item_givens.items():
      given_neighborhood = ItemsetItemkeysItem_Shortunionfold(item_givens, item_equivalents, item)
      candidates = given_neighborhood - item_equivalents[item]
      remaining_candidates = set(candidates)
      immediates = set()
      for candidate in candidates:
         if not any(candidate in item_stricts[x] for xs in [immediates, remaining_candidates] for x in xs): 
            immediates.add(candidate)
         remaining_candidates.remove(candidate)
      result[item] = immediates
   return result


drive, both:

Total time: 77.9241 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    52
    53
    54    220214        967.8      0.0      1.2
    55    220214         97.0      0.0      0.1
    56    220214        451.1      0.0      0.6
    57    124205         41.7      0.0      0.1
    58
    59     96009      72967.6      0.8     93.6
    60
    61
    62     96009        465.5      0.0      0.6
    63
    64
    65     96009        673.1      0.0      0.9
    66     96009        131.2      0.0      0.2
    67    206116        922.8      0.0      1.2
    68    110107        276.6      0.0      0.4
    69    110107         75.0      0.0      0.1
    70    110107        100.1      0.0      0.1
    71     96009        722.9      0.0      0.9
    72     96009         31.4      0.0      0.0

Total time: 7.21283 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   168
   169
   170     96010         58.0      0.0      0.8
   171   1168886        535.5      0.0      7.4
   172   1072876       5428.4      0.0     75.3
   173   1072876       1159.0      0.0     16.1
   174
   175
   176
   177
   178
   179
   180     96010         32.0      0.0      0.4

Total time: 5.31306 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   182
   183
   184     96010       5313.1      0.1    100.0


again:

otal time: 77.8329 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    52
    53
    54    220214        970.7      0.0      1.2
    55    220214         95.4      0.0      0.1
    56    220214        451.5      0.0      0.6
    57    124205         39.9      0.0      0.1
    58
    59     96009      72871.3      0.8     93.6
    60
    61
    62     96009        469.5      0.0      0.6
    63
    64
    65     96009        676.3      0.0      0.9
    66     96009        130.6      0.0      0.2
    67    206116        920.3      0.0      1.2
    68    110107        276.9      0.0      0.4
    69    110107         80.0      0.0      0.1
    70    110107        101.6      0.0      0.1
    71     96009        719.0      0.0      0.9
    72     96009         29.8      0.0      0.0

Total time: 6.83497 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   168
   169
   170     96010         55.9      0.0      0.8
   171   1168886        527.6      0.0      7.7
   172   1072876       5071.1      0.0     74.2
   173   1072876       1148.9      0.0     16.8
   174
   175
   176
   177
   178
   179
   180     96010         31.5      0.0      0.5

Total time: 5.53227 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   182
   183
   184     96010       5532.3      0.1    100.0

 
 Total time: 75.2589 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    52
    53
    54    220214        942.5      0.0      1.3
    55    220214        119.2      0.0      0.2
    56    220214        516.8      0.0      0.7
    57    124205         47.6      0.0      0.1
    58
    59     96009      71019.8      0.7     94.4
    60
    61
    62     96009        477.8      0.0      0.6
    63
    64
    65     96009        764.7      0.0      1.0
    66     96009        135.2      0.0      0.2
    67    206116        425.3      0.0      0.6
    68    110107        307.2      0.0      0.4
    69    110107         92.1      0.0      0.1
    70    110107        104.7      0.0      0.1
    71     96009        270.9      0.0      0.4
    72     96009         35.1      0.0      0.0

Total time: 11.6604 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   168
   169
   170     96010         62.8      0.0      0.5
   171
   172
   173
   174   1168886        510.5      0.0      4.4
   175   1072876       2513.1      0.0     21.6
   176   1072876       3254.4      0.0     27.9
   177   1072876       1210.5      0.0     10.4
   178   1072876       2862.2      0.0     24.5
   179
   180   1072876       1212.8      0.0     10.4
   181     96010         34.2      0.0      0.3

Total time: 5.84894 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   183
   184
   185     96010       5848.9      0.1    100.0

240512    00:52:53    

counts = [result[1]-result[0] for result in G_COMPARE_results]
sum(counts)
newer_faster = [count for count in counts if count > 0]
sum(newer_faster)
old_faster = [count for count in counts if count < 0]
sum(old_faster)

newer_results = [result for result in G_COMPARE_results if result[1]-result[0] > 0]
old_results = [result for result in G_COMPARE_results if result[1]-result[0] < 0]

newer_lens0= [len(res[2][0]) for res in newer_results]
old_lens0= [len(res[2][0]) for res in old_results]

sorted(newer_lens0)
sorted(old_lens0)

def dict_sum(d):
   return sum(len(v) for v in d.values())

newer_lens = [dict_sum(res[2][0]) for res in newer_results]
sorted(newer_lens)

old_lens = [dict_sum(res[2][0]) for res in old_results]
sorted(old_lens)

newer_lens = [dict_sum(res[2][1]) for res in newer_results]
sorted(newer_lens)

old_lens = [dict_sum(res[2][1]) for res in old_results]
sorted(old_lens)


G_COMPARE_results = []
#doesn't account for caching. 
def COMPARE(fun1, fun2, *args):
#   print(args)
   WIPE_GLOBALCACHES()
   start1 = time.time()
   result1 = fun1(*args)
   end1 = time.time()
   WIPE_GLOBALCACHES()
   start2 = time.time()
   result2 = fun2(*args)
   end2 = time.time()
   WIPE_GLOBALCACHES()
   G_COMPARE_results.append((end1-start1, end2-start2, args, result2))

   assert result1 == result2, print( obj_diff(result1, result2))
#   assert result1 == result2, print( args, obj_diff(result1, result2))
   return result2


240512    09:08:23    

@line_profile
def WordrawtextLang_Wordtext(wordrawtext, lang):
   wordtext = wordrawtext
   wordtext = WordrawtextLang_Wordtext_Redirect_corrected(wordtext, lang)
   wordtext = WordrawtextLang_Wordtext_LUA_corrected(wordtext, lang)
   if wordtext == wordrawtext:
      return wordtext
   second = wordtext
   wordtext = WordrawtextLang_Wordtext_Redirect_corrected(wordtext, lang)
   assert second == wordtext, print('boooop here1: ', second, wordtext)
   wordtext = WordrawtextLang_Wordtext_LUA_corrected(wordtext, lang)
   assert second == wordtext, print('boooop here2: ', second, wordtext)
   return wordtext



240512    09:22:16    
not clear what to do with this. but 1. doing more than fthe first two seems to not matter much; 2. often neither of them do anything; and 3. the logical order, which i assume is what wiktionary actually does, would be lua and then redirect, both only once. 
@line_profile
def WordrawtextLang_Wordtext(wordrawtext, lang):
   wordtext = wordrawtext
   wordtext = WordrawtextLang_Wordtext_LUA_corrected(wordtext, lang)
   luaed = wordtext
   wordtext = WordrawtextLang_Wordtext_Redirect_corrected(wordtext, lang)
   luaed_redirected = wordtext

   if wordtext == wordrawtext:
      return wordtext

   wordtext = WordrawtextLang_Wordtext_LUA_corrected(wordtext, lang)
   luaed_redirected_luaed = wordtext

   assert luaed_redirected == luaed_redirected_luaed, print(f'boooop here1: lang {lang}; wordrawtext {wordrawtext}; luaed {luaed}; luaed_redirected {luaed_redirected}; luaed_redirected_luaed {luaed_redirected_luaed};')
   wordtext = WordrawtextLang_Wordtext_Redirect_corrected(wordtext, lang)
   luaed_redirected_luaed_redirected = wordtext

   assert luaed_redirected_luaed == luaed_redirected_luaed_redirected, print(f'boooop here1: lang {lang}; wordrawtext {wordrawtext}; luaed {luaed}; luaed_redirected {luaed_redirected}; luaed_redirected_luaed {luaed_redirected_luaed}; luaed_redirected_luaed_redirected {luaed_redirected_luaed_redirected};')
   return wordtext


240512    09:36:37    
Total time: 83.6864 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   246
   247
   248   4527842       2741.9      0.0      3.3
   249
   250   4527842      24767.6      0.0     29.6
   251   4527842       3967.7      0.0      4.7
   252   4527842       6867.8      0.0      8.2
   253   4527842       6161.1      0.0      7.4
   254   4527842       1367.8      0.0      1.6
   255   4527842       1001.9      0.0      1.2
   256   4527842       8520.0      0.0     10.2
   257      1524          0.4      0.0      0.0
   258      1524          2.0      0.0      0.0
   259
   260   4527842       1980.7      0.0      2.4
   261    830628        266.6      0.0      0.3
   262     11676          9.4      0.0      0.0
   263    830628        788.9      0.0      0.9
   264    830628        385.6      0.0      0.5
   265     11834          6.0      0.0      0.0
   266    830628        426.4      0.0      0.5
   267      1348        221.1      0.2      0.3
   268      1348        340.2      0.3      0.4
   269      1348          1.8      0.0      0.0
   270
   271    829280       1756.3      0.0      2.1
   272    829280       1669.9      0.0      2.0
   273    829280      18402.3      0.0     22.0
   274
   275   4527842       2033.1      0.0      2.4



Total time: 250.049 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   246
   247
   248  12863330       8452.6      0.0      3.4
   249
   250  12863330      72400.8      0.0     29.0
   251  12863330      11623.4      0.0      4.6
   252  12863330      20757.6      0.0      8.3
   253  12863330      17078.3      0.0      6.8
   254  12863330       4424.7      0.0      1.8
   255  12863330       3012.1      0.0      1.2
   256  12863330      25414.4      0.0     10.2
   257      5830          1.5      0.0      0.0
   258      5830          8.0      0.0      0.0
   259
   260  12863330       5814.5      0.0      2.3
   261   2884970        901.7      0.0      0.4
   262     74744         60.1      0.0      0.0
   263   2884970       2956.2      0.0      1.2
   264   2884970       1398.4      0.0      0.6
   265     39688         21.0      0.0      0.0
   266   2884970       1530.2      0.0      0.6
   267      5010        847.2      0.2      0.3
   268      5010       1216.3      0.2      0.5
   269      5010          6.5      0.0      0.0
   270
   271   2879960       6253.7      0.0      2.5
   272   2879960       5813.0      0.0      2.3
   273   2879960      53923.0      0.0     21.6
   274
   275  12863330       6133.4      0.0      2.5


240512    10:08:41    


#try without []. try with rsplit
def excise_substrings(text, div1, div2):
   return ''.join([x.split(div2)[-1] for x in text.split(div1)])

#try without []. try with rsplit
def new_excise_substrings(text, div1, div2):
   return ''.join(x.rsplit(div2, 1)[-1] for x in text.split(div1))

ok technically these are different if there are three } like }}} but who cares




Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   246
   247
   248   2263921       1548.5      0.0      2.6
   249
   250
   251   2263921      19961.7      0.0     34.0
   252   2263921       2199.7      0.0      3.7
   253   2263921       4092.0      0.0      7.0
   254   2263921       3353.5      0.0      5.7
   255   2263921        812.9      0.0      1.4
   256   2263921        546.3      0.0      0.9
   257   2263921       4419.3      0.0      7.5
   258       762          0.2      0.0      0.0
   259       762          1.0      0.0      0.0
   260
   261   2263921       1042.3      0.0      1.8
   262    415314        141.7      0.0      0.2
   263      5838          4.8      0.0      0.0
   264    415314        427.5      0.0      0.7
   265    415314        214.5      0.0      0.4
   266      5917          3.3      0.0      0.0
   267    415314        236.2      0.0      0.4
   268       674        116.0      0.2      0.2
   269       674        207.6      0.3      0.4
   270       674          0.9      0.0      0.0
   271
   272    414640        922.7      0.0      1.6
   273    414640        887.1      0.0      1.5
   274    414640      16474.5      0.0     28.1
   275
   276   2263921       1103.8      0.0      1.9



240512    10:50:06    

new way, with no line_prof:

Total time: 53.6585 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   246
   247
   248   2263921       1589.3      0.0      3.0
   249
   250
   251   2263921      13444.0      0.0     25.1
   252   2263921       2260.6      0.0      4.2
   253   2263921       3935.2      0.0      7.3
   254   2263921       3514.1      0.0      6.5
   255   2263921        806.2      0.0      1.5
   256   2263921        555.3      0.0      1.0
   257   2263921       4645.6      0.0      8.7
   258       762          0.2      0.0      0.0
   259       762          1.1      0.0      0.0
   260
   261   2263921       1090.1      0.0      2.0
   262    415314        146.9      0.0      0.3
   263      5838          5.0      0.0      0.0
   264    415314        459.5      0.0      0.9
   265    415314        225.9      0.0      0.4
   266      5917          3.4      0.0      0.0
   267    415314        247.3      0.0      0.5
   268       674        126.1      0.2      0.2
   269       674        205.6      0.3      0.4
   270       674          1.0      0.0      0.0
   271
   272    414640        996.4      0.0      1.9
   273    414640        923.2      0.0      1.7
   274    414640      17331.8      0.0     32.3
   275
   276   2263921       1144.6      0.0      2.1


old way, with no line_prof:
Total time: 45.0582 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   246
   247
   248   2263921       1345.3      0.0      3.0
   249
   250
   251   2263921       9656.9      0.0     21.4
   252   2263921       1962.1      0.0      4.4
   253   2263921       3609.6      0.0      8.0
   254   2263921       2916.1      0.0      6.5
   255   2263921        667.7      0.0      1.5
   256   2263921        482.2      0.0      1.1
   257   2263921       4212.0      0.0      9.3
   258       762          0.2      0.0      0.0
   259       762          0.9      0.0      0.0
   260
   261   2263921        970.7      0.0      2.2
   262    415314        127.4      0.0      0.3
   263      5838          4.4      0.0      0.0
   264    415314        393.8      0.0      0.9
   265    415314        194.3      0.0      0.4
   266      5917          3.1      0.0      0.0
   267    415314        218.2      0.0      0.5
   268       674        102.6      0.2      0.2
   269       674        222.7      0.3      0.5
   270       674          1.1      0.0      0.0
   271
   272    414640        861.8      0.0      1.9
   273    414640        817.3      0.0      1.8
   274    414640      15264.7      0.0     33.9
   275
   276   2263921       1023.1      0.0      2.3

@line_profile
def excise_substrings(text, div1, div2):
   splits = text.split(div1)
   if len(splits) == 1:
      return splits[0]
   else:
      return ''.join(x.rsplit(div2, 1)[-1] for x in splits)


240512    11:13:58    
in ReftextRefdict(ref):
                           if word.strip(', []/()') not in ['-', '']]




#@line_profile
def newer_GivensStrictsEquivalents_Immediates(item_givens, item_stricts, item_equivalents):
   result = {}
#   for item, ulteriors in item_stricts.items():
#      candidates_stricts = ItemsetKeys_Shortunionfold(item_stricts, ulteriors)

   for item in item_givens.keys():
      given_neighborhood = ItemsetItemkeysItem_Shortunionfold(item_givens, item_equivalents, item)
      given_full_neighborhood = unionfold(item_equivalents[neighbor] for neighbor in given_neighborhood)
      candidates = given_full_neighborhood - item_equivalents[item]
      candidates_stricts = ItemsetKeys_Shortunionfold(item_stricts, candidates)

      result[item] = candidates - candidates_stricts 
   return result

def old_GivensStrictsEquivalents_Immediates(_givens, item_stricts, _equivs):
   return {item: ulteriors - unionfold(item_stricts[x] for x in ulteriors) for item, ulteriors in item_stricts.items()}


#   if slow, can maybe speed up a lot by just checking each given ulterior whether it's in the ulteriors of the others.... idk. 
   # wait ..... NEDEED to makes ure to include equivalents of given guys which might NOT BE given!!!!
#the followoing is WRONG. would need to exclude Equivalents of item. but this could be faster. ... and it fails to get children of equivalents of the guy
#   return {item: ulteriors - unionfold(Item_Allulteriors[ulterior] for ulterior in ulteriors)
#                          for item, ulteriors in Item_Givenulteriors.items()}







#NONSTRICT
#@line_profile
def UlteriorsPriors_NonstrictTransitiveClosure_Equivalents(Item_Givenulteriors, Item_Givenpriors):
   Item_Equivalents = {item:{item} for item in Item_Givenulteriors.keys()}
   cycle_bound = 1
   mprint("taking transitive closure")
   remaining_domain, upcoming_candidates = set(Item_Givenulteriors.keys()), set(Item_Givenulteriors.keys())
   Item_AllNonstrictUlteriors = {item:{item} for item in Item_Givenulteriors.keys()}
   while len(remaining_domain)>0:
      madeprogress = False
      current_candidates, upcoming_candidates = upcoming_candidates & remaining_domain, set()
      for x in current_candidates:
#         equivalents_ulteriors = (Item_Givenulteriors[x] if len(Item_Equivalents[x])==1 else 
#                              unionfold(Item_Givenulteriors[equivalent] for equivalent in Item_Equivalents[x]))
#         equivalents_ulteriors = ItemsetItems_Shortunionfold(Item_Givenulteriors, Item_Equivalents[x])
         equivalents_ulteriors = ItemsetItemkeysItem_Shortunionfold(Item_Givenulteriors, Item_Equivalents, x)
         if not any(y in remaining_domain and y not in Item_Equivalents[x] for y in equivalents_ulteriors): 
            remaining_domain.difference_update(Item_Equivalents[x])
            ALL_ULTERIORS = unionfold(Item_AllNonstrictUlteriors[y] 
                      for equivalent in Item_Equivalents[x] for y in ({equivalent}|Item_Givenulteriors[equivalent]))
            for equivalent in Item_Equivalents[x]:
               Item_AllNonstrictUlteriors[equivalent] = ALL_ULTERIORS
#            upcoming_candidates.update(Item_Givenpriors[x] if len(Item_Equivalents[x])==1 else 
#                                       unionfold(Item_Givenpriors[z] for z in Item_Equivalents[x]))

            upcoming_candidates.update(ItemsetItemkeysItem_Shortunionfold(Item_Givenpriors, Item_Equivalents, x))
            upcoming_candidates.difference_update(Item_Equivalents[x])
            madeprogress = True
      if not madeprogress:
         if cycle_bound>50:
            raise Exception('cycle bound exceeded 50')
         cycle_bound += 1
         UlteriorsEquivalentsK_MarkKcycles(Item_Givenulteriors, Item_Equivalents, cycle_bound)
         upcoming_candidates = remaining_domain
   mprint("done taking transitive closure")
   return Item_AllNonstrictUlteriors, Item_Equivalents

240512    14:19:09    
def special_WordsDirection_UlteriorclosureGivens(words, direction):

#            maybe NOLNY when put in BY givens and YOU are a nongiven (immediates); only then might we be missing eqlivaulents. if you are put in by immediates, your equivalents weer also put in by immdeatise. if you are using your givens, then you will point at some of your equivalents; and they will also be using their givens (else, you'd have your immediates as well)

precomputing/5_wordpastmosts.py

12 May 2024
#doesn't seem to help? (have to turn off WIPE ofc, but even so)
#SQL_WORD_Direction_GivenRefWords.ALLCACHE()

240512    14:53:06    
i guess it's fine as is? was there something better about the complicated version? maybe slightly faster because it doesn't have to go over the equivalents's guys to insert them? realy doesn't matter much assuming cycles rare. 


#### WWWARNING: this is WRONG. it doesn't handle equivalents correctly. consider: cycle A<>B. then C given-points to A. so first we've done A, which means we've gotten  A,B's immediates--but excluded B,A from each other. now we do C. problem: C doesn't see B. we gotta instaed keep track of equivalents... and addem in... when? i guess when getting from Given, but not from RAM immediate, we have to also add equivalents.... well, or we could just .... like, to add a word, instead you add the list of equivalents......
## this is probably still a bit buggy, i think only at the start?
@line_profile
def special_WordsDirection_UlteriorclosureGivens(words, direction):
   Word_Directionwardset = {}
   def update(Liveword_Explored, word_exploring):
      using_givens = word_exploring not in RAM_word_immediate_pastwards 
      Word_Directionwardset[word_exploring] = WordDirection_Set(word_exploring, direction) if using_givens else RAM_word_immediate_pastwards[word_exploring] 
      for new_word in Word_Directionwardset[word_exploring]:
         if using_givens and new_word in RAM_word_immediate_pastwards and not Liveword_Explored.get(new_word, False):
            for equivalent in RAM_word_equivalents[new_word]:
               Liveword_Explored.setdefault(equivalent, False)

#            equivalents = RAM_word_equivalents[new_word]
#            for equivalent in equivalents:
#               Word_Directionwardset[equivalent] = RAM_word_immediate_pastwards[new_word] 
#               Liveword_Explored[equivalent] = True
#            for new_new_word in RAM_word_immediate_pastwards[new_word]:
#               Liveword_Explored.setdefault(new_word, False)
         else:
            Liveword_Explored.setdefault(new_word, False)

   FIX({eq_word: False for word in words for eq_word in RAM_word_equivalents.get(word, [word])}, update)
   return Word_Directionwardset


240512    15:42:15    

from pympler import asizeof
import sys

def mbs(k):
   return round(k/1000000, 1)


for key in globals().keys():
   obj = globals()[key]
   better_size = mbs(asizeof.asizeof(obj))
   if better_size > 10:
      print(key, len(obj), mbs(sys.getsizeof(obj)), 'MB', better_size, 'MB')


for key in globals().keys():
   if  not key.startswith('_') and not key=='GLOBALCACHES':
      obj = globals()[key]
      try:
         print(key,  mbs(sys.getsizeof(obj)), 'MB', mbs(asizeof.asizeof(obj)), 'MB')
      except:
         pass



#WiktionaryXMLfile = "SUBUNIVERSE_WIKTIONARY_FULL_24.04.25_sing24.05.06-08_42_14.xml"
#WiktionaryXMLfile = "SUBUNIVERSE_WIKTIONARY_FULL_24.04.25_drive24.05.06-08_58_15.xml"
#WiktionaryXMLfile = "SUBUNIVERSE_WIKTIONARY_FULL_24.04.25_value24.05.05-20_05_53.xml"

#     4,653,441 May  6 08:42 SUBUNIVERSE_WIKTIONARY_FULL_24.04.25_sing24.05.06-08_42_14.xml
#    29,495,035 May  6 08:58 SUBUNIVERSE_WIKTIONARY_FULL_24.04.25_drive24.05.06-08_58_15.xml
#    64,245,104 May  5 20:05 SUBUNIVERSE_WIKTIONARY_FULL_24.04.25_value24.05.05-20_05_53.xml
#   807,316,584 May  6 08:57 SUBUNIVERSE_WIKTIONARY_FULL_24.04.25_coalesce24.05.06-08_56_02.xml
#   873,927,739 May  6 08:58 SUBUNIVERSE_WIKTIONARY_FULL_24.04.25_enjoin24.05.06-08_57_02.xml


240513    01:50:05    


# note: this does a weird thing with cycles between pastmosts. here we exclude them from each other's things, because we restrict to all_ulteriors; and even word_exploring doesn't have to have everything in the poset be in its past! 

def FUN(word_exploring, _dir, _givens):
   WIPE_GLOBALCACHES()
   results = []
   if word_exploring in RAM_words_already_explored:
      return results
#   pastward_poset = COMPARE(special_Word_Pastwardposet, Word_Pastwardposet, word_exploring)
   pastward_poset = special_Word_Pastwardposet(word_exploring)
#   pastward_poset = Word_Pastwardposet(word_exploring)

   RAM_word_immediate_pastwards.update(pastward_poset.immediate_ulteriors)
   for item, equivalents in pastward_poset.item_equivalents.items():
      RAM_word_equivalents.setdefault(item, equivalents)


   pastmosts = Poset_Ulteriormosts(pastward_poset)
   RAM_all_pastmosts.update(pastmosts)
   for word in set(pastward_poset.domain) - RAM_words_already_explored:
      word_domain = pastward_poset.nonstrict_ulteriors(word)
      results.append((SQL_WORD_Allpastwards, word, word_domain))
      results.append((SQL_WORD_Pastmosts, word, word_domain & pastmosts))
   RAM_words_already_explored.update(pastward_poset.domain)
   return results


to_do_1 = {'generator source':SQL_WORD_Direction_GivenRefWords, 'function':FUN, 'target tables':[SQL_WORD_Allpastwards, SQL_WORD_Pastmosts],} 

to_do_2 = OBJECT_TABLE_TODO(RAM_all_pastmosts, SQL_AllPastmosts)
#to_do_2 = {'generator source':[(RAM_all_pastmosts, )], 'function':lambda x:[(x,)], 'target tables':[SQL_AllPastmosts], 'message': 'writing '}

to_do_3 = OBJECT_TABLE_TODO(RAM_word_immediate_pastwards, SQL_WORD_ImmediatePastwardset)
#to_do_3 = {'generator source':RAM_word_immediate_pastwards.items(), 'function':lambda x,y: [(x,y)], 'target tables':[SQL_WORD_ImmediatePastwardset], 'message': 'writing '}

to_do_4 = OBJECT_TABLE_TODO(RAM_word_equivalents, SQL_WORD_Equivalents)
#to_do_4 = {'generator source':RAM_word_equivalents.items(), 'function':lambda x,y: [(x,y)], 'target tables':[SQL_WORD_Equivalents], 'message': 'writing '}

240513    11:25:42    

#import scalene
#      scalene_profiler.start()
#      scalene_profiler.stop()
240513    12:03:33    


def FUN(word_exploring, _dir, _givens):
   WIPE_GLOBALCACHES()
   results = []
   if word_exploring in RAM_words_already_explored:
      return results
#   pastward_poset = COMPARE(special_Word_Pastwardposet, Word_Pastwardposet, word_exploring)
   pastward_poset = special_Word_Pastwardposet(word_exploring)
#   pastward_poset = Word_Pastwardposet(word_exploring)

   RAM_word_immediate_pastwards.update(pastward_poset.immediate_ulteriors)
   for item, equivalents in pastward_poset.item_equivalents.items():
      RAM_word_equivalents.setdefault(item, equivalents)

   pastmosts = Poset_Ulteriormosts(pastward_poset)
   RAM_all_pastmosts.update(pastmosts)
   for word in set(pastward_poset.domain) - RAM_words_already_explored:
      results.append((SQL_WORD_Strictpastwardset, word, pastward_poset.strict_ulteriors[word]))
      results.append((SQL_WORD_Pastmosts, word, pastward_poset.nonstrict_ulteriors(word)& pastmosts))
   RAM_words_already_explored.update(pastward_poset.domain)
   return results


240513    12:38:20    

   RAM_word_immediate_pastwards.update(pastward_poset.immediate_ulteriors)
   RAM_word_equivalents.update(pastward_poset.item_equivalents)
#   for item, equivalents in pastward_poset.item_equivalents.items():
#      RAM_word_equivalents.setdefault(item, equivalents)


240513    14:29:18    
to_do_3 = OBJECT_TABLE_TODO(MENTIONERS, SQL_WORD_LinkerSenses)
#to_do_3 = {'generator source':MENTIONERS.items(), 'function':lambda x,y: [(x,y)], 'target tables':[SQL_WORD_LinkerSenses], 'message': 'writing '}


240515    17:58:34    
exec(open('src/radix.py').read())
k1,k2 = AssertTablesEqual(SQL_WORD_Allfuturewards, SQL_WORD_Strictfutureset)
obj_diff(k1,k2)

k1,k2 = AssertTablesEqual(SQL_WORD_Direction_Coveringsenses, SQL_WORD_Direction_Coveringsenses_tweak)


exec(open('src/precomputing/run_all.py').read())


exec(open('src/radix.py').read())
k1,k2 = AssertTablesEqual(SQL_WORD_Strictfutureset, SQL_WORD_Strictfuturewardset)
k1 == k2

obj_diff(k1,k2)

len(k1)
22350
len(k2)
2563

results = G_Err_value[2:]
k1 = results[0]
key= list(k2.keys())[0]

k1.pop('')

exec(open('src/radix.py').read())
SQL_WORD_Direction_GivenRefWords.ALLCACHE()

len(GLOBALCACHES['SQL_WORD_Direction_GivenRefWords_CACHE_Select'])

SQL_WORD_Strictpastwardset.ALLCACHE()
d = GLOBALCACHES['SQL_WORD_Strictpastwardset_CACHE_Select']
len(d)
WUT. oh nvm



results[0]
('remove', [([lessen, nl],)], [(0, {[les, id], [ls, jvn], [acteerles, nl], [gymnastiekles, nl], [lesgeven, nl], [gymles, nl], [leslokaal, nl], [avondles, nl], [ls, pap], [les, af], [zwemles, nl], [lesplan, nl], [lesrooster, nl], [paardrijles, nl], [lesuur, nl], [rijles, nl], [bijles, nl], [schoolles, nl]})])
k2[(str_Word('[lessen, nl]'),)]
{[zangles, nl]}
k1[(str_Word('[lessen, nl]'),)]
{[ls, jvn], [gymnastiekles, nl], [zangles, nl], [gymles, nl], [avondles, nl], [les, af], [lesrooster, nl], [bijles, nl], [schoolles, nl], [les, id], [acteerles, nl], [lesgeven, nl], [leslokaal, nl], [ls, pap], [zwemles, nl], [lesplan, nl], [paardrijles, nl], [lesuur, nl], [rijles, nl]}



k2[(str_Word('[lessen, nl]'),)]


pastward_poset = special_Word_Pastwardposet(word_exploring)

exec(open('src/radix.py').read())
word_exploring =  str_Word('[ls, jvn]')
word_exploring =  str_Word('[gymles, nl]')
pastward_poset = Word_Pastwardposet(word_exploring)
pastward_poset.strict_ulteriors[word_exploring]
spec_pastward_poset = special_Word_Pastwardposet(word_exploring)
spec_pastward_poset.strict_ulteriors[word_exploring]

SQL_WORD_Strictpastwardset.Select(word_exploring)


240515    18:08:17    
SQL_Table('WORD_Strictfutureset', ['WORD'], 'strict_futures', storage={'pickle', 'compress'})


SQL_Table('WORD_Allfuturewards', ['WORD'], 'all_futurewards', storage={'pickle', 'compress'})



240515    22:51:44    
exec(open('src/radix.py').read())
k1,k2 = AssertTablesEqual(SQL_WORD_Direction_Coveringsenses, SQL_WORD_Direction_Coveringsenses_tweak)
k1== k2
obj_diff(k1,k2)


printing obj_diff of the two results:
('add', [([wateren, nl], 'futureward')], [(0, {[water, dum]_1})])


240515    23:03:12    

Total time: 26.3114 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

exec(open('src/precomputing/N_IJ_word_coverings.py').read())

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     9
    10
    11     44700      26311.4      0.6    100.0

Total time: 29.7153 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    16
    17
    18      1581        153.7      0.1      0.5
    19      1581          2.0      0.0      0.0
    20     66391         64.4      0.0      0.2
    21     64810        223.5      0.0      0.8
    22     22350         40.1      0.0      0.1
    23
    24     22350        115.8      0.0      0.4
    25
    26     22350       2181.9      0.1      7.3
    27     22350         72.8      0.0      0.2
    28
    29     22350      22354.2      1.0     75.2
    30
    31     22350         46.9      0.0      0.2
    32     22350         69.5      0.0      0.2
    33     22350       4389.5      0.2     14.8
    34      1581          1.0      0.0      0.0





exec(open('src/precomputing/N_GI_word_coverings.py').read())

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    11
    12
    13     44680      79144.9      1.8    100.0

Total time: 88.9586 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    18
    19
    20     22350       4279.1      0.2      4.8
    21     22350         31.8      0.0      0.0
    22     22350        139.3      0.0      0.2
    23     22340       2469.7      0.1      2.8
    24     22340       1510.4      0.1      1.7
    25     22340         65.7      0.0      0.1
    26
    27     22340      72056.7      3.2     81.0
    28     22340        237.0      0.0      0.3
    29
    30     22340       8038.9      0.4      9.0
    31     22340        107.6      0.0      0.1
    32     22350         22.5      0.0      0.0



240516    21:06:49    
rg "\{\{senseid\|"

rg --count "\{\{senseid\|" enwiktionary-latest-pages-meta-current-28mar2024.xml
13349
rg --count "\{\{senseid\|en" enwiktionary-latest-pages-meta-current-28mar2024.xml
7095

rg --count "\|id=" enwiktionary-latest-pages-meta-current-28mar2024.xml
53479

rg --count "\{\{etymid\|" enwiktionary-latest-pages-meta-current-28mar2024.xml
4060

rg --count "\{\{anchor\|" enwiktionary-latest-pages-meta-current-28mar2024.xml
4328

https://en.wiktionary.org/wiki/Template:senseid
https://en.wiktionary.org/wiki/Template:etymid
https://en.wiktionary.org/wiki/Template:anchor


rg --count "<id" enwiktionary-latest-pages-meta-current-28mar2024.xml

240518    07:17:41    
NIJ
indexing SQL_WORD_Direction_Coveringsenses
done indexing SQL_WORD_Direction_Coveringsenses
done with WORD_Direction_Coveringsenses
Timer unit: 0.001 s

Total time: 50.5495 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     9
    10
    11    220212      50549.5      0.2    100.0

Total time: 58.685 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    16
    17
    18
    19     10325         29.2      0.0      0.0
    20     10325          5.4      0.0      0.0
    21    383021        181.3      0.0      0.3
    22    372696        640.5      0.0      1.1
    23    110106        123.0      0.0      0.2
    24
    25
    26    110106        417.9      0.0      0.7
    27
    28    110106       5266.3      0.0      9.0
    29
    30    110106      43437.6      0.4     74.0
    31
    32    110106        131.0      0.0      0.2
    33    110106        279.9      0.0      0.5
    34    110106       8168.8      0.1     13.9
    35     10325          4.0      0.0      0.0


indexing SQL_WORD_Direction_Coveringsenses_tweak
done indexing SQL_WORD_Direction_Coveringsenses_tweak
done with WORD_Direction_Coveringsenses_tweak
Timer unit: 0.001 s

Total time: 55.4838 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    11
    12
    13    220096      55483.8      0.3    100.0

Total time: 65.5701 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    18
    19
    20    110106        332.2      0.0      0.5
    21
    22    110106         48.1      0.0      0.1
    23    110106        202.3      0.0      0.3
    24    110048       4920.6      0.0      7.5
    25    110048       2657.1      0.0      4.1
    26    110048        121.6      0.0      0.2
    27
    28    110048      47303.2      0.4     72.1
    29    110048        285.0      0.0      0.4
    30
    31    110048       9451.3      0.1     14.4
    32    110048        191.4      0.0      0.3
    33    110106         57.4      0.0      0.1


240518    08:07:41    


indexing SQL_WORD_Direction_Coveringsenses
done indexing SQL_WORD_Direction_Coveringsenses
done with WORD_Direction_Coveringsenses
Timer unit: 0.001 s

Total time: 14.6002 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    92
    93
    94    119880      14600.2      0.1    100.0

Total time: 14.0131 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    96
    97
    98    119880         50.1      0.0      0.4
    99    119880      13963.0      0.1     99.6

Total time: 0.784175 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   101
   102
   103    119880        724.8      0.0     92.4
   104    119880         59.4      0.0      7.6

1 0 0.6666666666666667


exec(open('src/radix.py').read())
k1,k2 = AssertTablesEqual(SQL_WORD_Direction_Coveringsenses, SQL_WORD_Direction_Coveringsenses_tweak)
k1== k2
obj_diff(k1,k2)

N_IJ_word_coverings
240518    12:06:59    

#@CACHIFY
#def SenseDirection_Wordrefs(sense, direction):
#   return set() if sense.num == -1 else internal_SenseDirection_Wordrefs(sense, direction)

#def internal_SenseDirection_Wordrefs(sense, direction):
#   source_index = 0 if direction == 'futureward' else 1
#   return {link[1-source_index] for link in Sense_FutureWordlinks(sense) if link[source_index] == sense.word}
#
#def Sense_FutureWordlinks(sense):
#   wordlinks = SQL_SENSE_FutureWordlinks_CACHE_Select(sense)
#   return wordlinks if wordlinks else set() 


240518    13:53:07    

to_do_1 = {'generator source':SQL_AllPastmosts.OneValue(), 'function':FUN, 'target tables':[SQL_WORD_FuturePoset], }
#to_do_1 = {'generator source':[(x,) for x in SQL_AllPastmosts.OneValue()], 'function':FUN, 'target tables':[SQL_WORD_FuturePoset], }



to_do_1 = OBJECT_TABLE_TODO(GWordtitle_Redirect, SQL_WordtitleRedirect, single_value=True)
#to_do_1 = {'generator source':[(GWordtitle_Redirect, )], 'function':lambda x: [(x,)], 'target tables':[SQL_WordtitleRedirect], 'message': 'writing '}

240518    15:03:10    
def Pastmostsense_Futuresenseposets(pastmost_sense):
   if 'futureward_senselangposet' in G_included_levels:
      return [SQL_SENSE_Lang_Futuresenseposet.Select(pastmost_sense, lang) for lang in G_Coglangs]
   else:
      raise Exception('boop')
      wordposet = Word_Futureposet(pastmost_sense.word)
      includedSenses = WordposetWordsenses_Includedsenses(wordposet, {pastmost_sense.word:{pastmost_sense}})
      return [PosetLangIncludedsenses_Subsenseposet(wordposet, lang, includedSenses) for lang in G_Coglangs]


240518    15:08:58    
def Word_Futureposet(word):
   pastmost_poset = Pastmost_Futureposet(list(Word_Pastmosts(word))[0])
   return WordPoset_Ulteriorsubposet(word, pastmost_poset)


240518    16:01:18    

for table in [SQL_WORD_Parsed, SQL_SENSE_FutureWordlinks, SQL_WORD_Direction_GivenRefWords, SQL_WORD_Pastmosts, SQL_WORD_RealGivenSenses, SQL_WORD_FuturePoset, SQL_WORD_Direction_Coveringsenses, SQL_WORD_Coglang_Langfutures, SQL_SENSE_Direction_Wordrefs, SQL_WORD_Equivalents]:

240518    17:27:29    

def WordDirection_ManualPoset(main_word, direction):
   domain_set = WordDirection_Stricts(main_word, direction) | {main_word}
#   given_ulteriors = {word:WordDirection_Immediates(word, direction) for word in domain_set}

#   return Poset(given_ulteriors, direction=direction)
#   domain_set = WordDirection_Stricts(main_word, direction) | SQL_WORD_Equivalents_CACHE_Select(main_word) 
   all_attributes = {'direction': direction, 
         'item_equivalents':{word:SQL_WORD_Equivalents_CACHE_Select(word) for word in domain_set}, 
         'immediate_ulteriors':{word:WordDirection_Immediates(word, direction) for word in domain_set}, 
         'immediate_priors':{word:WordDirection_Immediates(word, Direction_Reversed(direction)) & domain_set for word in domain_set}, 
         'strict_ulteriors':{word:WordDirection_Stricts(word, direction) for word in domain_set}}
   all_attributes['domain'] = ImmediateulteriorsImmediatepriors_Sorted(all_attributes['immediate_ulteriors'], all_attributes['immediate_priors'])
   all_attributes['item_equivalents'][main_word] = {main_word}
   return Poset(all_attributes, manual=True)



# new way seems to both work, and be extremely similar to old, so go with it
def WordDirection_ManualPoset(main_word, direction):
   # this is the old way, but the below seems more correct:
#   domain_set = WordDirection_Stricts(main_word, direction) | {main_word}
   domain_set = WordDirection_Stricts(main_word, direction) | SQL_WORD_Equivalents_CACHE_Select(main_word) 
   all_attributes = {'direction': direction, 
         'item_equivalents':{word:SQL_WORD_Equivalents_CACHE_Select(word) for word in domain_set}, 
         'immediate_ulteriors':{word:WordDirection_Immediates(word, direction) for word in domain_set}, 
         'immediate_priors':{word:WordDirection_Immediates(word, Direction_Reversed(direction)) & domain_set for word in domain_set}, 
         'strict_ulteriors':{word:WordDirection_Stricts(word, direction) for word in domain_set}}
   all_attributes['domain'] = ImmediateulteriorsImmediatepriors_Sorted(all_attributes['immediate_ulteriors'], all_attributes['immediate_priors'])
   # old way, but hopefully not needed?
#   all_attributes['item_equivalents'][main_word] = {main_word}
   return Poset(all_attributes, manual=True)

240518    17:38:54    
python src/precomputing/P_O_pastmostsense_futuresenseposet.py
indexing SQL_SENSE_Lang_Futuresenseposet
done indexing SQL_SENSE_Lang_Futuresenseposet
done with SENSE_Lang_Futuresenseposet
Timer unit: 0.001 s

Total time: 7.20943 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    62
    63
    64      2306        290.1      0.1      4.0
    65      4612          3.9      0.0      0.1
    66      2306        856.9      0.4     11.9
    67      2306        881.4      0.4     12.2
    68      2306       1209.8      0.5     16.8
    69      2306       2291.4      1.0     31.8
    70      2306       1662.1      0.7     23.1
    71      2306         14.0      0.0      0.2



Total time: 5.98758 s

Could not find file <string>
Are you sure you are running this program from the same directory
that you ran the profiler from?
Continuing without the function's contents.

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    62
    63
    64      2306        468.7      0.2      7.8
    65      4612          4.0      0.0      0.1
    66      2306        858.8      0.4     14.3
    67      2306        456.9      0.2      7.6
    68      2306        617.2      0.3     10.3
    69      2306       1971.6      0.9     32.9
    70      2306       1597.4      0.7     26.7
    71      2306         13.0      0.0      0.2

240518    17:49:16    
delete R_O_pastmostsense_futuresenseposet


240518    20:20:35    
#def Pastmost_Futureposet(pastmost):
#   if 'futureposets' in G_included_levels:
#      return SQL_WORD_FuturePoset_CACHE_Select(pastmost)
#   else:
#      return WordsDirection_Closureposet({pastmost},'futureward')

#J_G_pastmost_futureposet

N_IJ_word_coverings

240518    21:15:53    
#to_do_2 = {'generator source':[(all_pastmost_senses, )], 'function':lambda x:[(x, )], 'target tables':[SQL_AllPastmostsenses], }



240525    18:03:17    

BREAKING some stuff (commonlists, make universe)

def Word_Pastmosts(word):
   if 'pastmosts' in G_included_levels:
      return SQL_WORD_Pastmosts_CACHE_Select(word)
   else:
      return Poset_Ulteriormosts(Word_Pastwardposet(word))

SQL_Table('SQL_WORD_Pastmosts', ['WORD'], 'pastmosts')

      results.append((SQL_WORD_Pastmosts, word, pastward_poset.nonstrict_ulteriors(word)& pastmosts))
to_do_1 = {'generator source':SQL_WORD_Direction_GivenRefWords, 'function':FUN, 'target tables':[SQL_WORD_Strictpastwardset, SQL_WORD_Pastmosts],} 

      results.append((SQL_WORD_Strictpastwardset, word, pastward_poset.strict_ulteriors[word]))
      o




next: get rid of RAM_all_pastmosts.update(pastmosts) ??

RAM_all_pastmosts = set()
   RAM_all_pastmosts.update(pastmosts)
to_do_2 = OBJECT_TABLE_TODO(RAM_all_pastmosts, SQL_AllPastmosts)

   pastmosts = Poset_Ulteriormosts(pastward_poset)

240525    18:55:53    

def Sense_Pastwardsenseposet(sense):
   print(f'pastward senseposet of: {sense}. ', end='')
   if 'sense_pastwardsenseposet' in G_included_levels:
      return SQL_SENSE_Pastwardsenseposet.Select(sense)
   else:
      return SensePoset_Senseposet(sense, Word_Pastwardposet(sense.word))




240525    19:04:10    
def WordsDirection_Closureposet(words, direction):
   return Poset(WordsDirection_UlteriorclosureGivens(words, direction), direction)


240525    19:29:03    
#to_do_2 = {'generator source':ALL_MENTIONERS.items(), 'function':lambda x,y: [(x,y)], 'target tables':[SQL_WORD_AllMentioningWords], 'message': 'writing '}

for todo of:
exec(open('src/precomputing/O_KN_sense_pastwardsenseposet.py').read())
# wrong format
#GLOBALCACHES['SQL_WORD_ImmediateFuturewardset_CACHE_Select'].items()

240525    19:32:57    
k1,k2 = AssertTablesEqual(SQL_WORD_Direction_Coveringsenses, SQL_WORD_Direction_Coveringsenses_tweak)
k1== k2
obj_diff(k1,k2)

exec(open('src/radix.py').read())
k1,k2 =Table_AssertTweakEqual(SQL_SENSE_Pastwardsenseposet)
obj_diff(k1,k2)

240526    18:08:54    
to_do_3 = OBJECT_TABLE_TODO(MENTIONERS, SQL_WORD_LinkerSenses)

CALL([to_do_1, to_do_2, to_do_3])
MENTIONERS = {}
def Register_Mentioners(sense, link):
   for word in link:
      MENTIONERS.setdefault(word, set()).add(sense)
   [Register_Mentioners(sense, link) for link in future_wordlinks]

def FUN(word, parsed):
#   WIPE_GLOBALCACHES()
#   GLOBALCACHES['SQL_WORD_Parsed_CACHE_Select']= {(word, ): parsed}
   return [(word, Word_AllRealGivenSenses(word))]


def Word_AllRealGivenSenses(word: Word):
   if 'real_given_senses' in G_included_levels:
      result = SQL_WORD_RealGivenSenses_CACHE_Select(word)
      return result if result else set() 
   else:
      return {Sense(word, sensenum) for sensenum in Word_Parsed(word).keys()
           if sensenum != 0 or any(token['kind'] == 'reference' for _,v in Word_Parsed(word)[0].items() for token in v)}

240526    18:48:17    
def Metadata():
   return Paragraph(Color(Legend_color, 'Metadata:') + Color(Legend_color, ' levels: ' + str([level for level in G_all_levels if level in G_included_levels])))


def Sense_Displaycontent(sense):
   if 'html_caches' in G_included_levels:
      result = SQL_SENSE_HTMLs.Select(sense)
      if result != None:
         return result
   return Cognates_Displaycontent(Sense_Cognates(sense))


G_all_levels = ['xml', 'parsed', 'links_refs', 'pastwards_futurewards', 'pastmosts', 'futureposets', 'real_given_senses', 'sense_pastwardsenseposet', 'futureward_senselangposet', 'strict_futures', 'html_caches']
def cachelevel_setcachelevels(level):
   global G_included_levels 
   G_included_levels = set(G_all_levels[:G_all_levels.index(level)+1])

#cachelevel_setcachelevels('pastwards_futurewards')
#cachelevel_setcachelevels('pastmosts')
#cachelevel_setcachelevels('futureposets')
#cachelevel_setcachelevels('real_given_senses')
#cachelevel_setcachelevels('sense_pastwardsenseposet')
cachelevel_setcachelevels('futureward_senselangposet')
#cachelevel_setcachelevels('strict_futures')
#cachelevel_setcachelevels('html_caches')

find . -type f -exec wc -l {} +

240526    21:37:26    
notes from parsearticle.py:
# terminology 
# lang: language code. e.g. "en"
# lang_name: language name. plain text name of the language, e.g. "English". this is what shows up in xml files, and hence also in parsed dictionaries
# Langcodes: global dictionary that converts language names to language codes
# Codelangs: global dictionary that converts language codes to language names
# wordrawtext: this is a string taken from the raw xml dump that represents a word. wordrawtexts should only be interacted with when parsing from xml; tokens should have wordtexts instead of wordrawtexts. but the raw text saved in a token representing a reference will have the wordrawtext
# wordtext: this is a string converted to wiktionary's standard representation of a word, via a LUA function. this may not do very much, but for example removes some macrons on some latin words. 
# Word: see classes.py . this is an object with a wordtext and a lang
# wordtitle: this is the name of the article for the word, showing up between <title>wordtitle<\title> tags in the xml dump. this shouldn't be used except when interacting with the database (edit: only for getting raw XML; everything else should be wortexts). this should be basically the same as wordtext, except that for reconstructions the format is expanded, see e.g. 
#https://en.wiktionary.org/wiki/Reconstruction:Proto-Indo-European/web%CA%B0-
# where in general the wordtitle is Reconstruction:Proto-Indo-European/web- , i.e. everything that comes  after https://en.wiktionary.org/wiki/ 


#                  {title:redirect for title, redirect in SQL_Wordtitle_Redirect.AllRows()})


#https://en.wiktionary.org/wiki/Wiktionary:Entry_layout#Part_of_speech
part_of_speech_headers =  ['Adjective', 'Adverb', 'Ambiposition', 'Article', 'Circumposition', 'Classifier', 'Conjunction', 'Contraction', 'Counter', 'Determiner', 'Ideophone', 'Interjection', 'Noun', 'Numeral', 'Participle', 'Particle', 'Postposition', 'Preposition', 'Pronoun', 'Proper noun', 'Verb']
#unused parts of speech:
"""
Circumfix, Combining form, Infix, Interfix, Prefix, Root, Suffix
Diacritical mark, Letter, Ligature, Number, Punctuation mark, Syllable, Symbol
Phrase, Proverb, Prepositional phrase[22]
Han character, Hanzi, Kanji, Hanja
Romanization
Logogram
Determinative
"""


#basicallly the point is that pos with inflection starts a new sense (unless at the beginning of etym or root sense), and absorbs pos that doesn't have inflection; root sense starts new sense unless it is immediately after an etym
"""
Sense =  Zero_sense | POS_inflection_sense | Root_sense | Etymology_sense 
POS = POS_inflection | POS_not_inflection
inert = (non POS/Root/Etym) 
nonstarter = inert | POS_not_inflection
Zero_sense = Start +  nonstarter *
POS_inflection_sense = POS_inflection + nonstarter *
Root_sense = Root + nonstarter * 
Etymology_sense = Etymology + inert * + followed_by_Etym |  Etymology + inert * + POS_inflection_sense |  Etymology + inert * + Root_sense
"""

# ... could do an intial chomp if issensestart or ispart of speech. then really discard the initial thing, and then proceed; so we get a sense at the beginning if there's like Noun (not inflection) at beginning. 

def Tokens_Langdict(tokens):
   do_Tokens_MarkLookaheads(tokens)
   token_lists = ListPredicate_Split(tokens, isSenseStarter)
   [token.pop('inflection next') for token_list in token_lists for token in token_list]


#https://en.wiktionary.org/wiki/Category:Layout_templates
#https://en.wiktionary.org/wiki/Category:Column_templates
#generally, seems like column templates are the only relevant things. and things that are top or bottom are like {der-top} stuff {der-bottom} and therefore irrelevant.
#'derived terms' apparently only shows up like 50 times..? https://en.wiktionary.org/wiki/Template:derived_terms
# des-top / des-bottom generally sandwich things with {{desc| anyway
# all the ones past col seem irrelevant.
# wait. actually the ders are relevant. they're like sort of deprecated but they show up everywhere. 
columns_templates = {'col', 'col-auto', 'col-u', 'col1', 'col1-u', 'col2', 'col2-u', 'col3', 'col3-u', 'col4', 'col4-u', 'col5', 'col5-u', 'der2', 'der3', 'der4'}

#   ref = COMPARE(excise_substrings, new_excise_substrings, ref, '{{', '}}')

# really would want to replace this with wiktionary source code :(
# what about the wiktionary parsed xml dumps?? there's no such thing?? (i.e. the html from the webpages already rendered.....)
#https://en.wiktionary.org/wiki/Category:Modules
#https://www.mediawiki.org/wiki/API:Client_code#Python
#https://en.wiktionary.org/wiki/Wiktionary:Parsing
#https://stackoverflow.com/questions/3364279/has-anyone-parsed-wiktionary/35384254
#https://en.wiktionary.org/wiki/Wiktionary:Entry_layout
#https://hewgill.com/journal/entries/343-the-abomination-of-mediawiki-templates.html
#https://en.wiktionary.org/wiki/Module:parameters
#https://en.wiktionary.org/wiki/Module:category_tree/templates

# TODO what to do with these archaic / obsolete forms? question is whether they really show up in main words. like does an english entry point to a guy, who says he's an obsolete form of another guy , who then points to the real thing. 
# should obsolete form of or speling of or archaic form  or dated form or archaic spelling of be trearted like this? it points futurewards, not backwards!
# Wait! can't include alternate form of, right? or maybe we can ?  where does it appear?
# EXCLUDE! cognate, descendant, link , mention. also backformation? 




### these are NOOOOOTTTTT pastward etymological links. for example see:
# comparative: https://en.wiktionary.org/wiki/maior#Portuguese "comparative". 
# superlative : https://en.wiktionary.org/wiki/moins
#['ellipsis of'], 
#bad because e.g.  {{lb|en|transitive|slang}} {{ellipsis of|en|curb stomp}}; just creating a bunch of loops for little benefit
#['romanization of'],  maybe should include but createing a lot of links for not strong benefit for my use case
#['attributive form of'], meh. loops for no reason probably? 
#['contraction of', 'contr of']. exclude because the wordtexts are weird, eg {{contraction of|en|[[get]] [[your]]}} ; added complication, litlte benfeit
#note: possible should include these . reason: there could be associated futurewards, like in desc or derived terms, which should only come from them.... i mean probably they ... .idk. 
# abbreviation of: yes, i guess? actually nah.... similar to ellipsis, eg https://en.wiktionary.org/wiki/free

# genitive: deprecated
# definite singular of: deprecated

#YES:
# gerund: YES, apparently, this is good pastwards
#masculine plural past participle of: YES
#feminine singular past participle of: YES
#female equivalent of: Yes 
#feminine plural past participle of: Yes, i think (metre / mesas occitan?)
#misspelling of: yes i think
#past participle of: yes i think
#adj form of: yes
#masculine plural of: yes
# GUESSING YES (or deprecate):
#['feminine singular of'],  ['feminine plural of'],  ['participle of'], ['present participle of'], ['verb form of'], ['plural of'], 
#['dative plural of'], , ['dative of'] ,  ['noun form of'], 
#['contraction of', 'contr of']]
#yes : ['inflection of', 'infl of'],    
#yes: ['diminutive of', 'dim of'], 
#yes, i guess...['initialism of', 'init of']

#(moved) (yes probably ) should these really be conflated??: ['desc', 'descendant', 'desctree', 'descendants tree']. yes i think so based on the template pages

#hm..... i dk what to do. if including intiialism, should maybe also include Phrase and Symbol as part of speech headers. .... 

#TODO rewrite to be faster. use sets to check membership; use a dict to find the kind name. or just a dict, and the keys are a set?

def lists_firstsdict(lists):
   return {k:ll[0] for ll in lists for k in ll}

## common form
#   for kindnames in all_common_reference_forms:
#   if kind in kindnames:
   if kind in G_refdict_common_forms:
      d['lang'] = args[1]
      d['wordtexts'] = [args[2]]
      d['kind'] = G_refdict_common_forms[kind]
## main etymons
#   for kindnames in [ ['inherited', 'inh', 'inh+'], ['derived', 'der', 'der+'], ['borrowed', 'bor', 'lbor', 'bor+'], [ 'calque', 'clq', 'cal',], ]:
#      if kind in kindnames:
   elif kind in G_refdict_main_etymons:
      d['inheritor lang'] = args[1]
      d['lang'] = args[2]
      if ',' in d['lang']:
         #having multiple langs is extremely rare, less than 1 in 50k worddicts; annoying to make langs a list or return a list of refs. 
         d['lang'] = d['lang'].split(',')[0]
      d['wordtexts'] = [args[3]]
      d['kind'] = G_refdict_main_etymons[kind]

## TODO   with inference i guess
#   if kind in ['etyl']:
#      pass
#         print('ETYL: ' + ref)
#   for kindnames in G_refdict_columns_like:
   elif kind in G_refdict_columns_like:
      d['lang'] = args[1].split('&')[0]
      d['wordtexts'] = [x.split('&')[0] for x in args[2:]]
      d['kind'] = G_refdict_columns_like[kind]
#      if kind in ['noncognate', 'noncog', 'ncog', 'nc']:
##language specific templates
   elif kind in G_refdict_language_specific:
      d['lang'] = G_refdict_language_specific[kind].split('-')[0]
      d['wordtexts'] = [args[1]]
      d['kind'] = G_refdict_language_specific[kind]
   elif kind in G_refset_others:
      if kind in ['root']:
         d['inheritor lang'] = args[1]
         d['lang'] = args[2]
         d['wordtexts'] = args[3:]
         d['kind'] = 'root'
      elif kind in ['form of']:
         d['lang'] = args[1]
         d['wordtexts'] = [args[3]]
         d['kind'] = 'form of'
   #
   ## *fixes
      elif kind in ['suffix', 'suf']:
         d['lang'] = args[1]
         d['wordtexts'] = [args[2]]
         d['affixes'] = ['-'+args[3]]
         d['kind'] = 'suffix'
      elif kind in ['prefix', 'pre']:
         d['lang'] = args[1]
         # note different order
         d['affixes'] = [args[2] + '-']
         d['wordtexts'] = [args[3]]
         d['kind'] = 'prefix'
      elif kind in ['affix', 'af']:
         d['lang'] = args[1]
         d['affixes'] = [x for x in args[2:] if '-' in x]
         d['wordtexts'] = [x for x in args[2:] if '-' not in x]
         d['kind'] = 'affix'
      elif kind in ['confix', 'con']:
         d['lang'] = args[1]
         ws = args[2:]
         d['affixes'] = [ws[0] + '-', '-' + ws[-1]]
         d['wordtexts'] = ws[1:-1]
         d['kind'] = 'confix'

   ##related words
      elif kind in ['also']:
         d['lang'] = 'unknown'
         d['wordtexts'] = args[1:]
         d['kind'] = 'also'
      elif kind in ['alter', 'alt']:
         d['lang'] = args[1]
         d['wordtexts'] = args[2:]
         d['kind'] = 'alter'



# don't use unless needed, and redo following. breaks assumption that refs are wellformed
#      if kind in ['see descendants', 'see desc']:
#         d['kind'] = 'see descendants'

#      if kind in ['noncognate', 'noncog', 'ncog', 'nc']:
##language specific templates


def TextReftokens(text):
   tokens = []
   depth = 0
   token = ''
   for x in re.split('({{|}})', text):
      if x == '{{':
         if depth > 0:
            token += x
         depth += 1
      elif x == '}}':
         depth -=1
         if depth > 0:
            token += x
         elif depth == 0:
#            maybe just: right here, we check if the type is one of those types like col4, and if so, instead of adding it, we will parse its contents as a series of tokens.... or something? there's all those ||
#TODO use takewhile or similar, instead of splits...
            if token.split('|')[0] in columns_templates:
#               print(token)
               tokens.extend(TextReftokens('|'.join(token.split('|')[1:]))) 
#               https://en.wiktionary.org/wiki/wer#Old_English =====Derived terms===== {{col3|ang|werwulf|weorold|werl|werield}}
# hacky, but: we sort of double up. we first add the stuff as though its text, and this gets nested-contained refs. then we parse the col3 thing as though it's a regular ref, and this gets unnested col refs....
            tokens.append({'kind': 'reference', 'content': ReftextRefdict(token)})
            token = ''
         else:
#            unfortunately occasionally wikt pages are misformatted eg with extra }}; so we allow this to happen and will probably get some nonsense stuff, but it's better than not having the page at all.
            Log('Reached negative depth parsing: ' + text)
            depth = 0
      else:
         token += x
         if depth == 0:
            tokens.append({'kind': 'text', 'content': token})
            token = ''
   return tokens





240529    21:52:33    
https://en.wiktionary.org/wiki/Template:col
rg --no-ignore "<id:" -B 5 -A 5
id: sense ID; see {{senseid}}
apparently this LITERALLY never happens.


240530    20:49:52    
#to get all parent nodes:
# include:
#<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
#use:
#$(x).parents()


def Json_Inspection(json):
   word_json = json['inspect_word']
   word = Json_Word(word_json)

   starting_sense = Json_Sense(json['starting_sense'])
   senseposet = Sense_Pastwardsenseposet(starting_sense)
   if word not in senseposet.immediate_ulteriors:
      return 'not in pastward'

   result = ''
   result += Element('div', 'included senses: ' + ', '.join(str(x) for x in sorted(Json_Sensenums(word_json))), style='white-space: nowrap')
   result += Element('div', 'excluded senses: ' + ', '.join(str(x) for x in sorted(Json_ExcludedSensenums(word_json))), style='white-space: nowrap')

#   wordposet = Word_Pastwardposet(word)
#def SensePoset_Senseposet(sense, poset):
   for maybe_prior_word, ulteriors in senseposet.immediate_ulteriors.items():
      if word in ulteriors:
         result += WordWordDirection_InspectionHTML(word, maybe_prior_word, senseposet.direction)
   result += WordSenseposet_CoveringsensesHTML(word, senseposet)
#   senseposet.includedSenses[word]
   return result


240531    20:09:05    
#pip install -U Flask

#   json_data = request.json

@app.route('/inspection/', methods=['POST',])
def serve_inspection():
   post_data = request.get_data()
   json_data = ujson.loads(post_data.decode('utf-8'))
   return Json_Inspection(json_data)
#      return mime_send_file(PageDir + '/index.html')
#      SelfText_Send(self, Json_Inspection(json_data))



##      args = [unquote(x) for x in self.path.split('/')[1:]]
#
#      content_length = int(self.headers['Content-Length'])
#      post_data = self.rfile.read(content_length)
#      json_data = ujson.loads(post_data.decode('utf-8'))
#      SelfText_Send(self, Json_Inspection(json_data))


#    elif request.method == 'POST':
#        return f'POST request for {param1} {param2}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=80,  threaded=False)



240531    23:36:20    

#   Displaycontent_HTML(Sense_Displaycontent(sense))

#def Sense_Displaycontent(sense):
#   return Cognates_Displaycontent(Sense_Cognates(sense))

#   Write(result, RadixRootdir + PrintingFile)
PrintingFile = 'src/page/index.html'

240601    00:20:54    

import markdown
def Markdown_HTML(markdown_text):
   return markdown.markdown(markdown_text,) 

def Read(file, read_type='r'):
   with open(file, read_type) as f:
      return f.read()


t = Read('src/printing/homepage.md')
k= Markdown_HTML(t)


240603    01:04:32    
#   return markdown.markdown(markdown_text)

#def Homepage():
#   html_content = Markdown_HTML(Read(Sourcedir + 'page/home.md'))
#   return Displaycontent_HTML({'HTML_CONTENT':html_content, 'JAVASCRIPT_CONTENT':'', 'TITLE':'radix'})
#





#def Aboutpage():
#   html_content = Markdown_HTML(Read(Sourcedir + 'page/_about.md'))
#   return Displaycontent_HTML({'HTML_CONTENT':html_content, 'JAVASCRIPT_CONTENT':'', 'TITLE':'about radix'})
#
#def Futurepage():
#   html_content = Markdown_HTML(Read(Sourcedir + 'page/_future.md'))
#   return Displaycontent_HTML({'HTML_CONTENT':html_content, 'JAVASCRIPT_CONTENT':'', 'TITLE':'future of radix'})

#@app.route('/_about', methods=['GET', ])
#def serve_aboutpage():
#   return send_html(Aboutpage())
#
#@app.route('/_future', methods=['GET', ])
#def serve_futurepage():
#   return send_html(Futurepage())

#@app.route('/<path:re("_.*")>', methods=['GET'])
#def handle_underscore_path(path):
#   return send_html(pagename_html(path))
#

#      logged_Cognates(wordtext, options.get('lang', 'en'), options.get('sensenum', 1), callers=True, num_to_show=50)
#      return mime_send_file(PageDir + '/index.html')

240603    02:26:54    

    `<a href="/boop">example.com/boop</a>`
  const placeholders = document.getElementsByClassName('rlink');

       Array.from(placeholders).forEach((element) => {

  const originalText = element.textContent;


    element.innerHTML = `<a href="/wish">radix.ink/wish</a>`;

//             `<button data-parent_id="${element.id}" class="link-button ${kind}-button"></button>`;

  });

240603    12:07:03    
   .page-button::after {
    content: '';
    display: block;
    width: 100%;
    border-bottom: 1px solid white;
}


240603    19:23:32    
if we want to use this, we should make it an option in the python probably

document.addEventListener('mouseover', function(event) {
    var targetElement = event.target;
    console.log('Mouse entered:', targetElement);
});

document.addEventListener('mouseout', function(event) {
    var targetElement = event.target;
    console.log('Mouse left:', targetElement);
});


240605    07:24:02    

#TODO make button panel= of pages.
def Button_panel():
#   return ''.join(f'<a href="_{page}" class="page-button" >{page}</a>' for page in G_page_names)  + '<div class="horizontal-line"></div><br>' 
   return ''.join(f'<a href="/_{page}" class="page-button" style="display: inline-block;">{page}</a>' for page in G_page_names)  + '<div class="horizontal-line"></div><br>' 
#   return ''.join(f'<button class="page-button" >{page}</button>' for page in G_page_names)  + '<div class="horizontal-line"></div><br>' 


exec(open('src/radix.py').read())

word  = Word('value', 'en')
sense = Sense(word, 1)
p = Sense_Pastwardsenseposet(sense)

f = SQL_WORD_ImmediateFuturewardset.Select(word)
s = Word_GivensOrPlaceholder(word)
givens = Word_AllRealGivenSenses(word)
SQL_WORD_RealGivenSenses.Select(word)
SQL_WORD_Parsed.Select(word)

SQL_Wordtitle_XML.Select(word.text)

rg    -A 500  -B 5 "<title>below<" WIKTIONARY_FULL_24.05.30.xml

rg    -A 500  "bilooghe" WIKTIONARY_FULL_24.05.30.xml


rg    -A 500  "<title>co-<" WIKTIONARY_FULL_24.05.30.xml


Ensure_usually_excluded_loaded()
s = list(pastmost_excluder_senses)
k=s[0]


pastmost_excluder_senses = map(pastmost_excluder_senseify, filter(lambda x: x.strip() != '' and x.strip()[0]!='#', Read(RadixRootdir + '/src/pastmost_excluders.txt').split('\n')))

def pastmost_excluder_senseify(string):
   args = [x.strip() for x in string.split(',')]
   wordtext = args[0]
   lang = 'en' if len(args)<2 else args[1]
   sensenum = 1 if len(args)<3 else args[2]
   return Sense(Word(wordtext, lang), sensenum)

240605    08:44:15    
notes on an abberration:
this edit shows up in the xml dump
https://en.wiktionary.org/w/index.php?title=below&oldid=79233682
as basically nothing:
rg    -A 100  -B 5 "<title>below<" WIKTIONARY_FULL_24.05.30.xml

4521327-  <page>
4521328:    <title>below</title>
4521329-    <ns>0</ns>
4521330-    <id>51479</id>
4521331-    <revision>
4521332-      <id>79233682</id>
4521333-      <parentid>78813002</parentid>
4521334-      <timestamp>2024-05-13T08:01:11Z</timestamp>
4521335-      <contributor>
4521336-        <username>P. Sovjunk</username>
4521337-        <id>4102084</id>
4521338-      </contributor>
4521339-      <model>wikitext</model>
4521340-      <format>text/x-wiki</format>
4521341-      <text bytes="13093" />
4521342-      <sha1>ga4830ugrai5hkottafbvpup9m15lxj</sha1>
4521343-    </revision>
4521344-  </page>

just goes on to next stuff:
521345-  <page>
4521346-    <title>palette</title>


word  = Word('co-', 'la')
sense = Sense(word, 0)
p = Sense_Pastwardsenseposet(sense)

Ensure_usually_excluded_loaded()
s = list(pastmost_excluder_senses)
k=s[0]

word = Word('-ier', 'fro')

word = Word('-ier', 'fro')
sense = Sense(word, 1)

p = Sense_Pastwardsenseposet(sense)
Senseposet_Ulteriormostsenses(p)

240605    17:39:01    
SQL_Table('SQL_WORD_FuturePoset', ['WORD'], 'future_poset')


rg    -A 100  -B 5 "<title>-ier<" WIKTIONARY_FULL_24.05.30.xml

Hello. I'm working on a project that involves dealing with the XML dumps of wiktionary. On May 30, I got the latest dump from: https://dumps.wikimedia.org/enwiktionary/latest/enwiktionary-latest-pages-meta-current.xml.bz2
This worked fine, but there was something strange. A number of articles seem to have been nuked. For example, here's the article for "below":

<page>
  <title>below</title>
  <ns>0</ns>
  <id>51479</id>
  <revision>
    <id>79233682</id>
    <parentid>78813002</parentid>
    <timestamp>2024-05-13T08:01:11Z</timestamp>
    <contributor>
      <username>P. Sovjunk</username>
      <id>4102084</id>
    </contributor>
    <model>wikitext</model>
    <format>text/x-wiki</format>
    <text bytes="13093" />
    <sha1>ga4830ugrai5hkottafbvpup9m15lxj</sha1>
  </revision>
</page>

Of course, this isn't the real article. This was the only result of the command

rg    -A 100  -B 5 "<title>below<" WIKTIONARY_FULL_24.05.30.xml

so I think that somehow the "below" article just doesn't show up in this dump. This also happened for "-ier":

<page>
  <title>-ier</title>
  <ns>0</ns>
  <id>79624</id>
  <revision>
    <id>79212250</id>
    <parentid>79212188</parentid>
    <timestamp>2024-05-11T13:10:32Z</timestamp>
    <contributor>
      <username>Jberkel</username>
      <id>1580588</id>
    </contributor>
    <comment>/* Further reading */</comment>
    <model>wikitext</model>
    <format>text/x-wiki</format>
    <text bytes="4242" />
    <sha1>ly9lx1angt2on9stnywb3wxvcattzpk</sha1>
  </revision>
</page>


And I believe a number of other articles. What's going on here? It's possible I'm confused somehow. E.g.:
1. I haven't looked in any standard wiktionary help pages--sorry, I don't know where to look. 
2. Maybe this is intended behavior. 
3. Maybe the "latest" XML dumps often contain weird stuff, and I'm supposed to stick to the bi-monthly dumps. 
4. Something else. 

Thoughts? 


Hello. I'm working on a project that involves dealing with the XML dumps of wiktionary. On May 30, I got the latest dump from: https://dumps.wikimedia.org/enwiktionary/latest/enwiktionary-latest-pages-meta-current.xml.bz2
This worked fine, but there was something strange. A number of articles seem to have been nuked. For example, here's the article for "below":

<page>
  <title>below</title>
  <ns>0</ns>
  <id>51479</id>
  <revision>
    <id>79233682</id>
    <parentid>78813002</parentid>
    <timestamp>2024-05-13T08:01:11Z</timestamp>
    <contributor>
      <username>P. Sovjunk</username>
      <id>4102084</id>
    </contributor>
    <model>wikitext</model>
    <format>text/x-wiki</format>
    <text bytes="13093" />
    <sha1>ga4830ugrai5hkottafbvpup9m15lxj</sha1>
  </revision>
</page>

Of course, this isn't the real article. This was the only result of the command

rg    -A 100  -B 5 "<title>below<" WIKTIONARY_FULL_24.05.30.xml

so I think that somehow the "below" article just doesn't show up in this dump. This also happened for "-ier":

<page>
  <title>-ier</title>
  <ns>0</ns>
  <id>79624</id>
  <revision>
    <id>79212250</id>
    <parentid>79212188</parentid>
    <timestamp>2024-05-11T13:10:32Z</timestamp>
    <contributor>
      <username>Jberkel</username>
      <id>1580588</id>
    </contributor>
    <comment>/* Further reading */</comment>
    <model>wikitext</model>
    <format>text/x-wiki</format>
    <text bytes="4242" />
    <sha1>ly9lx1angt2on9stnywb3wxvcattzpk</sha1>
  </revision>
</page>


And I believe a number of other articles. What's going on here? It's possible I'm confused somehow. E.g.:
1. I haven't looked in any standard wiktionary help pages--sorry, I don't know where to look. 
2. Maybe this is intended behavior. 
3. Maybe the "latest" XML dumps often contain weird stuff, and I'm supposed to stick to the bi-monthly dumps. 
4. Something else. 

Thoughts? 


SQL_Table('ALT_SQL_WordtitleRedirect', [], 'wordtitle_redirect')
SQL_Table('ALT_SQL_Wordtitle_XML', ['wordtitle'], 'raw_article_xml', storage=None)

ALT_WiktionaryXMLfile = 'enwiktionary-20240501-pages-articles.xml'
NORMAL_XML_MODE = True

sql_WiktionaryXMLfullPath = xmlpath_fullxmlpath(WiktionaryXMLfile)
read_WiktionaryXMLfullPath = xmlpath_fullxmlpath(WiktionaryXMLfile if NORMAL_XML_MODE else ALT_WiktionaryXMLfile)


Wiktionary_database = '.'.join(read_WiktionaryXMLfullPath.split('.')[:-1]) + '-' + GetTimestamp() + '.db' 

sql_db_activate(Wiktionary_database)

print('searching for redirects...')
command = """rg --no-ignore -B 5 '<redirect title="' """ + THIS_WiktionaryXMLfullPath
result = subprocess.run(command, shell=True, capture_output=True, text=True).stdout


to_do_1 = OBJECT_TABLE_TODO(GWordtitle_Redirect, SQL_WordtitleRedirect if NORMAL_XML_MODE else ALT_SQL_WordtitleRedirect, single_value=True)

   with open(read_WiktionaryXMLfullPath, 'r') as file:


to_do_1 = {'generator source':(9450000, Wiktionaryfile_Generator()), 'function':FUN, 'target tables':[SQL_Wordtitle_XML if NORMAL_XML_MODE else ALT_SQL_Wordtitle_XML], 'printing_args':False}

precompute_logFile = RadixRootdir + 'log/precompute/LOG' + GetTimestamp() +'.txt'
ensure_file_exists(precompute_logFile)


240609    20:22:20    
   } else if (target.classList.contains('radix-button')) {
      EventButtonkind_click(event, 'radix');
//      const parent_data = document.getElementById(target.dataset.parent_id).dataset;
//      window.open(parent_data.radix_url_stem, '_blank');
   } else if (target.classList.contains('wiktionary-button')) {
          if (event.shiftKey) {
             window.location.href = document.getElementById(target.dataset.parent_id).dataset.wiktionary_url;
          } else {
            window.open(document.getElementById(target.dataset.parent_id).dataset.wiktionary_url, '_blank');
          }
   } else if (target.classList.contains('etymonline-button')) {
      window.open(document.getElementById(target.dataset.parent_id).dataset.etymonline_url, '_blank');
   } else if (target.id !== 'universal_hover_popup' && target.id !== 'universal_click_popup' && !(target.id.startsWith('word_element')) ) {
      document.getElementById("universal_click_popup").style.display = "none";
      document.getElementById("universal_hover_popup").style.display = "none";
    }});

240609    22:01:24    

      const wikt_buttons = document.getElementsByClassName('ReplaceWiktionaryButton');
      Array.from(wikt_buttons).forEach((element) => {
               element.textContent = ''; 
               element.classList.add('wiktionary-button','link-button');
      });



240610    01:10:45    

* BX STL lightgreen; Piece of false information STL BX--by showing e.g. a wrong definition or wrong language for a word.

240610    01:38:47    

Here are ways that the radix page for word W can be wrong. 
* BX STL lightgreen; False cognate STL BX--by showing another word V in a list of cognates of W and/or in a pastward or futureward cognate poset of W, when in reality V is not a cognate of W. 
* BX STL lightgreen;Missing cognate STL BX--by failing to list a true cognate V. 
* BX STL lightgreen; Misleading sense STL BX--by showing a word V as a cognate because V has a sense S that's cognate with W, but S is a surprising, uncommon, or obscure sense of V that is homographic with a more common sense Z of V that is *not* cognate with W.
* BX STL lightgreen; Overconfident cognate STL BX--by showing a word V that is only possibly, plausibly, perhaps cognate with W.
* BX STL lightgreen; Non-existent word STL BX--by showing a word V that is not actually a word, e.g. it is misspelled or contains markup or punctuation.
* BX STL lightgreen; False descent STL BX--by showing V as descending from another word U, when in reality V does not descend from U.
* BX STL lightgreen; Missing descent STL BX--by failing to show V as descending immediately from U, when in reality U is the most recent ancestor of V.


| In many cases, the page will have a: | Which means that the page: | 
| --- | --- |
| January  | $250    |
| February | $80     |
| March    | $420    |


240610    05:57:41    

#to_do_1 = {'generator source':[(get_commonwords(), )], 'function':lambda x:[(x, )], 'target tables':[SQL_CommonEnglishWords], 'message': 'writing '}

240613    17:15:43    



#already_included_color_string = 'rgb(230, 160, 50)'
def WordParentid_AlreadyincludedElement(word, parent_id):
#   return already_included_mark
   dataset = {'wordtext':word.text,'lang':word.lang, 'parent_id': parent_id}
#   styles = {'color':already_included_color_string}
#   styles = {'outline':f'2px solid rgb{Gray(240)}', 'color':f'rgb{(230, 160, 50)}'}
   content = already_included_mark
   return  ' ' + f'<span class="jump-button button" {Dataset_String(dataset)}  onmouseover="handleMouseover(event)" >{content}</span>' 
#   return  ' ' + f'<span class="jump-button button" {Dataset_String(dataset)} {Styles_String(styles)} onmouseover="handleMouseover(event)" >{content}</span>' 


#   return '<p class="header">{text}<p>'
def Legend():
#   global G_Cognates_printing
#   G_Cognates_printing = {'trunk':Poset({}), 'tree':Poset({})}
#   global ELEMENT_ID
#   ELEMENT_ID = 0
#   global G_direction_wordlang_firstID
#   G_direction_wordlang_firstID = init_IDdict()

   return result + Paragraph('<br>'.join(x[0]+x[1] for x in things)) 
#+ f'<script>const js_direction_wordlang_firstID = {G_direction_wordlang_firstID}; </script>'

   sense = WordtextLangSensenum_Realsense(wordtext, lang, sense_num) 
   display_content = Cognates_Displaycontent(SenseCoglangs_Cognates(sense, Coglangs))
#   try:
#      display_content = Cognates_Displaycontent(SenseCoglangs_Cognates(sense))
#   except:
#      display_content = Sense_ErrorDisplaycontent(sense)
   return Displaycontent_HTML(display_content)

def Cognates_Displaycontent(cognates_given):
#   global ELEMENT_ID
#   ELEMENT_ID = 0
#   global G_direction_wordlang_firstID
#   G_direction_wordlang_firstID = init_IDdict()
   global G_Cognates_printing
   G_Cognates_printing = cognates_given 
   print(G_Cognates_printing['trunk'].non_generally_excluded_domain())

def Info():
   result = Text_Legendheader('Information: ')
   return Warning_message() + Alpha_disclaimer() + Usage_notes() + Legend()

def Warning_message():
   return Paragraph(Color((255,0,0), 'WARNING: many words found by radix will not actually be related to the given word. radix is a tool for DISCOVERING *plausible* Cognates. To verify, look at wiktionary and its sources.'), style='font-size: 16px')

def Alpha_disclaimer():
   return Paragraph(Color((205,50,50), 'NOTE: this is a (pre-)alpha version of radix. Everything is subject to errors and change. PLEASE DO NOT POST THIS ANYWHERE! The server isn\'t ready '))

def Usage_notes():
   return Paragraph(Color(Darken(Legend_color, 1.2), 'The URL format as of 18 Apr 2024 is: http://radix.ink/word&lang:en/thing . in general: http://radix.ink/word&lang:X/wordtext. where X is the wiktionary language code of the language of the word https://en.wiktionary.org/wiki/Wiktionary:List_of_languages , and wordtext is the text of the word. if the language is english, the code isn\'t needed. currently you cannot specify senses; radix automatically uses the first sense. that will be enabled eventualy. i will add a legend / more explanation of what\'s going  on at some point. '))


#   html_content += Info()

   javascript_content = f'''
   const js_starting_sense = {{ wordtext: "{starting_word.text}",
                        lang: "{starting_word.lang}",
                        sense: "{starting_sense.num}",
}};
   '''
#   const js_direction_wordlang_firstID = {G_direction_wordlang_firstID};

#def IDnumber_HTMLid(ID_number):
#   return f'word_element_{ID_number}'

def Word_Element(word, info):
#   ID_number = NewIDnumber()
   HTMLid = NewIDnumber()
#   HTMLid =  IDnumber_HTMLid(ID_number)
#   HTMLid = f"word_element_{info['kind']}"
   dataset = {'wordtext':word.text,'lang':word.lang,'senses_included':WordInfo_IncludedSensenumsJSON(word, info), 
              'senses_excluded':WordInfo_ExcludedSensenumsJSON(word, info), 'include_jump_down':True,
           'click_url':Word_URL(word), 'wiktionary_url':Word_URL(word), 'radix_url':WordInfo_RadixURLstem(word, info)}
   if word.lang == 'en':
      dataset['etymonline_url'] = Word_EtymonlineURL(word)
   styles = {'color':f'rgb{LangCol(word.lang)}', 'z-index': 999, 'position': 'relative'}



   if info['kind'] == 'tree':
      dataset['include_jump_down'] = False
      include_lang_element = info['with_lang']
      include_definitions_element = True
      if info['generally_excluded']:
         color_highlightStyles(Gray(150))
         #TODO make this transport you. also, maybe use a different symbol for trunk thinhgs. in general add a button taht takes you to the first tree instance. but have to figure out how to track the IDs of the first instances in the tree.... also want button from trunk to tree. 
      dataset['direction'] = info['direction']
      include_alreadyincluded_element = info['already_included']
#      if not info['already_included']: 
#         G_direction_wordlang_firstID[info['direction']].setdefault(word.text, {})[word.lang] = HTMLid

240613    22:24:13    
def Alpha_banner():
   return '''<p style="color:red">NOTE: This is an alpha release. <span style="background-color:rgb(120, 50, 50)">PLEASE DON'T SHARE AROUND.</span> The server isn't ready.<p>'''

   contents['HTML_HEADER'] = Alpha_banner() + Button_panel() 

#def init_IDdict():
#   return {'pastward':{}, 'futureward':{}, 'sandbox':{}}

240614    08:46:29    
#   styles = {'color':f'rgb{LangCol(word.lang)}', }

240614    09:27:53    
#   return 'BAD LANG' + lang if lang not in Codelangs else Color(Darken(LangCol(lang), 1.4), ShortenLang(Codelangs[lang]) + " ") 

240614    11:36:24    

def Word_Element(word, info):
   HTMLid = NewIDnumber()
   dataset = {'wordtext':word.text,'lang':word.lang,'senses_included':WordInfo_IncludedSensenumsJSON(word, info), 
              'senses_excluded':WordInfo_ExcludedSensenumsJSON(word, info), 'include_jump_down':True,
           'click_url':Word_URL(word), 'wiktionary_url':Word_URL(word), 'radix_url':WordInfo_RadixURLstem(word, info)}
   if word.lang == 'en':
      dataset['etymonline_url'] = Word_EtymonlineURL(word)

   styles = {}
   classes = ['button', 'word', f'lang-{word.lang}', f'kind-{info["kind"]}']

   include_lang_element = True
   include_definitions_element = False
   include_alreadyincluded_element = False

   def color_highlightStyles(color):
      styles['outline'] = f'1px solid rgb{Darken(color, 1.39)};'
      styles['background-color'] = f'rgba{Darken(color, 2.95)};'

   if info['kind'] == 'etymonline_wordlist':
      include_lang_element = False
      if not info['in_radix_words']:
         color_highlightStyles(G_etymonline_only_color)
         dataset['include_jump_down'] = False
      else:
         color_highlightStyles(G_shared_cognate_color)
      dataset['click_url'] = Word_EtymonlineURL(word)

   if info['kind'] == 'summary_wordlist':
      include_lang_element = False
      if info['in_etymonline_wordtexts']:
         color_highlightStyles(G_shared_cognate_color)

   if info['kind'] == 'summary_wordlist_trunk':
#      styles['outline'] = f'1px solid rgb{Gray(140)}'
      include_definitions_element = True

   if info['kind'] == 'tree':
      dataset['include_jump_down'] = False
      include_lang_element = info['with_lang']
      include_definitions_element = True
      if info['generally_excluded']:
         color_highlightStyles(Gray(150))
      dataset['direction'] = info['direction']
      include_alreadyincluded_element = info['already_included']

   if info['kind'] == 'inspection':
      styles['white-space'] = 'nowrap'

   content = ((Lang_Element(word.lang) if include_lang_element else "") 
            + word.text
            + (Word_DefinitionsElement(word) if include_definitions_element else "")
            + (WordParentid_AlreadyincludedElement(word, HTMLid) if include_alreadyincluded_element else ""))

   return  f'<span class="{" ".join(classes)}" {Dataset_String(dataset)} {Styles_String(styles)} onmouseover="handleMouseover(event)" onmouseout="hidePopup(event, {HTMLid})" id={HTMLid}>{content}</span>' 

240614    17:35:25    
def Styles_String(styles):
   return ' style="' + ';'.join(f'{key}:{value}' for key,value in styles.items()) + '" '


240614    20:39:26    
# should differentiate trunk word vs tree weord? 
def Word_DefinitionsElement(word):
   def_str = Word_Definitionstring(word, G_Cognates_printing['tree'].includedSenses) if LangIsold(word.lang) and 'G_Cognates_printing' in globals() else ''
   def_str = ':' + def_str if len(def_str)>0 else ''
#   return Color(Darken(LangCol(word.lang), 1.3), def_str)
   return f'<span class="def-element">{def_str}</span>' 

240615    09:59:28    
SQL_Table('SQL_SENSE_PastwardsenseposetOLD', ['SENSE'], 'pastward_senseposet')

240615    19:38:16    
+ '<button class="lang-setter">boop </button>' 

   } else if (target.classList.contains('lang-setter')) {
      LangColor_setColors('sa', [240, 180, 230]);

240628    23:09:13    

ok well indeed there are thingies with multiple parents. 
#PIE_langcode_poset 
root = PIE_langcode_poset.domain[0]
PIE_langcode_poset.nonstrict_ulteriors(root) == set(PIE_langcode_poset.domain)

for x in PIE_langcode_poset.domain:
   if len(PIE_langcode_poset.immediate_priors[x])!=1:
      print(x)

things= """
nb
hns
hca
rge
rmd
rme
rmg
rmq
srm
sth
rmu
trl
pml
rmi
crp-rsn
crp-slb
rsb
"""

for x in things.split('\n'):
   if x.strip()!='':
      print(x)
      PIE_langcode_poset.immediate_priors[x.strip()]

240629    18:13:46    
'''
python src/page/server.py
exec(open('src/radix.py').read())
https://en.wiktionary.org/wiki/Category:Proto-Indo-European_language
'''


def Langposet_Restricted(poset):
   return PosetDomain_Restricted(poset, set(poset.domain) - Langposet_Exclusionset(poset), domain_closed=False)

def Langposet_Exclusionset(poset):
   excluded = set()
   for lang in reversed(poset.domain):
      if (poset.immediate_ulteriors[lang] - excluded == set()) and (lang not in Codelangs or all(Codelangs[lang] == Codelangs.get(parent_lang, 'FALSE') for parent_lang in poset.immediate_priors[lang])): 
         excluded.add(lang)
   return excluded


def Langposet_FullExclusionset(poset):
   excluded = set()
   for lang in poset.domain:
      if lang not in excluded:
         for future_lang in poset.strict_ulteriors[lang]:
            if Codelangs[lang] == Codelangs.get(future_lang, Codelangs[lang]):
               excluded.add(future_lang)
   return excluded - {poset.domain[0]}

'''
def Langposet_FullExclusionset(poset):
   excluded = set()
   for lang in reversed(poset.domain):
      if (lang not in Codelangs) or all(Codelangs[lang] == Codelangs.get(parent_lang, 'FALSE') for parent_lang in poset.immediate_priors[lang]): 
         excluded.add(lang)
   return excluded - {poset.domain[0]}
'''

#PIE_langcode_poset_restricted = Langposet_Restricted(PIE_langcode_poset)

'''
exclusion = Langposet_FullExclusionset(PIE_langcode_poset)
PIE_langcode_poset.immediate_priors['hi']

full_to_include = set(PIE_langcode_poset.domain) - Langposet_FullExclusionset(PIE_langcode_poset)
'''

def make_restricted_PIE():
   full_to_include = set(PIE_langcode_poset.domain) - Langposet_FullExclusionset(PIE_langcode_poset)
   restricted_strict_ulteriors = ItemsetDomain_Restricted(PIE_langcode_poset.strict_ulteriors, full_to_include)
#   big_PIE_langcode_poset_restricted = Poset(restricted_strict_ulteriors)

#   merged = OrderFun_Merged(restricted_strict_ulteriors, code_rectified)

#   strict_priors = Order_Reversed(big_PIE_langcode_poset_restricted.strict_ulteriors)
#   super_restricted = OrderFun_Mapped(merged, code_rectified)
   super_restricted = OrderFun_Mapped(restricted_strict_ulteriors, code_rectified)

#   strict_priors = Order_Reversed(restricted_strict_ulteriors)
#   new_domain = unionfold({lang} | strict_priors[lang] for lang in G_Cached_Coglangs)
#   really_restricted_strict_ulteriors = ItemsetDomain_Restricted(restricted_strict_ulteriors, new_domain)
   return Poset(super_restricted)


def code_rectified(lang):
   return Langcodes[Codelangs[lang]]

#def OrderFun_Merged(order, fun):
#   domain = list(order.keys())
#   for x in order:
#      for y in order:
#         if x!=y:
#            if fun(x)==fun(y):

#            assert fun(x)!=fun(y)
 
def OrderFun_Mapped(order, fun):
   mapped = {}
   for k,v in order.items():
      mapped.setdefault(fun(k), set()).update({fun(x) for x in v})

#   for x in order:
#      for y in order:
#         if x!=y:
#            if fun(x)==fun(y):
#               print(x,y)
#            assert fun(x)!=fun(y)
   return mapped


#we want to know if it is the case that anyone 
#Codelangs
#Langcodes




PIE_langcode_poset_restricted = make_restricted_PIE()

def Settings():
   tree = LangUlteriors_PlaceholderWordTraversal('ine-pro', PIE_langcode_poset_restricted.immediate_ulteriors)
   return Paragraph('This is optimized for settings, not for viewing languages. If you want to see the PIE language family tree, look at <a href="https://en.wiktionary.org/wiki/Category:Proto-Indo-European_language">en.wiktionary.org/wiki/Category:Proto-Indo-European_language</a> and click the [show ] button.') + Paragraph(TreeDirection_HTML(tree, 'futureward'), style="white-space: pre; ") 

#   return ClearSettingsButton()+ '\n ' + ''.join(Lang_Colorpicker(lang) for lang in list(PIE_langcode_poset_restricted.domain)[:100])

#   <input type="text" id="userInput">' + Word_Element(Word('boop', 'sa'), info={'kind':'summary_wordlist_trunk'}) + '<input type="color" id="colorPicker" name="colorPicker" value="#ff0000"> '
#   return ' <input type="text" id="userInput">' + Word_Element(Word('boop', 'sa'), info={'kind':'summary_wordlist_trunk'}) + '<input type="color" id="colorPicker" name="colorPicker" value="#ff0000"> '

def ClearSettingsButton():
   return '<button class"clear-settings-button">clear all settings</button>'

def Lang_Colorpicker(lang):
   try:
      return f'<input type="color" id="colorPicker-{lang}" name="{lang}" value="#f2d7e6"> ' + Word_Element(Word('word', lang), info={'kind':'summary_wordlist_trunk'}) 
#      return ' <input type="text" id="userInput">' + Word_Element(Word('word', lang), info={'kind':'summary_wordlist_trunk'}) + f'<input type="color" id="colorPicker-{lang}" name="{lang}" value="#f2d7e6"> '
   except:
      return 'failed from lang ' + lang
#   return ' <input type="text" id="userInput">' + Word_Element(Word('boop', 'sa'), info={'kind':'summary_wordlist_trunk'}) + f'<input type="color" id="colorPicker-{lang}" name="{lang}" value="#ff0000"> '


#def PosetDomain_Restricted(poset, domain, domain_closed=False):
#PIE_langcode_poset




#sigh. ok we have to correct the lang. probably want to exclude guys who have no kids, and whose parent corrects to the same lang.
#shouldn'thave unreal settings.
#what do with bad langs??c"c"""c

def LangUlteriors_PlaceholderWordTraversal(root_lang, immediate_ulteriors):
   info = {'already included': False}
   children = [LangUlteriors_PlaceholderWordTraversal(child_lang, immediate_ulteriors) for child_lang in sorted(immediate_ulteriors[root_lang])]
   return {'node': [{'word':Lang_PlaceholderWord(root_lang), 'info': info}], 'children': children}

def Lang_PlaceholderWord(lang):
   return Word(lang, lang)
#   try:
#      return Word_Element(Word('word', lang), info={'kind':'summary_wordlist_trunk'})
#   except:
#      return 'failed lang ' + lang

240629    22:04:42    
https://stackoverflow.com/questions/38821432/remove-border-around-the-color-in-an-inputtype-color-in-firefox
https://stackoverflow.com/questions/11167281/webkit-css-to-control-the-box-around-the-color-in-an-inputtype-color
https://css-tricks.com/color-inputs-a-deep-dive-into-cross-browser-differences/

240629    23:51:47    

getComputedStyle(document.getElementById('colorPicker-ine-pro')).getPropertyValue('width')
getComputedStyle(document.getElementById('colorPicker-ine-pro')).fontSize

printWidthContributions(document.getElementById('colorPicker-ine-pro'))

getComputedStyle(document.getElementById(231)).fontSize

getComputedStyle(document.getElementById(231)).getPropertyValue('width')

window.getComputedStyle(document.getElementById(231)).width;
document.getElementById(231).offsetWidth
document.getElementById(244).offsetWidth
document.getElementById(247).offsetWidth
Sanskrit word
O.East Slavic word

18*7.23



document.documentElement.style.setProperty(`--scale-factor`, ``);
         console.log(getComputedStyle(document.getElementById('colorPicker-ine-pro')).getPropertyValue('width'));

:root { --scale-factor: 1; }
   transform-origin: bottom left;
   transform: scale(var(--scale-factor));

def Lang_PlaceholderWord(lang):
   return Word(lang, lang)

240630    15:33:30    
   'Old English': 'OE',
   'French': 'Fr',
   'Old French': 'OFr',

240630    20:58:49    
else if (event.key === 'Enter'){
         if (event.target.id === 'userInput') {
            console.log('User typed:', event.target.value);
            let splitArray = event.target.value.split(',').map(function(item){ return parseInt(item.trim(), 10); });
            console.log('got:', splitArray);
            store_update('user_colors', 'sa', splitArray);
//            localStorageProxy.user_colors['sa'] = splitArray;

//            localStorage.setItem('sa', JSON.stringify(splitArray));

            Lang_refreshColor('sa');

         };
      }

240701    01:57:54    

def Lang_ColorSettings(lang):
   return f'<span class="color-picker-wrapper">{Lang_dummy_element(lang)}{Lang_ColorPicker(lang)}<span style="display:none;">bop</span></span>'


240707    23:24:37    
REDOING printing cognates displaycontent
from lxml import etree

def Sense_Displaycontent(sense):
#   display_content = Cognates_Displaycontent(SenseCoglangs_Cognates(sense, Coglangs))
#   sense_poset = SenseCoglangs_Senseposet(sense, Coglangs)

   trunk_poset = Sense_Pastwardsenseposet(sense)


def Cognates_Displaycontent(cognates_given):
   global G_Cognates_printing
   G_Cognates_printing = cognates_given 
   print(G_Cognates_printing['trunk'].non_generally_excluded_domain())

   starting_word = cognates_given['trunk'].domain[0]
   starting_sense = list(cognates_given['trunk'].includedSenses[starting_word])[0]

   html_content = ''
   html_content += Starting_sense(starting_word, starting_sense)
   html_content += Text_Legendheader('Pastward trunk: ')
   html_content += WordUlteriorsAlreadysDirection_Html(G_Cognates_printing['trunk'].domain[0], G_Cognates_printing['trunk'].immediate_ulteriors, set(), 'pastward')
   html_content += Summaries()
   html_content += Text_Legendheader('Futureward tree: ')
   html_content += FutureposetHTML()

   javascript_content = f'''
   const js_starting_sense = {{ wordtext: "{starting_word.text}",
                        lang: "{starting_word.lang}",
                        sense: "{starting_sense.num}",
}};
   '''

   html_content += '<div id="async-content">Loading...</div>'
   return {'HTML_CONTENT':html_content, 'JAVASCRIPT_CONTENT':javascript_content, 'TITLE':Title(starting_sense)}

def Title(starting_sense):
   return '_'.join([starting_sense.word.text, Codelangs[starting_sense.word.lang], str(starting_sense.num), GetTimestamp()])

def FutureposetHTML():
   alreadyIncluded=set() 
   return ''.join(WordUlteriorsAlreadysDirection_Html(word, G_Cognates_printing['tree'].immediate_ulteriors, alreadyIncluded, 'futureward') 
            for word in G_Cognates_printing['trunk'].non_generally_excluded_domain())

def WordUlteriorsAlreadysDirection_Html(word, immediate_ulteriors, alreadyIncluded, direction):
   tree = WordUlteriors_Traversal(word, immediate_ulteriors, alreadyIncluded)
   # this is  aa bbbug. we should show the lang for the first of each lang. see for example 
   #   .ink/weave;  PG *wabnPWG *wabbjan:cause to weave;wrap,entangle ; *wabn ; rfa 
   #localhost   PG *waibijan:wind (around),wrap;move around;turn;wave-PWG *waibijan:move around;turn;waver;go back and forth ; veifa:wave 
   def node_text(node):
      return G_semicolon_element.join(Word_Element(child['word'], info={'kind':'tree','with_lang':i==0, 
               'direction': direction, 'already_included':child['info']['already included'], 
               'generally_excluded':'G_Cognates_printing' in globals() and child['word'] in G_Cognates_printing['trunk'].generally_excluded_domain_set()}) 
              for i,child in enumerate(node))

   return Paragraph(TreeDirection_HTML(tree, node_text), style="white-space: pre; ") 

#def Word_eytmoline
#TODO break this out:
def Summaries():
   restricted_radix_wordtexts = {word.text for word in G_Cognates_printing['tree'].immediate_ulteriors.keys() if word.lang == 'en'}
   etymonline_wordtexts = set() 
   start_word = G_Cognates_printing['trunk'].domain[0]
   result = ''

   if start_word.lang == 'en' and start_word.text in GWordtext_EtymonlineCognateLists:
      for cognate_list in GWordtext_EtymonlineCognateLists[start_word.text]:
         etymonline_wordtexts.update(cognate_list)

   result += Text_Legendheader('Cognates from etymonline: ') 
   result += Paragraph((G_semicolon_element + ' ').join([Word_Element(Word(cognate, 'en'), info={'kind':'etymonline_wordlist', 'in_radix_words': cognate in restricted_radix_wordtexts}) for cognate in English_word_frequency_sort(etymonline_wordtexts)]))

   result += Text_Legendheader('Cognates from radix: ')
   result += Wordlists_Print(etymonline_wordtexts)
   for lang in G_print_summary_langs:
      if lang != 'en':
         result += Text_Legendheader('Words, from ' + Codelangs[lang] + ': ') 
#         result += Wordlists_Print(some posets, [], lang=lang)
         result += "OMIT for space"
   return result

def Wordlists_Print(etymonline_wordtexts, lang='en'):
   acc_radix = ''
   already_included_radix_wordtexts = set()
   for trunk_word in G_Cognates_printing['trunk'].non_generally_excluded_domain():
      acc_trunk = ''
      got_first = False
      for word in English_word_frequency_sort(SQL_WORD_Coglang_Langstrictfutures_CACHE_Select(trunk_word, lang) & G_Cognates_printing['tree'].immediate_ulteriors.keys()):
         if word.text not in already_included_radix_wordtexts:
            already_included_radix_wordtexts.add(word.text)
            if not got_first:
               acc_trunk += Word_Element(trunk_word, info={'kind':'summary_wordlist_trunk'}) + ': ' 
               got_first = True
            else:
               acc_trunk += G_semicolon_element + ' ' 
            acc_trunk += Word_Element(word, info={'kind':'summary_wordlist', 'in_etymonline_wordtexts':word.text in etymonline_wordtexts})
      acc_radix += '' if acc_trunk == '' else Paragraph(acc_trunk)
   return acc_radix

InflectionLinks =  ['superlative of', 'genitive of', 'gerund of', 'definite singular of', 'obsolete form of', 'masculine plural past participle of', 'feminine singular past participle of', 'abbreviation of',  'female equivalent of', 'feminine plural past participle of', 'misspelling of', 'past participle of', 'adj form of', 'masculine plural of', 'feminine singular of', 'alternative spelling of', 'feminine plural of', 'alternative form of',  'participle of', 'present participle of', 'verb form of', 'plural of', 'inflection of',   'form of', 'pt-verb form of', 'ca-verb form of', 'es-verb form of', 'de-adj form of', 'el-form-of-nounadj', 'el-form-of-verb', 'en-past of', 'en-third-person singular of', 'en-superlative of', 'en-comparative of', 'en-archaic second-person singular of', 'en-irregular plural of', 'en-archaic third-person singular of', 'en-simple past of']

def WordParentword_isOnlyInflection(word, parent_word):
   return all(SensedictParentword_isInflection(Sense_Sensedict(sense), parent_word) for sense in Word_AllRealGivenSenses(word))

def WordParentword_isInflection(word, parent_word):
   return any(SensedictParentword_isInflection(sensedict, parent_word) for sensedict in Word_Parsed(word).values())

def SensedictParentword_isInflection(sensedict, parent_word):
   defs = Sensedict_DefinitionsUnprocessed(sensedict)
   for d in defs:
      for tok in d:
         if tok['kind'] == 'reference':
            if tok['content']['kind'] in InflectionLinks:
               if 'wordtexts' in tok['content'] and parent_word.text in tok['content']['wordtexts']:
                  return True
   return False

def ListPredicate_Divide(xs, predicate):
   yes, no = [], []
   [(yes if predicate(x) else no).append(x) for x in xs]
   return yes, no

def ListKeyfun_Divide(xs, keyfun):
   results = []
   remaining = xs
   while len(remaining)> 0:
      key = keyfun(remaining[0])
      sublist, remaining = ListPredicate_Divide(remaining, lambda x: keyfun(x) == key)
      results.append(sublist)
   return results

def WordUlteriors_Traversal(root_word, immediate_ulteriors, alreadyIncluded):
   info = {'already included': root_word in alreadyIncluded, }
   alreadyIncluded.add(root_word)
   if info['already included'] or root_word not in immediate_ulteriors:
      children = []
   else:
      children_candidates = [child_word for child_word in sorted(immediate_ulteriors[root_word]) if not child_word not in immediate_ulteriors]
      children_already_included, children_not_already_included = ListPredicate_Divide(children_candidates, lambda x: x in alreadyIncluded)  
      children_not_already_included.sort()
      children_not_already_included.reverse()
      children_pre_chunked = [WordUlteriors_Traversal(child_word, immediate_ulteriors, alreadyIncluded)
                  for child_word in children_already_included + children_not_already_included]
      initial_alreadys, remaining_children = ListPredicate_ChompUntil(children_pre_chunked, lambda tree: not tree['node'][0]['info']['already included'])
      children = [] if len(initial_alreadys) == 0 else [{'node': [tree['node'][0] for tree in initial_alreadys], 'children': []}]

      for next_lang in ListKeyfun_Divide(remaining_children, lambda tree: tree['node'][0]['word'].lang):

#         lang_children = []
         #todo: can omit, assuming there's never things here
         next_lang_alreadys, next_lang_not_alreadys = ListPredicate_Divide(next_lang, lambda tree: tree['node'][0]['info']['already included'])
         assert len(next_lang_alreadys) == 0, print(root_word, next_lang_alreadys)

         for children_equivalent_list in ListKeyfun_Divide(next_lang, lambda tree: set(node_element['word'] for child_tree in tree['children'] for node_element in child_tree['node'])):
            children.append({'node': [node_element for equivalent_tree in children_equivalent_list 
                             for node_element in equivalent_tree['node']], 'children': children_equivalent_list[0]['children']}) #relies on ListKeyfun_Divide order-preserving

   return {'node': [{'word':root_word, 'info': info}], 'children': children}


G_lxml_parser = etree.HTMLParser()
def html_just_text(html):
   return etree.parse(StringIO(html), G_lxml_parser).xpath('string()')

def TreeDirection_HTML(tree, node_text, stack=[], is_first=True):
   acc = ''
   acc += StackNodeIsfirst_Prefix(stack, tree['node'], is_first)

   text = node_text(tree['node'])
   acc += text
   stack.append({'text': html_just_text(text),  'has_more': True, 'node': tree['node']})
   for i,ch in enumerate(tree['children']):
      if i == len(tree['children'])-1:
         stack[-1]['has_more'] = False
      acc += TreeDirection_HTML(ch, node_text, stack=stack, is_first=(i==0))
   stack.pop()
   return acc

def StackNodeIsfirst_Prefix(stack, node, is_first):
   if is_first:
      if len(stack) > 0:
         return Color(Connector_color, ('' if stack[-1]['has_more'] else '-'))
      else:
         return '' # can add thing, e.g. ,  but would have to also add space to result below for lines below
   else:
      result = ''.join([ len(x['text'])*' ' + ('' if x['has_more'] else ' ')  for x in stack])
      if len(stack)>0:
         result = result[:-1] + ('' if stack[-1]['has_more'] else '')
      return '<br>' + Color(Connector_color, result)

def Sense_URL(sense):
   return Word_URL(sense.word)


def Word_URL(word):
   try:
      return "https://en.wiktionary.org/wiki/" + Word_Wordtitle(word) + '#' + Codelangs[word.lang].replace(' ', '_')
   except:
      return "error from word " + str(word)

def WordInfo_RadixURLstem(word, info):
   sensenums = Senses_Sensenums(WordInfo_IncludedSenses(word, info))
   if len(sensenums)==0:
      sensenums = {x.num for x in Word_GivensOrPlaceholder(word)}
   sensenum = sorted(list(sensenums))[0]
   return f'/word/lang:{word.lang}&sensenum:{sensenum}/{word.text}'

def Senses_Sensenums(senses):
   return {sense.num for sense in senses}

def WordInfo_IncludedSensenumsJSON(word, info):
   return Sensenumset_JSON(Senses_Sensenums(WordInfo_IncludedSenses(word, info)))

def WordInfo_IncludedSenses(word, info):
   try:
      if info['kind'] == 'tree':
         return G_Cognates_printing['trunk' if info['direction'] == 'pastward' else 'tree'].includedSenses.get(word, set()) 
      return (G_Cognates_printing['trunk'].includedSenses.get(word, set()) | 
              G_Cognates_printing['tree'].includedSenses.get(word, set()))
   except: 
      return set()

def WordInfo_ExcludedSensenumsJSON(word, info):
   return Sensenumset_JSON(Senses_Sensenums(Word_GivensOrPlaceholder(word) - WordInfo_IncludedSenses(word, info)))

def Sensenumset_JSON(sensenumset):
   return ujson.dumps(list(sensenumset))

def Word_EtymonlineURL(word):
   return 'https://www.etymonline.com/word/' + word.text

def Sense_Element(sense, info):
   return Color(Gray(150), Word_Element(sense.word, info) + '_' + str(sense.num))

ELEMENT_ID = 0
def NewIDnumber():
   global ELEMENT_ID
   ELEMENT_ID += 1
   return str(ELEMENT_ID)

def Word_Element(word, info):
   HTMLid = NewIDnumber()
   dataset = {'wordtext':word.text,'lang':word.lang,'senses_included':WordInfo_IncludedSensenumsJSON(word, info), 
        'senses_excluded':WordInfo_ExcludedSensenumsJSON(word, info), 'include_jump_down':True,
        'click_url':Word_URL(word), 'wiktionary_url':Word_URL(word), 'radix_url':WordInfo_RadixURLstem(word, info)}
   if word.lang == 'en':
      dataset['etymonline_url'] = Word_EtymonlineURL(word)

   classes = ['button', 'word', 'lang-DEFAULT', f'lang-{word.lang}', f'kind-{info["kind"]}']

   include_lang_element = True
   include_definitions_element = False
   include_alreadyincluded_element = False

   if info['kind'] == 'etymonline_wordlist':
      include_lang_element = False
      dataset['include_jump_down'] = info['in_radix_words']
      classes.append('etymonline_and_radix') if info['in_radix_words'] else classes.append('etymonline_only')
      dataset['click_url'] = Word_EtymonlineURL(word)

   if info['kind'] == 'summary_wordlist':
      include_lang_element = False
      classes.append('etymonline_and_radix') if info['in_etymonline_wordtexts'] else None

   if info['kind'] == 'summary_wordlist_trunk':
      include_definitions_element = True

   if info['kind'] == 'tree':
      dataset['include_jump_down'] = False
      include_lang_element = info['with_lang']
      include_definitions_element = True
      classes.append('generally_excluded') if info['generally_excluded'] else None
      dataset['direction'] = info['direction']
      include_alreadyincluded_element = info['already_included']

   if info['kind'] == 'settings':
      dataset['suppress_popups'] = True
      include_definitions_element = True

   content = ((Lang_Element(word.lang) if include_lang_element else "") 
            + word.text
            + (Word_DefinitionsElement(word, def_str=info.get('def_str', None)) if include_definitions_element else "")
            + (WordParentid_AlreadyincludedElement(word, HTMLid) if include_alreadyincluded_element else ""))

   return  f'<span class="{" ".join(classes)}" {Dataset_String(dataset)} onmouseover="handleMouseover(event)" onmouseout="hidePopup(event, {HTMLid})" id={HTMLid}>{content}</span>' 

# should differentiate trunk word vs tree weord? 
def Word_DefinitionsElement(word, def_str=None):
   if def_str == None:
      def_str = Word_Definitionstring(word, G_Cognates_printing['tree'].includedSenses) if LangIsold(word.lang) and 'G_Cognates_printing' in globals() else ''
      def_str = ':' + def_str if len(def_str)>0 else ''
   return f'<span class="def-element">{def_str}</span>' 

def WordParentid_AlreadyincludedElement(word, parent_id):
   dataset = {'wordtext':word.text,'lang':word.lang, 'parent_id': parent_id}
   content = already_included_mark
   return ' '+f'<span class="jump-button button" {Dataset_String(dataset)} onmouseover="handleMouseover(event)">{content}</span>' 

def Lang_Element(lang):
   return 'BAD LANG' + lang if lang not in Codelangs else f'<span class="lang-element">{ShortenLang(Codelangs[lang])} </span>' 

def Dataset_String(dataset):
   return ' '.join(f'data-{key}="{value}"' for key,value in dataset.items())

240710    04:32:22    
before redo localStorage
<style>

.color-picker-wrapper {
   outline: .9px solid rgb(80,80,110);
   z-index: 9999999;
   position: relative;
   border-top: 8px;
}

.reset-button {
   background: transparent;
}

.reset-button-wrapper {
   position: relative;
}

.reset-button-wrapper-unmodified { display:none; }

.reset-button-wrapper-modified::before {
   content: '';
   position: absolute;
   width: 100%;
   height: 100%; 
   z-index: -1; 
   background-color: rgb(62, 162, 230);
   box-shadow: inset 0 0 3px 2px rgb(192, 212, 255);
}


input[type="color"] {
   font-family: Menlo, monospace; 
   font-size: 12px; 
   width: 21.69px;
   height: 1.1em;
   border: none;
   padding: .2em; 
	-webkit-appearance: none;
   border-right: 3px solid #000;
}

input[type="color"]::-webkit-color-swatch-wrapper {
	padding: 0;
}
input[type="color"]::-webkit-color-swatch {
	border: none;
}

</style>

<script>


function handle_Reset_Click(event){
   if (target.dataset.target_kind == 'lang-color'){ 
      Lang_ResetColor(target.dataset.lang); 
   } else if (target.dataset.target_kind == 'lang-coglang'){ 
      Lang_ResetCoglang(target.dataset.lang); 
   };
}

function Lang_ResetCoglang(lang){
   return;
}

function Lang_setModified(lang){
   var elements = document.querySelectorAll(`.reset-button-wrapper[data-target_kind="lang-color"][data-lang="${lang}"]`);
   for (element of elements) {
      element.classList.add('reset-button-wrapper-modified');
      element.classList.remove('reset-button-wrapper-unmodified');
   }
}

function Lang_setUnmodified(lang){
   var elements = document.querySelectorAll(`.reset-button-wrapper[data-target_kind="lang-color"][data-lang="${lang}"]`);
   for (element of elements) {
      element.classList.remove('reset-button-wrapper-modified');
      element.classList.add('reset-button-wrapper-unmodified');
   }
}


function get_Settings(){
   var settings = localStorage.getItem('settings');
   if (settings === null) { var settings = '{}'; };
   return JSON.parse(settings);
}

function 

const localStorageProxy = new Proxy({}, {
    get: function(target, name) {
        const item = localStorage.getItem(name);
        try {
            return JSON.parse(item);
        } catch (e) {
            return item;
        }
    },
    set: function(target, name, value) {
      if (typeof value === 'object') { localStorage.setItem(name, JSON.stringify(value));}
      else { localStorage.setItem(name, value); }
    },
    deleteProperty: function(target, name) { localStorage.removeItem(name); },
    has: function(target, name) {
        return localStorage.getItem(name) !== null;
    }
});

function store_update(attr, key, value){
   let dict = localStorageProxy[attr];
   dict[key] = value; 
   localStorageProxy[attr] = dict;
}

function store_delete(attr, key){
   let dict = localStorageProxy[attr];
   delete dict[key];
   localStorageProxy[attr] = dict;
}

function Lang_refreshColor(lang){
   LangColor_setColors(lang, localStorageProxy.user_colors[lang]);
}

if (localStorageProxy.user_colors === null) {
   localStorageProxy.user_colors = {};
}


function LangColor_setPickerColor (lang, color){
   var picker = document.getElementById(`colorPicker-${lang}`);
   if (picker) { picker.value = rgbToHex(color); };
}

function rgbToHex(rgb) {
    var r = rgb[0].toString(16).padStart(2, '0');
    var g = rgb[1].toString(16).padStart(2, '0');
    var b = rgb[2].toString(16).padStart(2, '0');
    return '#' + r + g + b;
}

function hexToRgb(hex) {
   hex = hex.replace('#', '');
   var r = parseInt(hex.substring(0, 2), 16);
   var g = parseInt(hex.substring(2, 4), 16);
   var b = parseInt(hex.substring(4, 6), 16);
   return [r,g,b];
}

document.addEventListener('input', function(event) {
   var target = event.target
   var kind = target.dataset.target_kind;
   if (kind === 'lang-coglang'){
      return;
   } else if (kind === 'lang-color'){
      var lang = target.dataset.lang;
      store_update('user_colors', lang, hexToRgb(target.value));
      Lang_setModified(lang);
      Lang_refreshColor(lang);
   };
});

function LangColor_setColors(lang, color){
   Lang_ensureColorsExist(lang);
   document.documentElement.style.setProperty(`--lang-color-${lang}`, `rgb(${color})`);
   document.documentElement.style.setProperty(`--lang-elem-color-${lang}`, `rgb(${Darken(color,1.4)})`);
   document.documentElement.style.setProperty(`--lang-def-color-${lang}`, `rgb(${Darken(color,1.3)})`);
}

function Lang_ResetColor(lang){
   var color = lang in js_color_defaults ? js_color_defaults[lang] : js_color_defaults['DEFAULT'];
   LangColor_setColors(lang, color);
   LangColor_setPickerColor(lang, color);
   store_delete('user_colors', lang);
   Lang_setUnmodified(lang);
}

function Darken(rgb, factor) {
    const [r, g, b] = rgb;
    const darkenedR = Math.floor(r / factor);
    const darkenedG = Math.floor(g / factor);
    const darkenedB = Math.floor(b / factor);
    return [darkenedR, darkenedG, darkenedB];
}

function Lang_setupRules(lang){
   addCSSrule(`:root { --lang-color-${lang}: rgb(230,230,230); } `);
   addCSSrule(`.lang-${lang} {color: var(--lang-color-${lang});}`);
   addCSSrule(`:root { --lang-elem-color-${lang}: rgb(200,200,200); } `);
   addCSSrule(`.lang-${lang} .lang-element {color:var(--lang-elem-color-${lang});}`);
   addCSSrule(`#colorPicker-${lang} {background-color:var(--lang-elem-color-${lang});}`);
   addCSSrule(`:root { --lang-def-color-${lang}: rgb(200,200,200); } `);
   addCSSrule(`.lang-${lang} .def-element {color:var(--lang-def-color-${lang});}`);
}

function addCSSrule(text){
   const sheet = document.styleSheets[0];
   sheet.insertRule(text, sheet.cssRules.length);
}

function Lang_ensureColorsExist(lang){
   const rootStyles = getComputedStyle(document.documentElement);
   const value = rootStyles.getPropertyValue(`--lang-color-${lang}`).trim();
   if (value === ''){
      Lang_setupRules(lang);
   };
}


</script>


pip install gunicorn
$ cat myapp.py
 def app(environ, start_response):
     data = b"Hello, World!\n"
     start_response("200 OK", [
         ("Content-Type", "text/plain"),
         ("Content-Length", str(len(data)))
     ])
     return iter([data])
$ gunicorn -w 4 myapp:app


240711    22:03:15    
def logged_Cognates(wordtext, lang='none given', sense_num='none given', num_to_show=20, callers=False, iterations=None):
   cmd = f"b=Cognates({repr(wordtext)}, {repr(lang)}, {repr(sense_num)})"
   if iterations!=None:
      cmd = cmd[2:]
   my_profile(cmd, log_file=RadixRootdir + 'aux/Cognates_logs.txt', log_string=cmd, num=num_to_show, callers=callers, iterations=iterations, to_print=False)



240712    06:26:05    

in file class.py:

class my_object:
   ....


in file myapp.app:

exec(Read('class.py'))

app = Flask(__name__)

... various routes ...

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=80,  threaded=False)



pickle.loads(object_string)
AttributeError: Can't get attribute 'my_object' on <module '__main__' from '/Users/my_name/.pyenv/versions/3.11.0/bin/gunicorn'>

--preload


@app.before_first_request
def before_first_request():
   sql_db_setup()



240712    06:46:12    

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=80,  threaded=False)


ok so my thing looks like: 

in file class.py:

class my_object:
   ....

in file myapp.app:

exec(Read('class.py'))

app = Flask(__name__)

... various routes ...

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=80,  threaded=False)


this works as a flask app. but when i call it with gunicorn i get: 

AttributeError: Can't get attribute 'my_object' on <module '__main__' from '/Users/my_name/.pyenv/versions/3.11.0/bin/gunicorn'>


this is an error from a line like 

pickle.loads(object_string)

where object_string is a string that's a pickle of an object of type my_object. what's going wrong?



240714    02:07:28    

G_multi_sense_color = (255, 110,100)
G_two_sense_element = Color(G_multi_sense_color, '')
#G_two_sense_element = Color(G_multi_sense_color, 'C')
#G_two_sense_element = Color(G_multi_sense_color, '')
G_poly_sense_element = Color(G_multi_sense_color, '')
#G_poly_sense_element = Color(G_multi_sense_color, '')
#G_poly_sense_element = Color(G_multi_sense_color, '')

really ... we should just move away from the monospace alignment thing :(((

240715    07:33:13    

picking languages. we want:
* IE
* spoken by lots
* has lots of entries
* is interesting
* is a leaf language, out of these


{ 'en', 'la', 'es', 'it', 'ru', 'fr', 'pt', 'de', 'sv', 'gl', 'ca', 'pl', 'nl', 'lv', 'ro', 'el', 'nb', 'sh', 'mk', 'nn', 'grc', 'cs', 'uk', 'enm', 'bg', 'da', 'xcl', 'ga', 'ang', 'is', 'ast', 'hi', 'lt', 'cy', 'hy', 'got', 'fa', 'sa', 'gd', 'non', 'fro', 'sk', 'sq', 'fo', 'bn', 'af', 'dum', 'nrf', 'yi', 'oc', 'cu', 'goh', 'gmh', 'sl', }


https://en.wiktionary.org/wiki/Category:Proto-Indo-European_language
ok so most of these isolated guys are nonsense--fewer than 100 words
but include tocharian: xto, txb
564 xto Tocharian A
2594 txb Tocharian B

13410 sq Albanian

622 hit Hittite

23293 hy Armenian

29796 lt Lithuanian
126889 lv Latvian

https://en.wikipedia.org/wiki/List_of_Balto-Slavic_languages
427786 ru Russian
59617 uk Ukrainian

polish
69812 sh Serbo-Croatian

https://en.wikipedia.org/wiki/Celtic_languages
42773 ga Irish

65763 grc Ancient Greek
82624 el Greek

845446 la Latin
758455 es Spanish
587798 it Italian
401197 fr French
377363 pt Portuguese

373267 de German
154217 nl Dutch
1261187 en English

https://en.wikipedia.org/wiki/North_Germanic_languages
75524 nb Norwegian Bokml
260639 sv Swedish
34540 is Icelandic

https://en.wikipedia.org/wiki/Indo-Aryan_languages
sad not more entries... 19973 fa Persian
could include pashto or kurdish but not so many entries...

30741 hi Hindi


annoying on the settings, and not so many entries...? 12094 bn Bengali
? 23172 got Gothic
? 61208 cs Czech
? 120993 ro Romanian


condensed:

564 xto Tocharian A
2594 txb Tocharian B
13410 sq Albanian
622 hit Hittite
23293 hy Armenian
29796 lt Lithuanian
126889 lv Latvian
427786 ru Russian
59617 uk Ukrainian
162124 pl Polish
69812 sh Serbo-Croatian
42773 ga Irish
65763 grc Ancient Greek
82624 el Greek
845446 la Latin
758455 es Spanish
587798 it Italian
401197 fr French
377363 pt Portuguese
373267 de German
154217 nl Dutch
1261187 en English
75524 nb Norwegian Bokml
260639 sv Swedish
34540 is Icelandic
19973 fa Persian
30741 hi Hindi



langcodes:
0swws$0u'A&Fa',n0
{'xto', 'txb', 'sq', 'hit', 'hy', 'lt', 'lv', 'ru', 'uk', 'pl', 'sh', 'ga', 'grc', 'el', 'la', 'es', 'it', 'fr', 'pt', 'de', 'nl', 'en', 'nb', 'sv', 'is', 'fa', 'hi', }


240715    09:45:38    


#G_processed_PIE_tree_file = RadixRootdir + 'resources/processed_coglangs_proto-indo-european-langs-tree.pickle'
#try:
#   PIE_langcode_poset_restricted = Unpickle(Read(G_processed_PIE_tree_file, read_type='rb'))
#except:
#   pass
#
240715    09:58:38    
#   styles.extend(f'''
#               :root {{ --lang-color-{lang}: rgb{color}; }}
#
#               .lang-{lang}{{color: var(--lang-color-{lang});}} 
#               .lang-{lang} .lang-element {{color: rgb{Darken(color, 1.4)};}} 
#               .lang-{lang} .def-element {{color: rgb{Darken(color, 1.3)};}} 
#                 ''' for lang,color in Langcolors.items() if lang not in ['de'])

def LangCol(lang):
   return Langcolors[lang] if lang in Langcolors else Gray(170)

240718    09:26:54    
STL red;
Therefore, please don't share this around. 
STL

STL rgb(225, 150, 40);
If you got this link directly from tsvibt, then you can share it with 1 or 2 people who will respect this wish. 
STL 


240718    13:12:31    

word = Word('profane', 'en')
Wordtext_Lang_SenseRefsLinks_X(word)
Word_Parsed(word)
SQL_Wordtitle_XML.Select('profane')
rg --no-ignore "<title>profane"
trunk_poset = Sense_Pastwardsenseposet(sense)

240722    06:39:33    
#   defs = [excise_substrings(x, '<ref>', '</ref>') for x in defs]
#   defs = [excise_substrings(x, '<ref', '/>') for x in defs]
#   defs = [excise_substrings(x, '<ref', '</ref>') for x in defs]
#   defs = [excise_substrings(x, '&lt;ref&gt;', '&lt;/ref&gt;') for x in defs]
#   defs = [excise_substrings(x, '&lt;ref', '/&gt;') for x in defs]
#   defs = [excise_substrings(x, '&lt;ref', '&lt;/ref&gt;') for x in defs]


#TODO
def Word_Definitionstring(word, included_senses=None):
   if len(included_senses)==0:
      defs = Word_Definitions(word)
   else:
      defs = [defn for sense in included_senses for defn in Sense_Definitions(sense)]
   defs = ShortenDefs(defs, first_time=True)
   defs = ShortenDefs(defs)
   defs = ShortenDefs(defs)
   defs = [x.strip() for x in defs if len(x.strip())>1]
   if LangIsold(word.lang):
      return ';'.join(defs)[:40]
   else:
      if len(defs)>0:
         return defs[0][:30]
      else:
         return ''


240724    15:47:10    
loool
def Token_Text(token):
   return token['content'] if token['kind'] == 'text' else token['content']['text']

def chunk_and_pad(xs, n):
   result = []
   for i in range(0, len(xs), n):
      result.append(xs[i:i+n])
   result[-1] += [''] * (n - len(result[-1]))
   return result

def Sense_FullDefinitionHTML(sense):
   defs = [Text_BasicClean(defn) for defn in Sense_Definitions(sense)]
   if len(defs) < 4:
      return ''.join(Paragraph(f'{i+1}. {defn}') for i,defn in enumerate(defs))
   else:
      index = (len(defs)+1)//2
      return Tuples_HTMLtable(list(zip(defs[:index], defs[index:])))
#      return Tuples_HTMLtable(list(zip(defs[::2], defs[1::2])))
#      return Tuples_HTMLtable(chunk_and_pad(defs, 2))
#      return Tuples_HTMLtable(chunk_and_pad(defs, 2 if len(defs)<9 else 3))
#   elif len(defs) < 9:
#      return chunk_and_pad(defs, 2)
#   else:
#      return chunk_and_pad(defs, 3)





240725    06:08:22    

def js_Langcolor_defaults():
   return Dict_JSstring({lang:list(color) for lang,color in Langcolors.items()})
#   return '{' + ', \n'.join(f"'{lang}':{list(color)}" for lang,color in Langcolors.items()) + '}'


function DictFun_ItemsMap(d, f) { Object.entries(d).forEach(([k, v]) => { f(k,v); }); }
   
//for (const lang in js_langcolor_defaults) { LangColor_setColors(lang, js_langcolor_defaults[lang]); }
//Object.entries(js_langcolor_defaults).forEach(([lang, color]) => { LangColor_setColors(lang, color); });
DictFun_ItemsMap(js_langcolor_defaults, LangColor_setColors);

//for (const lang in get_Settings().user_colors) { LangColor_setColors(lang, get_Settings().user_colors[lang]); }
//Object.entries(get_Settings().user_colors).forEach(([lang, color]) => { LangColor_setColors(lang, color); });
DictFun_ItemsMap(get_Settings().user_colors, LangColor_setColors);


function set_settings_after_DOM_loaded(){
   DictFun_ItemsMap(js_langcolor_defaults, LangColor_setPickerColor);
   for (const lang in js_langcolor_defaults) {
      LangColor_setPickerColor(lang, js_langcolor_defaults[lang]);
   }
   for (const lang in get_Settings().user_colors) {
      LangColor_setPickerColor(lang, get_Settings().user_colors[lang]);
      Lang_setModified(lang);
   }

   Refresh_CoglangsState();
}


function store_update(attr, key, value){
   let settings = get_Settings();
   settings[attr][key] = value;
   set_Settings(settings);
}

function store_delete(attr, key){
   let settings = get_Settings();
   delete settings[attr][key];
//   let dict = settings[attr];
//   delete dict[key];
   set_Settings(settings);
}


function rgbToHex(rgb) {
    var r = rgb[0].toString(16).padStart(2, '0');
    var g = rgb[1].toString(16).padStart(2, '0');
    var b = rgb[2].toString(16).padStart(2, '0');
    return '#' + r + g + b;
   rgb.map(fun).join('')
}


document.addEventListener('input', function(event) {
//   let [target, kind, lang] = [event.target, target.dataset.target_kind, target.dataset.lang]
   let target = event.target
   let [kind, lang] = [target.dataset.target_kind, target.dataset.lang]
//   var kind = target.dataset.target_kind;
//   var lang = target.dataset.lang;
   if (kind === 'lang-coglang'){
      store_update('user_coglangs', lang, target.checked);
      Refresh_CoglangsState();
   } else if (kind === 'lang-color'){
      store_update('user_colors', lang, hexToRgb(target.value));
      Lang_setModified(lang);
      Lang_refreshColor(lang);
   };
});


//function hexToRgb(hex) {
//   hex = hex.replace('#', '');
//   var r = parseInt(hex.substring(0, 2), 16);
//   var g = parseInt(hex.substring(2, 4), 16);
//   var b = parseInt(hex.substring(4, 6), 16);
//   return [r,g,b];
//}

function Darken(rgb, factor) {
   return rgb.map(value => Math.floor(value / factor));
//    const [r, g, b] = rgb;
//    const darkenedR = Math.floor(r / factor);
//    const darkenedG = Math.floor(g / factor);
//    const darkenedB = Math.floor(b / factor);
//    return [darkenedR, darkenedG, darkenedB];
}


function Lang_setModified(lang){
   Query_ResetElements(`[data-target_kind="lang-color"][data-lang="${lang}"]`).map(Reset_setModified);
//   var elements = Array.from(document.querySelectorAll(`.reset-button-wrapper[data-target_kind="lang-color"][data-lang="${lang}"]`));
//   elements.map(Reset_setModified);
//   for (element of elements) { Reset_setModified(element); }
}

function Query_ResetElements(query) {
   return Array.from(document.querySelectorAll(`.reset-button-wrapper${query}`));
}

function Lang_setUnmodified(lang){
   Query_ResetElements(`[data-target_kind="lang-color"][data-lang="${lang}"]`).map(Reset_setUnmodified);
//   var elements = Array.from(document.querySelectorAll(`.reset-button-wrapper[data-target_kind="lang-color"][data-lang="${lang}"]`));
//   elements.map(Reset_setUnmodified);
}

def Text_Legendheader(text):
   return f'<h4 id="jumping">{text}</h4>'
#   return Paragraph(Color(Legend_color, text), style="width: 100%; background-color: rgba"+str(Legend_color_background))



240727    06:16:23    
h2 {
   width: 100%;
   color: rgb(130,225,255);
   background-color: rgba(210,70,240, .3);
}

h3 {
   color: rgb(150,205,205);
   background-color: black;
   border-bottom: 1px solid rgba(210,70,240, .25); 
   margin-left: 10px;
}

h4 {
   width: 100%;
   color: rgb(130,225,255);
   background-color: rgba(210,70,240, .3);
   margin-top: .9em;
   margin-bottom: .3em;
}


240729    04:17:58    
p:not(:last-child)::after {
   content: "";
   display: block;
   position: relative;
   z-index: 99;
   bottom: 0;
   left: 0;
   width: 100%;
   height: 1px; 
   background-color: rgb(50,50,50); 
}



h4 {
   width: 100%;
   color: rgb(130,225,255);
   background-color: rgba(210,70,240, .3);
   margin-top: .9em;
   margin-bottom: .3em;
}

240730    06:40:59    

.color-picker-wrapper {
   outline: .9px solid rgb(80,80,110);
   z-index: 9999999;
   position: relative;
   border-top: 8px;
}


<div class='tree-container'> 
   <div style='display:inline-flex;'>
         bip 
      <div >
         <div style='display:inline-flex;'>
               bweep  
            <div style='display:inline-flex;'>
                  b2weep  
               <br>
               b2vappp 
            </div>
         </div>

         <br>
         bvappppa 
      </div>
   </div>
</div>

240730    10:04:37    

def HTML_treeWrap(html):
   return f'<div class="tree-word-wrapper">{html}</div>'

def TreeDirection_HTML(tree, node_text, root=True):
   acc = HTML_inlineFlex(HTML_treeWrap(node_text(tree['node'])) + HTML_Div('<br>'.join(TreeDirection_HTML(ch, node_text, root=False) for ch in tree['children'])))
   return acc if not root else f'<div class="tree-container">{acc}</div>'
#   return acc if not root else Paragraph(acc, style="white-space: pre; ") 


240730    17:22:16    

G_lxml_parser = etree.HTMLParser()
def html_just_text(html):
   return etree.parse(StringIO(html), G_lxml_parser).xpath('string()')

def old_TreeDirection_HTML(tree, node_text, stack=[], is_first=True):
   acc = ''
   acc += StackNodeIsfirst_Prefix(stack, tree['node'], is_first)

   text = node_text(tree['node'])
   acc += text
   stack.append({'text': html_just_text(text),  'has_more': True, 'node': tree['node']})
   for i,ch in enumerate(tree['children']):
      if i == len(tree['children'])-1:
         stack[-1]['has_more'] = False
      acc += TreeDirection_HTML(ch, node_text, stack=stack, is_first=(i==0))
   stack.pop()
   return acc

def StackNodeIsfirst_Prefix(stack, node, is_first):
   if is_first:
      if len(stack) > 0:
         return Color(Connector_color, ('' if stack[-1]['has_more'] else '-'))
      else:
         return '' # can add thing, e.g. ,  but would have to also add space to result below for lines below
   else:
      result = ''.join([ len(x['text'])*' ' + ('' if x['has_more'] else ' ')  for x in stack])
      if len(stack)>0:
         result = result[:-1] + ('' if stack[-1]['has_more'] else '')
      return '<br>' + Color(Connector_color, result)

240801    18:02:50    
      # more unified+general way could be something like "nearest ulterior covered by anyone"...
      elif (not word_explicitly_covered_by(word, 'included')) and word_explicitly_covered_by(word, 'excluded'):
         word_includesenses_update(word, set())
      else:
         backlinking_senses = set(filter(sense_is_backlinking, Word_AllRealGivenSenses(word)))
         word_includesenses_update(word, Word_GivensOrPlaceholder(word) if len(backlinking_senses) == 0 else backlinking_senses)


240804    14:32:16    
senseid

240805    15:14:48    
https://github.com/earwig/mwparserfromhell/?tab=readme-ov-file
exec(open('src/radix.py').read())

import mwparserfromhell

text = SQL_Wordtitle_XML.Select('sing')
wikicode = mwparserfromhell.parse(text)
dir(wikicode)

[len(x) for x in wikicode.nodes[5].contents.nodes]
[len(x) for x in wikicode.nodes[5].contents.nodes]

for x in wikicode.nodes[5].contents.nodes[:17]:
   print(x)

wikicode.nodes[5].contents.nodes[17].contents.nodes
len(wikicode.nodes[5].contents.nodes[17].contents.nodes)

[len(x) for x in wikicode.nodes[5].contents.nodes[17].contents.nodes]


for i, x in enumerate(wikicode.nodes[5].contents.nodes[17].contents.nodes):
   print('thing number', i)
   print(x)

wikicode.nodes[5].contents.nodes[17].tag

for x in wikicode.nodes:
   print(x.tag)

wikicode.nodes[0].type

<class 'mwparserfromhell.nodes.tag.Tag'>

def wikicode_inspect(wiki):
   for i,x in enumerate(wiki.nodes):
      print('\n')
      print('thing number', i)
      t = str(type(x)).split('.')[-2]
      ln = len(str(x))
      print('length:', ln)
      print('type:', type(x))
      if t == 'text':
         print('text:', x)
      if t == 'tag':
         print('tag:', x.tag)
         if ln < 100:
            print('contents:', str(x))

wikicode_inspect(wikicode)
wikicode_inspect(wikicode.nodes[5].contents)

.contents.nodes[17].tag

tag: revision
tag: text

def NodeTag_Select(node, tag):
   for x in node.nodes:
      if str(type(x)).split('.')[-2] == 'tag':
         if x.tag == tag:
            return x

def NodeType_isInstance(node, tp):
   return str(type(node)).split('.')[-2] == tp

type: <class 'mwparserfromhell.nodes.template.Template'>
    if isinstance(node, mwparserfromhell.nodes.Wikilink):
    elif isinstance(node, mwparserfromhell.nodes.Tag):
    elif isinstance(node, mwparserfromhell.nodes.Text):




thing = NodeTag_Select(wikicode, 'revision')

thing.contents
wikicode_inspect( thing.contents)


thing2 = NodeTag_Select(thing.contents, 'text')
wikicode_inspect( thing2.contents)
for some reason we get a wikicode object...
type(thing2.contents.nodes)
type(thing2)
thing2.contents.nodes[1661]

https://mwparserfromhell.readthedocs.io/en/latest/usage.html
https://mwparserfromhell.readthedocs.io/en/latest/api/mwparserfromhell.html#mwparserfromhell.wikicode.Wikicode

thing2.contents.nodes[1661].name
thing2.contents.nodes[1661].get(1)


240806    10:37:46    
apparently there's only 4 instances of a ref inside of col??????

240806    16:08:36    

exec(open('src/radix.py').read())

non_inflection_kind_lists_common = [ ['cognate', 'cog'], ['descendant', 'desc', 'desctree', 'descendants tree'], [ 'link', 'l',], [ 'mention', 'm',], ['back-formation', 'back-form', 'bf']]

rg --no-ignore "\{\{gloss"

a = '''
{{desc|bor=1|ne|}}
{{desc|bor=1|or|}}
{{desc|bor=1|ps||tr=min}}
{{desc|bor=1|pa|}}
{{desc|bor=1|si|}}
{{desc|bor=1|ur||tr=mina}}
{{desc|bor=1|gur|miniti}}
{{desc|ht|minit}}
{{desc|bor=1|ro|minut}}
{{desc|frm|minute}}
{{desctree|fr|minute}}
{{desctree|bor=1|nl|minuut}}
{{desc|nrf|minnute}}
{{desc|wa|munute}}
{{desc|bor=1|gmw-cfr|Menutt|alts=1}}
{{desctree|bor=1|de|Minute}}
{{desc|bor=1|dsb|minuta}}
{{desc|bor=1|lb|Minutt}}
{{desctree|bor=1|enm|mynute}}
{{desc|izh|verbi|bor=1}}
{{desctree|dum|cracht}}
{{desc|en|patronage}}
{{desc|fr|deal}}
{{desc|af|produk}}
{{desc|id|produk|bor=1}}</text>
{{desc|ja||tr=heddo|bor=1}}
{{desc|srn|ede}}
{{desc|srn|nen|bor=1}}
{{desc|ja||bor=1}}
{{desc|nl|naam}}
'''



ref_text = 'desc|nl|naam'
NEW_ReftextRefdict(ref_text) == ReftextRefdict(ref_text)



240807    09:12:40    
def ReftextRefdict(ref):
   d = {'text': '{{' + ref + '}}'}
   #   doesn't deal with sub sub refs...
   ref = excise_substrings(ref, '{{', '}}')
   allargs = ref.split('|')
   args = [a for a in allargs if '=' not in a]
   eqargs = [a for a in allargs if '=' in a]
   d['kind'] = 'unrecognized'
   try: 
      update_ref_dict(d, args)
   except:
      d = {'text': '{{' + ref + '}}', 'kind': 'bad reference'}

   if d['kind'] not in ['unrecognized', 'bad reference']:
#      if 'affixes' in d: #INCLUDING AFFIXES
#         d['wordtexts'] += d['affixes']
      d['lang'] = d['lang'].strip('\n ')
      if d['lang'] in GCode_Rectify:
         d['lang'] = GCode_Rectify[d['lang']]
      if d['lang'] not in Codelangs:
         Log("d['lang'] not in Codelangs:")
         Log(d)
         d['kind'] = 'bad reference'
      else:
         d['wordtexts'] = [word for wordtext in d['wordtexts'] for word in ([wordtext] if '[[' not in wordtext else wordtext.split('/'))]
         d['wordtexts'] = [word.strip(', []/') for word in d['wordtexts']] # don't strip )- b/c proto stuff is like ..(n)- or w/e
         d['wordtexts'] = [word for word in d['wordtexts'] if word.strip(', []/()-') != '']
         d['wordtexts'] = [WordrawtextLang_Wordtext(word, d['lang']) for word in d['wordtexts']]
   return d

def lists_firstsdict(lists):
   return {k:ll[0] for ll in lists for k in ll}

G_refdict_common_forms = lists_firstsdict(all_common_reference_forms)
G_refdict_main_etymons = lists_firstsdict(main_etymon_kindlists)
G_refdict_columns_like = lists_firstsdict(columns_like_kindlists)
G_refdict_language_specific = lists_firstsdict(language_specific_kindlists)
G_refset_others = {'root', 'form of', 'suffix', 'suf', 'prefix', 'pre', 'affix', 'af', 'con', 'confix', 'also', 'alter', 'alt'}
G_refset_markers = {'senseid', 'etymid'}

def update_ref_dict(d, args):
   kind = args[0]

   if kind in G_refdict_common_forms:
      d['lang'] = args[1]
      d['wordtexts'] = [args[2]]
      d['kind'] = G_refdict_common_forms[kind]
   elif kind in G_refdict_main_etymons:
      d['lang'] = args[2]
      if ',' in d['lang']:
         d['lang'] = d['lang'].split(',')[0]
      d['wordtexts'] = [args[3]]
      d['kind'] = G_refdict_main_etymons[kind]
   elif kind in G_refdict_columns_like:
      d['lang'] = args[1].split('&')[0]
      d['wordtexts'] = [x.split('&')[0] for x in args[2:]]
      d['kind'] = G_refdict_columns_like[kind]
   elif kind in G_refdict_language_specific:
      d['lang'] = G_refdict_language_specific[kind].split('-')[0] # wouldn't work for eg ine-pro
      d['wordtexts'] = [args[1]]
      d['kind'] = G_refdict_language_specific[kind]
   elif kind in G_refset_markers:
      d['kind'] = kind
      d['lang'] = args[1]
      d['marker'] = args[2]
      d['wordtexts'] = []
   elif kind in G_refset_others:
      if kind in ['root']:
         d['lang'] = args[2]
         d['wordtexts'] = args[3:]
         d['kind'] = 'root'
      elif kind in ['PIE word']:
         d['lang'] = args[2]
         d['wordtexts'] = args[3:]
         d['kind'] = 'PIE word'
      elif kind in ['form of']:
         d['lang'] = args[1]
         d['wordtexts'] = [args[3]]
         d['kind'] = 'form of'
   ## *fixes
      elif kind in ['suffix', 'suf']:
         d['lang'] = args[1]
         d['wordtexts'] = [args[2]]
         d['affixes'] = ['-'+args[3]]
         d['kind'] = 'suffix'
      elif kind in ['prefix', 'pre']:
         d['lang'] = args[1]
         # note different order
         d['affixes'] = [args[2] + '-']
         d['wordtexts'] = [args[3]]
         d['kind'] = 'prefix'
      elif kind in ['affix', 'af']:
         d['lang'] = args[1]
         d['affixes'] = [x for x in args[2:] if '-' in x]
         d['wordtexts'] = [x for x in args[2:] if '-' not in x]
         d['kind'] = 'affix'
      elif kind in ['confix', 'con']:
         d['lang'] = args[1]
         ws = args[2:]
         d['affixes'] = [ws[0] + '-', '-' + ws[-1]]
         d['wordtexts'] = ws[1:-1]
         d['kind'] = 'confix'
   ##related words
      elif kind in ['also']:
         d['lang'] = 'unknown'
         d['wordtexts'] = args[1:]
         d['kind'] = 'also'
      elif kind in ['alter', 'alt']:
         d['lang'] = args[1]
         d['wordtexts'] = args[2:]
         d['kind'] = 'alter'



#####
#def basic_dict(args, lang, wordtexts):
#   return {'lang':args[lang], 'wordtexts': [args[wordtexts]]})

240807    13:19:15    
KindlistsFun_register([['senseid', 'sid'], ['etymid']]  , lambda args: {'lang':args[1]    , 'wordtexts': []                 , 'target': args[2]})


exec(open('src/radix.py').read())
SQL_Wordtitle_XML.Select('dictionary')
Wordtext_Lang_SenseRefsLinks_X

xml_text= SQL_Wordtitle_XML.Select('dictionary')
XML_Dict(xml_text)

word = Word('dictionary', 'en')
Word_Parsed(word)

sense = Sense(word, '1')

Sense_Pastwardsenseposet(sense)

rg --no-ignore "\|gloss="
101235
rg --no-ignore "\|t=" | wc -l
1114831
rg --no-ignore "\|t1=" 
rg --no-ignore "\|t1=" | wc -l
499284
rg --no-ignore "\|t2=" | wc -l
343778

rg --no-ignore "\|gloss1=" | wc -l
62234
rg --no-ignore "\|gloss2=" | wc -l


240807    16:40:21    

      starting_word = trunk_poset.domain[0]
#   bro wtf was this for?
#   starting_sense = list(trunk_poset.includedSenses[starting_word])[0]


240809    03:38:56    

exec(open('src/radix.py').read())
PIE_langcode_poset
PIE_langcode_poset.prior_to('ine-arstpro', 'easrtn')
PIE_langcode_poset.prior_to('enm', 'en')


240809    16:49:17    
def WordWordDirection_InspectionHTML(word1, word2, direction, go_to_equivalents=True):
   result = ''
   mentioner_lists = WordWord_Mentionerlists(word1, word2, 'local')
   if len(mentioner_lists['pastward']) + len(mentioner_lists['futureward']) == 0:
      print('falling through to all mentioners for', word1, word2)
      mentioner_lists = WordWord_Mentionerlists(word1, word2, 'global')

   if len(mentioner_lists['pastward']) + len(mentioner_lists['futureward']) > 0:
      result += Paragraph('(had to look at all mentioners)')
      result += WordWordDirectionMentionerlists_Inspection(word1, word2, direction, mentioner_lists)
   elif go_to_equivalents:
      result += Paragraph('(had to look at equivalents)')
      for word1_equiv, word2_equiv in product(SQL_WORD_Equivalents.Select(word1), SQL_WORD_Equivalents.Select(word2)):
         if (word1_equiv == word1 and word2_equiv == word2):
            continue

         mentioner_lists = WordWord_Mentionerlists(word1_equiv, word2_equiv, 'global')
         result += WordWordDirectionMentionerlists_Inspection(word1_equiv, word2_equiv, direction, mentioner_lists)

   return result


240809    17:24:03    

exec(open('src/radix.py').read())
word = Word('*sperH-', 'ine-pro')
SQL_WORD_Equivalents.Select(word)

equiv_mentioner_lists = WordWord_Mentionerlists(thing1, thing2, 'global')

240811    14:45:11    

   html_content += SQL_WORD_Parsed.Select(sense.word)
   html_content += SQL_WORD_AllMentioningWords.Select(sense.word)
   html_content += SQL_SENSE_FutureWordlinks.Select(sense)
   html_content += SQL_SENSE_Direction_Wordrefs.Select(sense)
   html_content += SQL_WORD_Direction_GivenRefWords.Select(sense.word)
   html_content += SQL_WORD_Strictpastwardset.Select(sense.word)
   html_content += SQL_WORD_ImmediatePastwardset.Select(sense.word)
   html_content += SQL_WORD_Equivalents.Select(sense.word)
   html_content += SQL_WORD_ImmediateFuturewardset.Select(sense.word)
   html_content += SQL_WORD_Strictfuturewardset.Select(sense.word)
   html_content += SQL_WORD_RealGivenSenses.Select(sense.word)
   html_content += SQL_WORD_Direction_Coveringsenses.Select(sense.word)
   html_content += SQL_WORD_Coglang_Langstrictfutures.Select(sense.word)
   html_content += SQL_SENSE_Pastwardsenseposet.Select(sense)

#   html_content += SQL_SENSE_Lang_Futuresenseposet.Select(sense)


[SQL_WORD_Parsed, SQL_WORD_AllMentioningWords, SQL_SENSE_FutureWordlinks, SQL_SENSE_Direction_Wordrefs, SQL_WORD_Direction_GivenRefWords, SQL_WORD_Strictpastwardset, SQL_WORD_ImmediatePastwardset, SQL_WORD_Equivalents, SQL_WORD_ImmediateFuturewardset, SQL_WORD_Strictfuturewardset, SQL_WORD_RealGivenSenses, SQL_WORD_Direction_Coveringsenses, SQL_WORD_Coglang_Langstrictfutures, SQL_SENSE_Pastwardsenseposet]


def Json_Inspection(json):
   word_json = json['inspect_word']
   word = Json_Word(word_json)



   starting_sense = Json_Sense(json['starting_sense'])
   senseposet = Sense_Pastwardsenseposet(starting_sense)
   if word not in senseposet.immediate_ulteriors:
      return 'not in pastward'

   result = ''
   result += Paragraph('word: ' + Word_Element(word, info={"kind":"inspection"}))
   result += Element('div', 'included senses: ' + ', '.join(str(x) for x in sorted(Json_Sensenums(word_json))), style='white-space: nowrap')
   result += Element('div', 'excluded senses: ' + ', '.join(str(x) for x in sorted(Json_ExcludedSensenums(word_json))), style='white-space: nowrap')
   for sense_num in sorted(Json_Sensenums(word_json)):
      result += Paragraph(f'sense {sense_num}:' + ('' if sense_num != -1 else ' no wiktionary article'))
      result += Paragraph(Sense_Wiktionary(Sense(word, sense_num)))
      result += Paragraph(Sense_FullDefinitionHTML(Sense(word, sense_num)))
   for maybe_prior_word, ulteriors in senseposet.immediate_ulteriors.items():
      if word in ulteriors:
         result += WordWordDirection_InspectionHTML(word, maybe_prior_word, senseposet.direction)
   result += WordSenseposet_CoveringsensesHTML(word, senseposet)
   return result


def Json_IncludedSensenums(json):
   return sorted(ujson.loads(json['senses_included']))

def Json_ExcludedSensenums(json):
   return sorted(ujson.loads(json['senses_excluded']))


   result += WordSenseposet_CoveringsensesHTML(word, senseposet)

def WordSenseposet_CoveringsensesHTML(word, senseposet):
   result = ''
   result += Element('div', f'senses covering {Word_Element(word, info={"kind":"inspection"})}:', style='white-space: nowrap')
   coverings, all_covering_senses = WordSenseposet_Coverings(word, senseposet) 
   result += Paragraph(''.join('cover ' + clusion + ' senses: ' + str(coverings[clusion]) + '<br>' for clusion in ['by included', 'by excluded']))
   result += Paragraph('all covering senses: ' + str(all_covering_senses))
   return result

def WordSenseposet_Coverings(word, senseposet):
   all_covering_senses = SQL_WORD_Direction_Coveringsenses_CACHE_Select(word, senseposet.direction)
   coverings = {'by included':[], 'by excluded':[]}
   for covering_sense in all_covering_senses:
      if covering_sense.word in senseposet.includedSenses:
         if covering_sense in senseposet.includedSenses[covering_sense.word]:
            coverings['by included'].append(covering_sense)
         else:
            coverings['by excluded'].append(covering_sense)
   return (coverings, all_covering_senses)

def Json_Sense(json):
   return Sense(Json_Word(json), json['sense'])


   for maybe_prior_word, ulteriors in senseposet.immediate_ulteriors.items():
      if word in ulteriors:
         result += WordWordDirection_InspectionHTML(word, maybe_prior_word, senseposet.direction)


      children_candidates = [dict(child_word, {'parent_word':root_word}) for child_word in sorted(immediate_ulteriors[root_word]) if child_word in immediate_ulteriors]



def WordUlteriors_Traversal(root_word, immediate_ulteriors, alreadyIncluded):
   info = {'already included': root_word in alreadyIncluded, }
   alreadyIncluded.add(root_word)
   if info['already included'] or root_word not in immediate_ulteriors:
      children = []
   else:
      children_candidates = [child_word for child_word in sorted(immediate_ulteriors[root_word]) if not child_word not in immediate_ulteriors]
      children_already_included, children_not_already_included = ListPredicate_Divide(children_candidates, lambda x: x in alreadyIncluded)  
      children_not_already_included.sort()
      children_not_already_included.reverse()
      children_pre_chunked = [WordUlteriors_Traversal(child_word, immediate_ulteriors, alreadyIncluded)
                  for child_word in children_already_included + children_not_already_included]
      initial_alreadys, remaining_children = ListPredicate_ChompUntil(children_pre_chunked, lambda tree: not tree['node'][0]['info']['already included'])
      children = [] if len(initial_alreadys) == 0 else [{'node': [tree['node'][0] for tree in initial_alreadys], 'children': []}]

      for next_lang in ListKeyfun_Divide(remaining_children, lambda tree: tree['node'][0]['word'].lang):

#         lang_children = []
         #todo: can omit, assuming there's never things here
         next_lang_alreadys, next_lang_not_alreadys = ListPredicate_Divide(next_lang, lambda tree: tree['node'][0]['info']['already included'])
         assert len(next_lang_alreadys) == 0, print(root_word, next_lang_alreadys)

         for children_equivalent_list in ListKeyfun_Divide(next_lang, lambda tree: set(node_element['word'] for child_tree in tree['children'] for node_element in child_tree['node'])):
            children.append({'node': [node_element for equivalent_tree in children_equivalent_list 
                             for node_element in equivalent_tree['node']], 'children': children_equivalent_list[0]['children']}) #relies on ListKeyfun_Divide order-preserving

   return {'node': [{'word':root_word, 'info': info}], 'children': children}


#      children_not_already_included.sort() # lolwut? what's the point of sorting them, then dividing them , then sorting again??




#         lang_children = []
         #todo: can omit, assuming there's never things here
         next_lang_alreadys, next_lang_not_alreadys = ListPredicate_Divide(next_lang, lambda tree: tree['node'][0]['info']['already included'])
         assert len(next_lang_alreadys) == 0, print(root_word, next_lang_alreadys)

240814    16:20:16    

SQL_WORD_ImmediatePastwardset {[lonc, fro], [lunc, fr]}
O.Fr lonc, Fr lunc

wtf. it has this. so it should show up when we iterate
to_do_1 = {'generator source': SQL_WORD_ImmediateFuturewardset, 'function':FUN, 'target tables':[SQL_SENSE_Pastwardsenseposet], }#'limit': 50000} 

exec(open('src/radix.py').read())

word = Word('longue', 'fro')
poset = Word_Pastwardposet(word)


240814    17:01:54    

def CognatesPage(wordtext, lang='none_given', sense_num='none_given'):
   sense = WordtextLangSensenum_Realsense(wordtext, lang, sense_num) 
   display_content = Sense_Displaycontent(sense)
   return Displaycontent_HTML(display_content)

def ReportPage(wordtext, lang='none given', sense_num='none given'):
   sense = WordtextLangSensenum_Realsense(wordtext, lang, sense_num) 
   return Displaycontent_HTML(ReportPage_Displaycontent(sense))




@app.route('/word/<param1>/<param2>', methods=['GET',])
def serve_word(param1, param2):
   wordtext = urllib.parse.unquote(param2.strip())
   options = Optionstring_Options(param1)

   if len(wordtext)>0:
      if not globals().get('G_Server_mode', False) and options.get('mode', None) == 'report':
         return send_html(ReportPage(wordtext, options.get('lang', 'none_given'), options.get('sensenum', 'none_given')))
      else:
         return send_html(CognatesPage(wordtext, options.get('lang', 'none_given'), options.get('sensenum', 'none_given')))
   else:
      return 'no word'


def WordtextLangSensenum_Realsense(wordtext, lang, sense_num):
   print('wordtext:', wordtext)
   print('lang:', lang)
   print('sense_num:', sense_num)
   if lang == 'none_given':
      print('here')
      for maybe_lang in ['en', 'de'] + list(G_Cached_Coglangs):
         print(maybe_lang)
         if Word_Parsed(Word(wordtext, maybe_lang)) != {}:
            lang = maybe_lang
            break
   word = Word(wordtext, lang)
   print(word)
   if sense_num == 'none_given':
      sense = min(Word_GivensOrPlaceholder(word))
   else:
      sense = Sense(word, int(sense_num))
   print(sense)
   return sense

240822    05:23:58    

judgements_token = {}
for yes, no in product([True, False], 2):
   judgements_token[('yes' if yes else '') + ('no' if no else '')] = []


big_connector:
   global judgements_token
   yes = big_connector_yes(token)
   no = big_connector_no(token)
   judgements_token[('yes' if yes else '') + ('no' if no else '')].append()
   return yes and (not no)

how to deal with brackets ()[]<>?
check for unbalanached? if balanced, excise..??


wtf? edit to ad "um", no? is this common ? http://radix.ink/word/lang:de/Umwelt


240905    16:53:22    

def CALLED(to_dos, batchsize=30000, commit_wavelength=100000):
   start_time = time.time()

   def make_generator(generator_source):
#      match generator_source:
      if type(generator_source) == tuple:
         return generator_source
      elif type(generator_source) == set:
         return (len(generator_source), map(lambda x:(x,), generator_source))
      elif type(generator_source) == SQL_Table:
         words_count = generator_source.Count()
         def generator():
            iteration_cursor = generator_source.RowIterator()
            while True:
               print('\nfetching next batch...')
               rows = iteration_cursor.fetchmany(batchsize)
               if not rows:
                  iteration_cursor.close()
                  break
               for row in rows:
                  yield list(generator_source.SQLkey_Plainkey(row[:-1])) + [generator_source.Unstore(row[-1])]
         return (words_count, generator())
      else:
         return (len(generator_source), generator_source)

   recent_timestamps = [start_time]*4
   average_step = 0 
   average_step_step = 0 
   def list_index_step(xs, i):
      return xs[i]-xs[i-1]


#domain, immediate ulteriors, includedSenses, derivatives
class Senseposet:
   def __init__(self, ARG, kind='from_old'):
      if kind=='from_poset':
         poset, includedSenses = ARG
         self.direction = poset.direction
         domain = Dict_Nonempties(includedSenses)
         self.domain = [word for word in poset.domain if word in domain]
         self.immediate_ulteriors = ItemsetDomain_Restricted(poset.immediate_ulteriors, domain)
         self.includedSenses = includedSenses
         self.derivatives = {word for word in self.domain if 
            any(parent_word.lang == word.lang for parent_word in poset.immediate_priors[word])}
      elif kind=='restriction':
         senseposet, restriction_domain = ARG
         self.direction = senseposet.direction
         self.domain = [word for word in senseposet.domain if word in restriction_domain]
         self.immediate_ulteriors = ItemsetDomain_Restricted(senseposet.immediate_ulteriors, restriction_domain)
#  self.includedSenses = {word:senseposet.includedSenses[word] for word in self.domain if word in senseposet.includedSenses}
         self.includedSenses = ItemsetDomain_Keyfilter(senseposet.includedSenses, self.domain) 
         self.derivatives = senseposet.derivatives & restriction_domain
      elif kind=='merge':
         senseposets = ARG
         self.direction = senseposets[0].direction
         self.domain = []
         already_domained = set()
         for senseposet in senseposets:
            for word in senseposet.domain:
               if word not in already_domained:
                  already_domained.add(word)
                  self.domain.append(word)
         self.immediate_ulteriors = ItemSets_Merged(senseposet.immediate_ulteriors for senseposet in senseposets)
         self.includedSenses = ItemSets_Merged(senseposet.includedSenses for senseposet in senseposets)
         self.derivatives = unionfold(senseposet.derivatives for senseposet in senseposets)
      elif kind=='reverse':
         senseposet = ARG
         self.direction = Direction_Reversed(senseposet.direction)
         self.domain = reversed(senseposet.domain)
         self.immediate_ulteriors = Order_Reversed(senseposet.immediate_ulteriors)
         self.includedSenses = senseposet.includedSenses
         self.derivatives = set() # not sure about this
   def __eq__(self, other): 
      if other == None:
         return False
      if not isinstance(other, Senseposet):
         raise Exception("why can't fruit be compared")
      return all(getattr(other, attribute) == value for attribute, value in vars(self).items())
   def non_generally_excluded_domain(self):
      if not hasattr(self, '_internal_non_generally_excluded_domain'):
         self._internal_non_generally_excluded_domain = [x for x in self.domain if x not in self.generally_excluded_domain_set()]
      return self._internal_non_generally_excluded_domain
   def generally_excluded_domain_set(self):
      if not hasattr(self, '_internal_generally_excluded_domain_set'):
         self._internal_generally_excluded_domain_set = Senseposet_Wordsgenerallyexcluded(self)
      return self._internal_generally_excluded_domain_set



def IndexSiblings_Prefix(index, sibcount):
   if sibcount == 1:
      return HTML_Div('-', attr_string='class="tree-prefix tree-only-child"')
   elif index == 0:
      return HTML_Div('' + VerticalLine(), attr_string='class="tree-prefix tree-first-child"')
   elif index == sibcount-1:
      return HTML_Div('', attr_string='class="tree-prefix tree-last-child"')
   else:
      return HTML_Div('' + VerticalLine(), attr_string='class="tree-prefix tree-middle-child"')

240905    20:08:52    
'''
w=Word('value', 'en')

 File "<string>", line 13, in __init__
  File "<string>", line 4, in __setattr__
dataclasses.FrozenInstanceError: cannot assign to field 'field1'

'''

def test_sense_str_sense(senses):
   for sense in senses:
      print(sense)
      assert str_Sense(sense.__repr__()) == sense, sense.__repr__() + '  ' + str_Sense(sense.__repr__()).__repr__()

#test_sense_str_sense(b[1].domain)

def getbad_sense_str_sense(senses):
   for sense in senses:
      if not str_Sense(sense.__repr__()) == sense:
         return sense

#s = getbad_sense_str_sense(b[1].domain)
#t = str_Sense(s.__repr__())

#tests.py
def AssertTransitivelyClosed(Item_Ulteriors):
   mprint('checking transitive closure')
   #TODO reuse the sequenc thing above after testing
   for x in Item_Ulteriors:
      for y in Item_Ulteriors[x]:
         for z in Item_Ulteriors[y]:
            if z not in Item_Ulteriors[x]:
               raise Exception('order not transitively closed:', z, 'in', y, 'in', x, 'but', z, 'not in', x)

#tests.py
# todo: test that dicts exist for givensensese; ; test that the linkedsenses links have dicts, and that they link to these guys, and that they're in mentioners,; and that the XML for all the givens parses to the same (though this might not be desirable? and the equality dest might not work well? )



240906    03:10:28    

#def Word_DefinitionsUnprocessed(word):
#   return [defn for sensedict in Word_Parsed(word).values() for defn in Sensedict_DefinitionsUnprocessed(sensedict)]

240906    11:39:19    
'''
plan:
   
evaluate tests to build dict?
then get dict from table
for the keys... idk... 
i guess we say, if the key is there, assert equality
if the key is not there, we say so, but don't fail? and don't write..
but then if all the tests pass, we will write the new dict to the table..
yeah

difflib

'''

240906    22:21:03    

def Sense_CognatesHTMLContent(sense, new_id_count=None):
   MaybeNat_ResetID(new_id_count)
   trunk_poset = Sense_Pastwardsenseposet(sense) 
   html_content = [] 
   if trunk_poset == None:
      html_content.append(Paragraph('found nothing for ' + Word_Element(sense.word, info={'kind':'generic'}) + ', sense number: ' + str(sense.num)))
      if globals().get('G_SERVER_RUNNING', False):
         Log(sense, 'unsuccessfully_computed')
      return Sense_ReportHTMLContent(sense, prefix=''.join(html_content))
   else:
      html_content.append(Starting_sense(sense))
#      html_content.append(Text_Legendheader('Debug trunk: '))
#      html_content.append(Sense_DebugtrunkHTML(sense))
      html_content.append(Text_Legendheader('Pastward trunk: '))
      html_content.append(Trunkposet_PastwardtrunkHTML(trunk_poset))
#      html_content.append(Text_Legendheader('New pastward trunk: '))
#      html_content.append(Trunkposet_PastwardtrunkHTML(compute_Sense_Pastwardsenseposet(sense)))
      html_content.append(Async_retriever())
#      html_content.append('boop')
      if globals().get('G_SERVER_RUNNING', False):
         Log(sense, 'successfully_computed')
   return ''.join(html_content)


240906    22:28:29    
'''
q:
0Akb0kvt(5xuagg << 
w:
@q5""
'''


   classes = ['button', 'word', 'lang-DEFAULT', f'lang-{word.lang}', f'kind-{info["kind"]}']

   include_lang_element = True
   include_definitions_element = False
   include_alreadyincluded_element = False

   pretext = ''

   match info['kind']: 
      case 'etymonline_wordlist':
         include_lang_element = False
         dataset['include_jump_down'] = info['in_radix_words']
         classes.append('etymonline_and_radix') if info['in_radix_words'] else classes.append('etymonline_only')
         dataset['click_url'] = Word_EtymonlineURL(word)

      case 'summary_wordlist':
         include_lang_element = False
         classes.append('etymonline_and_radix') if info['in_etymonline_wordtexts'] else None

      case 'summary_wordlist_trunk':
         include_definitions_element = True

      case 'printing_tree':
         dataset['include_jump_down'] = False
         include_lang_element = info['with_lang']
         include_definitions_element = True
         classes.append('generally_excluded') if info['generally_excluded'] else None
         include_alreadyincluded_element = info['already_included']
         sense_count = len(WordInfo_IncludedSenses(word, info))
         pretext = ('' if sense_count<2 else (G_two_sense_element if sense_count==2 else G_poly_sense_element))

      case 'settings' | 'page':
         dataset['suppress_popups'] = True
         include_definitions_element = True

   content = (pretext
            + (Lang_Element(word.lang) if include_lang_element else "") 
            + word.text
            + ('' if not include_definitions_element else 
               Word_DefinitionsElement(word, info, def_str=info.get('def_str', None)))
            + ('' if not include_alreadyincluded_element else 
               WordParentid_AlreadyincludedElement(word, HTMLid)))

   return  f'<span class="{" ".join(classes)}" {Dataset_String(dataset)} onmouseover="handleMouseover(event)" onmouseout="hidePopup(event, {HTMLid})" id={HTMLid}>{content}</span>' 


   content = (pretext
            + (Lang_Element(word.lang) if include_lang_element else "") 
            + word.text
            + ('' if not include_definitions_element else 
               Word_DefinitionsElement(word, info, def_str=info.get('def_str', None)))
            + ('' if not include_alreadyincluded_element else 
               WordParentid_AlreadyincludedElement(word, HTMLid)))



   content = (pretext
            + (Lang_Element(word.lang) if include_lang_element else "") 
            + word.text
            + ('' if not include_definitions_element else 
               Word_DefinitionsElement(word, info, def_str=info.get('def_str', None)))
            + ('' if not include_alreadyincluded_element else 
               WordParentid_AlreadyincludedElement(word, HTMLid)))


# agg = Aggregator()
# agg << thing1
# agg.result


240907    01:02:48    


class Aggregator:
   def __init__(self, values=None, divider=''):
      self.values = values if values is not None else []
      self.divider = divider
   def has_any(self): return len(self.values) > 0
   def __lshift__(self, value): self.values.append(value)
   def __call__(self, wipe=False):
      result = self.divider.join(self.values)
      if wipe: self.values = []
      return result


class Aggregator:
   def __init__(self, values=None, divider=''):
      self.values = values if values is not None else []
      self.divider = divider
   def has_any(self): return len(self.values) > 0
   def __lshift__(self, value): self.values.append(value)
   def __call__(self, wipe=True):
      result = self.divider.join(self.values)
      if wipe: self.values = []
      return result



240907    02:32:51    
	    fun! AlignSkipString(lineno,indx)
	      let synid   = synID(a:lineno,a:indx+1,1)
	      let synname = synIDattr(synIDtrans(synid),"name")
	      let ret= (synname == "String")? 1 : 0
	      return ret
	    endfun


fun! get_syn_name()
   let synid   = synID(line('.') ,col('.') +1, 1)
   let synname = synIDattr(synIDtrans(synid),"name")
   return synname
endfun

echo Get_syn_name()


240907    03:48:09    


      if storage is None or len(storage)==0:
         self.Store, self.Unstore = IDENTITY, IDENTITY 
      elif storage == {'pickle'}:
         self.Store = lambda x: pickle.dumps(x)
         self.Unstore = lambda x: Unpickle(x)
      elif storage == {'pickle', 'compress'}:
         self.Store = lambda x: brotli.compress(pickle.dumps(x), quality=1)
         self.Unstore = lambda x: Unpickle(brotli.decompress(x))



240908    01:26:57    
"""
exec(open('src/sql.py').read())
exec(open('src/site/server.py').read())
"""

import sqlite3
import brotli
import pickle 
import io

G_Server_module_name = 'src.site.server'
G_Server_Unpickle = (__name__ == G_Server_module_name)
class Unpickler(pickle.Unpickler):
   def find_class(self, module, name):
      if G_Server_Unpickle and module == "__main__":
         module = G_Server_module_name
      return super().find_class(module, name)

def Unpickle(byte_string): return Unpickler(io.BytesIO(byte_string)).load()
def Pickle(obj)          : return pickle.dumps(obj)

def Uncompress(string): return brotli.decompress(string)
def Compress(string)  : return brotli.compress(string, quality=1)


class SQL_DB:
   def __init__(self, name, db_file):
      self.name = name
      globals()[self.name] = self
      print(f'intializing database {self.name} using Wiktionary_database: {db_file}')
      self.connection = sqlite3.connect(db_file)
      self.cursor = self.connection.cursor()
      self.tables = []




G_SQL_table_registry = []
#TODO use f strings
class SQL_Table:
   def __init__(self, name, key_columns, value_column, storage={'pickle', 'compress'}):
      self.name = name
      globals()[self.name] = self
      rest_of_columns = key_columns[1:] + [value_column]
      if key_columns[:1] == ['WORD']:
         self.columns = ['wordtext', 'lang'] + rest_of_columns 
         self.Plainkey_SQLkey = lambda key_cols: [key_cols[0].text, key_cols[0].lang] + list(key_cols[1:])
         self.SQLkey_Plainkey = lambda key_cols: [Word(key_cols[0],key_cols[1])]+ list(key_cols[2:])
      elif key_columns[:1] == ['SENSE']:
         self.columns = ['wordtext', 'lang', 'sensenum'] + rest_of_columns 
         self.Plainkey_SQLkey = lambda key_cols: [key_cols[0].word.text, key_cols[0].word.lang, key_cols[0].num] + list(key_cols[1:])
         self.SQLkey_Plainkey = lambda key_cols: [Sense(Word(key_cols[0],key_cols[1]), key_cols[2])]+ list(key_cols[3:])
      else:
         self.columns = key_columns[:1] + rest_of_columns 
         self.Plainkey_SQLkey = IDENTITY
         self.SQLkey_Plainkey = IDENTITY 

      to_compress = 'compress' in storage
      to_pickle = to_compress or 'pickle' in storage 
      self.Store   = lambda x: (Compress if to_compress else IDENTITY)((Pickle if to_pickle else IDENTITY)(x))
      self.Unstore = lambda x: (Unpickle if to_pickle else IDENTITY)((Uncompress if to_compress else IDENTITY)(x))

      G_SQL_table_registry.append(self)

   def EnsureExists(self):
      print(f'ensuring table {self.name} exists')
      Global_db_cursor.execute(f'CREATE TABLE IF NOT EXISTS {self.name} \
                              ({", ".join(column + " text" for column in self.columns)})')
      Global_db_connection.commit()

   def Drop_Create(self):
      print('dropping table ' + self.name)
      Global_db_connection.commit()
      Global_db_cursor.execute('DROP TABLE IF EXISTS ' + self.name)
      Global_db_connection.commit()
      self.EnsureExists()
      print('dropped and created table ' + self.name)

   def InsertRow(self, row):
      # could track number, and then commit when hit 10000....
      row = list(self.Plainkey_SQLkey(row[:-1])) + [self.Store(row[-1])]
#      Global_db_cursor.execute(f"INSERT INTO {self.name} \
#                               ({', '.join(self.columns)}) VALUES ({', '.join(['?']*len(self.columns))})", row)
      Global_db_cursor.execute('INSERT INTO ' + self.name 
                + ' (' + ', '.join(self.columns) + ') VALUES (' + ', '.join(['?']*len(self.columns)) + ')', row)

   def Select(self, *args):
      args = list(args)
      row = Global_db_cursor.execute('SELECT * FROM ' + self.name + ' WHERE  ' + ' AND '.join([arg_name + '=?' for arg_name in self.columns[:-1]]), tuple(self.Plainkey_SQLkey(args))).fetchone()
      return None if not row else self.Unstore(row[-1])

   def Index(self):
      if len(self.columns)>1:
         print('')
         print('indexing', self.name)
         Global_db_cursor.execute('CREATE INDEX Index_' + self.name + ' ON ' + self.name + ' (' + ', '.join(self.columns[:len(self.columns)-1]) + ')')
         Global_db_connection.commit()
         print('done indexing ' + self.name)

   def Count(self):
      print('counting', self.name)
      return Global_db_cursor.execute('SELECT COUNT(*) FROM ' + self.name).fetchone()[0]

   def RowIterator(self):
      iterator_cursor = Global_db_connection.cursor()
      iterator_cursor.execute('SELECT * FROM ' + self.name)
      return iterator_cursor

   def ALLCACHE(self, column_value=None):
      print('getting all rows for table', self.name)
      if column_value is None: all_rows = self.AllRows()
      else:                    all_rows = Global_db_cursor.execute(f'SELECT * FROM {self.name} WHERE {column_value[0]} = ?',
                                                                   (column_value[1],)).fetchall()

      print('done getting all rows for table', self.name)
      print('adding all rows to cache for table', self.name)
      global GLOBALCACHES
      GLOBALCACHES[Tablename_CACHESelectname(self.name)] = {}
      for i,row in enumerate(all_rows):
         GLOBALCACHES[Tablename_CACHESelectname(self.name)][tuple(self.SQLkey_Plainkey(row[:-1]))] = self.Unstore(row[-1]) 
         if i%10==0:
            print('cached', self.name, 'row', i, 'of', len(all_rows), 'or', str(round(100*(i/len(all_rows)), 1)) + '% done.')
      print('done adding all rows to cache for table', self.name)

   def AllRows(self):
      return Global_db_cursor.execute('SELECT * FROM ' + self.name).fetchall()
   def OneValue(self):
      return self.Unstore(self.RowIterator().fetchone()[0])

def SQL_Vacuum():
   print('\nvacuuming database...')
   Global_db_connection.execute("VACUUM")


SQL_Table('SQL_SANDBOX'                        , []                     , 'no_thing')
SQL_Table('SQL_WordtitleRedirect'              , []                     , 'wordtitle_redirect')
SQL_Table('SQL_Wordtitle_XML'                  , ['wordtitle']          , 'raw_article_xml')            
SQL_Table('SQL_WORD_Parsed'                    , ['WORD']               , 'parsed')
SQL_Table('SQL_WORD_AllMentioningWords'        , ['WORD']               , 'all_mentioning_words')
SQL_Table('SQL_SENSE_FutureWordlinks'          , ['SENSE']              , 'wordlinks')
SQL_Table('SQL_SENSE_Direction_Wordrefs'       , ['SENSE', 'direction'] , 'wordrefs')
SQL_Table('SQL_WORD_Direction_GivenRefWords'   , ['WORD', 'direction']  , 'given_ref_words')
SQL_Table('SQL_WORD_Strictpastwardset'         , ['WORD']               , 'strict_pastwards')
SQL_Table('SQL_AllPastmosts'                   , []                     , 'all_pastmosts_set')
SQL_Table('SQL_WORD_ImmediatePastwardset'      , ['WORD']               , 'immediate_pastwards'        , storage={'pickle'})
SQL_Table('SQL_WORD_Equivalents'               , ['WORD']               , 'equivalents'                , storage={'pickle'})
SQL_Table('SQL_WORD_ImmediateFuturewardset'    , ['WORD']               , 'immediate_futurewards'      , storage={'pickle'})
SQL_Table('SQL_WORD_Strictfuturewardset'       , ['WORD']               , 'strict_futurewards')
SQL_Table('SQL_WORD_RealGivenSenses'           , ['WORD']               , 'real_given_senses'          , storage={'pickle'})
SQL_Table('SQL_WORD_Direction_Coveringsenses'  , ['WORD', 'direction']  , 'covering_senses')
SQL_Table('SQL_WORD_Coglang_Langstrictfutures' , ['WORD', 'coglang']    , 'coglang_futures')
SQL_Table('SQL_SENSE_Pastwardsenseposet'       , ['SENSE']              , 'pastward_senseposet')
SQL_Table('SQL_AllPastmostsenses'              , []                     , 'all_pastmost_senses')
SQL_Table('SQL_SENSE_Lang_Futuresenseposet'    , ['SENSE', 'coglang']   , 'futureward_senselangposet')
SQL_Table('SQL_CommonEnglishWords'             , []                     , 'common_english')
SQL_Table('SQL_SENSE_HTMLs'                    , ['SENSE']              , 'HTMLs')
SQL_Table('SQL_Tests'                          , []                     , 'testkind_dict')

def OBJECT_TABLE_TODO(obj, sql_table, single_value=False):
   if single_value or type(obj) in [set, list]:
      generator_source = [(obj, )]
      function = lambda x:[(x,)]
   elif type(obj) == dict:
      generator_source = obj.items()
      function = lambda x,y: [(x,y)]
   return {'generator source':generator_source, 'function':function, 'target tables':[sql_table], 'message': 'writing '}

def Tablename_CACHESelectname(name):
   return name+'_CACHE_Select'

for table in G_SQL_table_registry:
   define_cache = f'''
@CACHIFY
def {Tablename_CACHESelectname(table.name)}(*args):
   return {table.name}.Select(*args)
   '''
   exec(define_cache)


### database setup
def sql_db_activate(db_file):
   print('using Wiktionary_database:', db_file)
   global Global_db_connection 
   global Global_db_cursor 
   Global_db_connection = sqlite3.connect(db_file)
   Global_db_cursor = Global_db_connection.cursor()

WiktionariesDir = "wiktionaries/"
def xmlpath_fullxmlpath(xml_path):
   return RadixRootdir + WiktionariesDir + xml_path

WiktionaryXMLfullPath = xmlpath_fullxmlpath(WiktionaryXMLfile)

if globals().get('G_not_loading_to_xml_redirects', True):
   if 'G_override_wiktionary_database' in globals(): 
      Wiktionary_database = RadixRootdir + WiktionariesDir + G_override_wiktionary_database.replace('/',':')
   else:
      for file in sorted(next(os.walk(RadixRootdir + WiktionariesDir))[2]):
         if file.startswith(WiktionaryXMLfile[:-4]) and file.endswith('.db'):
            Wiktionary_database = RadixRootdir + WiktionariesDir + file
   sql_db_activate(Wiktionary_database)


240908    02:02:06    

TABLES = [
('SQL_SANDBOX'                        , []                     , 'no_thing'),
('SQL_WordtitleRedirect'              , []                     , 'wordtitle_redirect'),
('SQL_Wordtitle_XML'                  , ['wordtitle']          , 'raw_article_xml')            ,
('SQL_WORD_Parsed'                    , ['WORD']               , 'parsed'),
('SQL_WORD_AllMentioningWords'        , ['WORD']               , 'all_mentioning_words'),
('SQL_SENSE_FutureWordlinks'          , ['SENSE']              , 'wordlinks'),
('SQL_SENSE_Direction_Wordrefs'       , ['SENSE', 'direction'] , 'wordrefs'),
('SQL_WORD_Direction_GivenRefWords'   , ['WORD', 'direction']  , 'given_ref_words'),
('SQL_WORD_Strictpastwardset'         , ['WORD']               , 'strict_pastwards'),
('SQL_AllPastmosts'                   , []                     , 'all_pastmosts_set'),
('SQL_WORD_ImmediatePastwardset'      , ['WORD']               , 'immediate_pastwards'        , {'pickle'}),
('SQL_WORD_Equivalents'               , ['WORD']               , 'equivalents'                , {'pickle'}),
('SQL_WORD_ImmediateFuturewardset'    , ['WORD']               , 'immediate_futurewards'      , {'pickle'}),
('SQL_WORD_Strictfuturewardset'       , ['WORD']               , 'strict_futurewards'),
('SQL_WORD_RealGivenSenses'           , ['WORD']               , 'real_given_senses'          , {'pickle'}),
('SQL_WORD_Direction_Coveringsenses'  , ['WORD', 'direction']  , 'covering_senses'),
('SQL_WORD_Coglang_Langstrictfutures' , ['WORD', 'coglang']    , 'coglang_futures'),
('SQL_SENSE_Pastwardsenseposet'       , ['SENSE']              , 'pastward_senseposet'),
('SQL_AllPastmostsenses'              , []                     , 'all_pastmost_senses'),
('SQL_SENSE_Lang_Futuresenseposet'    , ['SENSE', 'coglang']   , 'futureward_senselangposet'),
('SQL_CommonEnglishWords'             , []                     , 'common_english'),
('SQL_SENSE_HTMLs'                    , ['SENSE']              , 'HTMLs'),
('SQL_Tests'                          , []                     , 'testkind_dict'),
]


### database setup
#def sql_db_activate(db_file):
#   print('using Wiktionary_database:', db_file)
#   global Global_db_connection 
#   global Global_db_cursor 
#   Global_db_connection = sqlite3.connect(db_file)
#   Global_db_cursor = Global_db_connection.cursor()



def SQL_Vacuum():
   print('\nvacuuming database...')
   DB.VACUUM()
#   Global_db_connection.execute("VACUUM")

240908    11:14:53    
sense = Sense(Word('pond', 'en'), 1)
Sense_Pastwardsenseposet(sense)

240908    15:29:48    
WiktionariesDir = "wiktionaries/"
def wiktionaryPath_fullPath(xml_path): return RadixRootdir + WiktionariesDir + xml_path
WiktionaryXMLfullPath = wiktionaryPath_fullPath(WiktionaryXMLfile)

if globals().get('G_not_loading_to_xml_redirects', True):
   if 'G_override_wiktionary_database' in globals(): 
      Wiktionary_database = wiktionaryPath_fullPath(G_override_wiktionary_database.replace('/',':'))
   else:
      for file in sorted(next(os.walk(RadixRootdir + WiktionariesDir))[2]):
         if file.startswith(WiktionaryXMLfile[:-4]) and file.endswith('.db'):
            Wiktionary_database = wiktionaryPath_fullPath(file)
   sql_db_activate(Wiktionary_database)



240908    17:34:34    
#GWordtitle_Redirect = (SQL_WordtitleRedirect.OneValue() if globals().get('G_not_loading_to_xml_redirects', True) else {})


if globals().get('G_not_loading_to_xml_redirects', True):
   sql_db_activate(G_Wiktionary_db_file)


G_not_loading_to_xml_redirects = False
exec(open('src/precomputing/abstracted.py').read())
G_not_loading_to_xml_redirects = True


# don't actually use this anywhere apparently?
('SQL_AllPastmosts'                   , []                     , 'all_pastmosts_set'),



240908    20:42:08    
#def current_total_pageins():
#   return psutil_process.memory_info().pageins


   '''
from deepdiff import DeepDiff
from deepdiff.helper import DeepDiffDefaultConfig
from deepdiff.helper import literal_eval_extended

literal_eval_extended.update({
    "Word": str

})

poset0= G_Err_value[0]
poset1= G_Err_value[1]

word = str_Word('[scheur, nl]')

poset0.item_equivalents[word]
poset1.item_equivalents[word]
poset0.domain
poset1.domain

ItemsetDomain_Keyfilter(poset1.item_equivalents, poset1.domain)
   item_equivalents[poset.domain[0]] = {poset.domain[0]}

   '''


240909    00:08:55    

SQL_TABLE_SPECS = [
#('SQL_SANDBOX'                        , []                     , 'no_thing'),
('SQL_WordtitleRedirect'              , []                     , 'wordtitle_redirect'),
('SQL_Wordtitle_XML'                  , ['wordtitle']          , 'raw_article_xml'),
('SQL_WORD_Parsed'                    , ['WORD']               , 'parsed'),
('SQL_WORD_AllMentioningWords'        , ['WORD']               , 'all_mentioning_words'),
('SQL_SENSE_FutureWordlinks'          , ['SENSE']              , 'wordlinks'),
('SQL_SENSE_Direction_Wordrefs'       , ['SENSE', 'direction'] , 'wordrefs'),
('SQL_WORD_Direction_GivenRefWords'   , ['WORD', 'direction']  , 'given_ref_words'),
('SQL_WORD_Strictpastwardset'         , ['WORD']               , 'strict_pastwards'),
('SQL_WORD_ImmediatePastwardset'      , ['WORD']               , 'immediate_pastwards'        , {'pickle'}),
('SQL_WORD_Equivalents'               , ['WORD']               , 'equivalents'                , {'pickle'}),
('SQL_WORD_ImmediateFuturewardset'    , ['WORD']               , 'immediate_futurewards'      , {'pickle'}),
('SQL_WORD_Strictfuturewardset'       , ['WORD']               , 'strict_futurewards'),
('SQL_WORD_RealGivenSenses'           , ['WORD']               , 'real_given_senses'          , {'pickle'}),
('SQL_WORD_Direction_Coveringsenses'  , ['WORD', 'direction']  , 'covering_senses'),
('SQL_WORD_Coglang_Langstrictfutures' , ['WORD', 'coglang']    , 'coglang_futures'),
('SQL_SENSE_Pastwardsenseposet'       , ['SENSE']              , 'pastward_senseposet'),
('SQL_AllPastmostsenses'              , []                     , 'all_pastmost_senses'),
('SQL_SENSE_Lang_Futuresenseposet'    , ['SENSE', 'coglang']   , 'futureward_senselangposet'),
('SQL_CommonEnglishWords'             , []                     , 'common_english'),
('SQL_SENSE_HTMLs'                    , ['SENSE']              , 'HTMLs'),
('SQL_Tests'                          , []                     , 'testkind_dict'),
]


240909    14:36:28    

@total_ordering
@dataclass(frozen=True)
class Gram:
   text: str
   lang: str
   def __repr__(self): return f'[{self.text}, {self.lang}]'
   def __lt__(self, gram2): 
      return (Lang_age(self.lang), self.lang, gram2.text) 
           > (Lang_age(gram2.lang), gram2.lang, self.text)
#      if self.lang == gram2.lang : return self.text < gram2.text
#      else                       : return LangLang_isPastward(self.lang, gram2.lang)

def LangLang_isPastward(lang1, lang2): return (Lang_age(lang1), lang1) > (Lang_age(lang2), lang2)

240909    21:16:09    

def make_regex(constructor, name, arg, kind=None):
   if type(constructor) == str:
      connector = {'OR': any, 'AND': all, 'NOT': lambda x: not any(x)}[constructor]
      return make_regex(Primitive, name, lambda tok: connector(regex.test(tok) for regex in arg), kind=kind)
   elif kind is None : globals()[name] = constructor(name, arg)
   else              : globals()[name] = constructor(name, lambda tok : tok['kind'] == kind and arg(tok['content']))
   return globals()[name] 



def make_regex(constructor, name, arg, kind=None):
   match constructor:
      case str():
         connector = {'OR': any, 'AND': all, 'NOT': lambda x: not any(x)}[constructor]
         return make_regex(Primitive, name, lambda tok: connector(regex.test(tok) for regex in arg), kind=kind)
#      Primitive(name, lambda tok: connector(regex.test(tok) for regex in arg))

      case Primitive: globals()[name] = constructor(name, lambda tok : tok['kind'] == kind and arg(tok['content']))
      case Pattern: globals()[name] = constructor(name, arg)

   return globals()[name] 



def make_regex(constructor, name, arg, kind=None):
   match constructor:
      case 'OR' |'AND'|'NOT':
         connector = {'OR': any, 'AND': all, 'NOT': lambda x: not any(x)}[constructor]
         return make_regex('Primitive', name, lambda tok: connector(regex.test(tok) for regex in arg), kind=kind)
#      Primitive(name, lambda tok: connector(regex.test(tok) for regex in arg))

      case 'Primitive': globals()[name] = Primitive(name, lambda tok : tok['kind'] == kind and arg(tok['content']))
      case 'Pattern': globals()[name] = Pattern(name, arg)

   return globals()[name] 


def make_regex(constructor, name, arg, kind=None):
   if type(constructor) == str:
      connector = {'OR': any, 'AND': all, 'NOT': lambda x: not any(x)}[constructor]
      globals()[name] = Primitive(name, lambda tok: connector(regex.test(tok) for regex in arg))
#      return make_regex(Primitive, name, lambda tok: connector(regex.test(tok) for regex in arg), kind=kind)
   elif kind is None : globals()[name] = constructor(name, arg)
   else              : globals()[name] = constructor(name, lambda tok : tok['kind'] == kind and arg(tok['content']))
#   return globals()[name] 



def make_regex(code, name, arg, kind=None):
   match code:
      case 'OR' |'AND'|'NOT':
         connector = {'OR': any, 'AND': all, 'NOT': lambda x: not any(x)}[code]
         constructor, arg = Primitive, lambda tok: connector(regex.test(tok) for regex in arg)
      case 'Primitive':
         constructor, arg = Primitive, lambda tok : tok['kind'] == kind and arg(tok['content'])
      case 'Pattern': 
         constructor, arg = Pattern, arg
   globals()[name] = constructor(name, arg)


def make_regex(code, name, arg, kind=None):
   match code:
      case 'OR' |'AND'|'NOT':
         globals()[name] = Primitive(name, lambda tok: connector(regex.test(tok) for regex in arg))
      case 'Primitive': globals()[name] = Primitive(name, lambda tok : tok['kind'] == kind and arg(tok['content']))
      case 'Pattern': globals()[name] = Pattern(name, arg)


240910    00:13:01    



def Element_RecursiveTokens(element):
   if isinstance(element, Structure) : return element.token
   else                              : return list(map(Element_RecursiveTokens, element.elements))

#   return element.token if not isinstance(element, Structure) else list(map(Element_RecursiveTokens, element.elements))


def make_regex(code, name, arg, kind=None):
   match code:
      case 'OR'|'AND'|'NOT': pattern, pred = Primitive , lambda tok: G_code_fun[code](regex.test(tok) for regex in arg)
      case 'Primitive'     : pattern, pred = Primitive , lambda tok: tok['kind'] == kind and arg(tok['content'])
      case 'Pattern'       : pattern, pred = Pattern   , arg
   globals()[name] = pattern(name, pred)



240910    00:43:03    


   import subprocess
   command = """python src/precomputing/run_all.py """
   result = subprocess.run(command, shell=True, )
   command = f"""python src/radix.py previous {previous_db}"""
   result = subprocess.run(command, shell=True, )

def __X__FullTest():
   previous_db = '/Users/tbt/radix/wiktionaries/40k-initial-24.09.09-02_04_25.db'
   exec(RR('precomputing/run_all'))
   exec(RR('radix'))

   command = f"""python src/radix.py previous {previous_db}"""


def PatternTokens_Matches(pattern, tokens):
   automaton = Automata(pattern)
   tokens=[dict(token,index=i) for i,token in enumerate(tokens)]
   [automaton.feedToken(token) for token in tokens]
   return [[Element_RecursiveTokens(element)
         for element in final_state.connect(automaton.pattern.name)[1].elements]
         for final_state in list(automaton.finalStates)]


      if isinstance(regex_object, Primitive):
         primitive_test = regex_object.test
         self.matches = lambda tokens: [[x] for x in filter(primitive_test, tokens)]
      else:
         self.matches = lambda tokens: PatternTokens_Matches(regex_object, tokens)


      if kind == 'ref' and type(extractor) == int:
         self.raw_reflinks = lambda match, word: [match[extractor]]
      elif type(extractor) == tuple and extractor[1] == 'pass_word':
         self.raw_reflinks = lambda match, word: extractor[0](match, word)
      else:
         self.raw_reflinks = lambda match, word: extractor(match)





      if type(extractor) == int and kind == 'ref':
         self.raw_reflinks = lambda match, word: [match[extractor]]
      elif type(extractor) == tuple and extractor[1] == 'pass_word':
         self.raw_reflinks = lambda match, word: extractor[0](match, word)
      else:
         self.raw_reflinks = lambda match, word: extractor(match)



      match type(extractor):
         print(extractor)
         case int() if kind == 'ref':
            self.raw_reflinks = lambda match, word: [match[extractor]]
         case tuple() if extractor[1] == 'pass_word':
            self.raw_reflinks = lambda match, word: extractor[0](match, word)
         case _: self.raw_reflinks = lambda match, word: extractor(match)




      if (type(extractor) == int) and (kind == 'ref'):
         self.raw_reflinks = lambda match, word: [match[extractor]]
      elif (type(extractor) == tuple) and (extractor[1] == 'pass_word'):
         self.raw_reflinks = lambda match, word: extractor[0](match, word)
      else:
         self.raw_reflinks = lambda match, word: extractor(match)


      match type(extractor):
         case int() if (kind == 'ref'):
            self.raw_reflinks = lambda match, word: [match[extractor]]
         case tuple() if (extractor[1] == 'pass_word'):
            self.raw_reflinks = lambda match, word: extractor[0](match, word)
         case _: self.raw_reflinks = lambda match, word: extractor(match)


exec(open('src/radix.py').read())

for i,x in enumerate(G_thingy):
   print('')
   print(x)
   print(G_thangy[i])

      if (type(extractor) == int) and (kind == 'ref'):
         self.raw_reflinks = lambda match, word: [match[extractor]]
         G_thingy.append((extractor, kind, 1))
      elif (type(extractor) == tuple) and (extractor[1] == 'pass_word'):
         self.raw_reflinks = lambda match, word: extractor[0](match, word)
         G_thingy.append((extractor, kind, 2))
      else:
         self.raw_reflinks = lambda match, word: extractor(match)
         G_thingy.append((extractor, kind, 3))

      match type(extractor):
         case int() if (kind == 'ref'):
            self.raw_reflinks = lambda match, word: [match[extractor]]
            G_thangy.append((extractor, kind, 1))
         case tuple() if (extractor[1] == 'pass_word'):
            self.raw_reflinks = lambda match, word: extractor[0](match, word)
            G_thangy.append((extractor, kind, 2))
         case _: 
            self.raw_reflinks = lambda match, word: extractor(match)
            G_thangy.append((extractor, kind, 3))



   def reflinks(self, match, word):
      gram_token = {'kind': 'reference', 'content': {'kind':'INFERRED', 'lang': word.gram.lang, 'gramtexts':[word.gram.text]}}
      extracts = self.raw_reflinks(match, word)
      results = extracts if self.kind != 'ref' else [(gram_token, x) for x in extracts]
      results = [(tok1['content'], tok2['content']) for tok1, tok2 in results]
      if self.direction == 'pastward': #regularize everything to be futureward
         results = [(ref2, ref1) for ref1, ref2 in results]
      return results

#DB._GRAM_Direction_GivenRefGrams,
#DB._GRAM_Direction_Coveringwords,
#DB._GRAM_Coglang_Langstrictfutures,



def Word_report(word):
   print('')
   print('word', word)
   Gram_report(word.gram)


'''
Word_report(word)
'''


def Word_report(word):
   print('')
   print('word', word)
   Gram_report(word.gram)


def Gram_report(gram):
   print('')
   print('gram', gram)
   Table_arg_report(DB._Gramtitle_XML, gram.text)
   for table in  [ DB._GRAM_Parsed, DB._GRAM_AllMentioningGrams, DB._GRAM_Strictpastwardset, DB._GRAM_ImmediatePastwardset, DB._GRAM_Equivalents, DB._GRAM_ImmediateFuturewardset, DB._GRAM_Strictfuturewardset,  DB._GRAM_RealGivenWords,]:
      Table_arg_report(table, gram)

def Table_arg_report(table, arg):
   print('')
   print(table.name)
   print( table.Select(arg))


# blech. 
def Gram_GivensOrPlaceholder(gram: Gram):
   givens = Gram_AllRealGivenWords(gram)
   return givens if len(givens)>0 else {Word(gram, -1)}


240910    18:01:12    

#### inspection

# external
def Json_Inspection(gram_json):
   gram = Json_Gram(gram_json)


def Json_Inspection(gram_json):
#   {'gramtext': 'dictionarius', 'lang': 'la', 'words_included': '[-1]', 'words_excluded': '[]', 'include_jump_down': 'False', 'click_url': 'https://en.wiktionary.org/wiki/dictionarius#Latin', 'wiktionary_url': 'https://en.wiktionary.org/wiki/dictionarius#Latin', 'radix_url': '/word/lang:la&wordnum:-1/dictionarius', 'parent_gramtext': 'dictionarium', 'parent_lang': 'la', 'parent_direction': 'pastward'}
   gram = Json_Gram(gram_json)

   result = Aggregator()
   result << Paragraph('gram: ' + Gram_Element(gram, info={"kind":"inspection"}))
   result << Json_WordsInspection(gram_json)

   for word_num in sorted(JsonClusion_Wordnums(gram_json, 'included')):
      result << Paragraph(f'word {word_num}:' + ('' if word_num != -1 else ' no wiktionary article'))
      result << Paragraph(Word_Wiktionary(Word(gram, word_num)))
      result << Paragraph(Word_FullDefinitionHTML(Word(gram, word_num)))

   if 'parent_gramtext' in gram_json:
      parent_gram = Json_Gram(gram_json, prefix='parent_')
      direction = gram_json['parent_direction']
      if direction == 'both':
         result << GramGramDirection_InspectionHTML(gram, parent_gram, 'pastward')
         result << GramGramDirection_InspectionHTML(gram, parent_gram, 'futureward')
      else:
         result << GramGramDirection_InspectionHTML(gram, parent_gram, direction)
   return result()

def Json_WordsInspection(json):
      words = JsonClusion_Wordnums(json, clusion)

def ClusionWordnums_WordsInspection(clusion_wordnums):
   result = Aggregator() 
   for clusion in ['included', 'excluded']:
      words = JsonClusion_Wordnums(json, clusion)
#      words = JsonClusion_Wordnums(json, clusion)
      if len(words) > 0:
         result << Element('div', clusion + ' words: ' + ', '.join(str(x) for x in words), style='white-space: nowrap')
   return result()

def JsonClusion_Wordnums(json, clusion):
   return sorted(ujson.loads(json.get('words_' + clusion, '[]')))

def Json_Gram(json, prefix=''):
   return Gram(json[prefix + 'gramtext'], json[prefix + 'lang'])

# external
def Json_Word(json):
   return Word(Json_Gram(json), json['word'])


240910    22:37:17    
def Refs_Grams(refs): return {gram for ref in refs for gram in Ref_Grams(ref)}

def Inspection_HTMLrow(inspection):
   source_word, from_gram, to_gram = inspection
   direction = 'futureward'
   G_add_popups = False  
   result = (Word_Element(source_word, info={'kind':'inspection'}), Gram_Element(from_gram, info={'kind':'inspection'}), 
             Color(Gray(150), '->'), Gram_Element(to_gram, info={'kind':'inspection'})) 
   G_add_popups = True 
   return result


240910    23:12:12    

# move elswhere. rearrange printing in general.
# redo using Div?
from collections import deque
def _div_list_wrapped(div, xs): xs.appendleft(f'<{div}>').appendright(f'</{div}>')

   html = deque()

def Tuples_HTMLtable(data):
   html = Aggregator(['<table>'])
   for row in data:
      html << '<tr>'
      [html << f'<td>{col}</td>' for col in row]
      html << '</tr>'
   html << '</table>'
   return html()

# move elswhere. rearrange printing in general.
def _div_xs_wrapped(div, xs): return [f'<{div}>', xs, f'</{div}>']

def Tuples_HTMLtable(data):
   html = Aggregator(['<table>'])
   for row in data:
      html << '<tr>'
      [html << f'<td>{col}</td>' for col in row]
      html << '</tr>'
   html << '</table>'
   return html()

_div_xs_wrapped('tr'[_div_xs_wrapped('td', col) for col in row]


240910    23:52:56    

#TODO use f strings
class SQL_Table:
   def __init__(self, db, name, key_columns, value_column, storage={'pickle', 'compress'}):
      if name[0] != '_': raise Exception(f'tried to make table named {name} but tables should start with "_".')
      self.db, self.name = db, name
      globals()[self.name] = self
      rest_of_columns = key_columns[1:] + [value_column]
      match key_columns[:1]:
         case ['GRAM']:
            self.columns = ['gramtext', 'lang'] + rest_of_columns 
            self.Plainkey_SQLkey = lambda key_cols: [*key_cols[0]] + list(key_cols[1:])
            self.SQLkey_Plainkey = lambda key_cols: [Gram(*key_cols[:2])] + list(key_cols[2:])
         case ['WORD']:
            self.columns = ['gramtext', 'lang', 'wordnum'] + rest_of_columns 
            self.Plainkey_SQLkey = lambda key_cols: [*key_cols[0]] + list(key_cols[1:])
            self.SQLkey_Plainkey = lambda key_cols: [Word(Gram(*key_cols[:2]), key_cols[2])] + list(key_cols[3:])
         case _:
            self.columns = key_columns[:1] + rest_of_columns 
            self.Plainkey_SQLkey = IDENTITY
            self.SQLkey_Plainkey = IDENTITY 


240911    00:25:12    

      match key_columns[:1]:
         case ['GRAM']:
            cols_plainSQL_SQLplain = (['gramtext', 'lang'], 
                     lambda key_cols: [*key_cols[0]] + list(key_cols[1:]),
                     lambda key_cols: [Gram(*key_cols[:2])] + list(key_cols[2:]))
         case ['WORD']:
            cols_plainSQL_SQLplain = (['gramtext', 'lang', 'wordnum'],
                     lambda key_cols: [*key_cols[0]] + list(key_cols[1:]),
                     lambda key_cols: [Word(*key_cols[:3])] + list(key_cols[3:]))
         case _:
            cols_plainSQL_SQLplain = (key_columns[:1], IDENTITY, IDENTITY)

      self.columns, self.Plainkey_SQLkey, self.SQLkey_Plainkey = cols_plainSQL_SQLplain
      self.columns.extend(key_columns[1:] + [value_column])



240911    00:58:30    

      match (key0 := key_columns[:1]):
         case ['GRAM'] | ['WORD']:
            if key0 == ['GRAM'] : constructor = Gram; initial_columns = ['gramtext', 'lang']
            else                : constructor = Gram; initial_columns = ['gramtext', 'lang', 'wordnum']

            init_col_count = len(initial_columns)
            cols_plainSQL_SQLplain = (initial_columns,
                  lambda key_cols: [*key_cols[0]] + list(key_cols[1:]),
                  lambda key_cols: [constructor(*key_cols[:init_col_count])] + list(key_cols[init_col_count:]))
         case _:
            cols_plainSQL_SQLplain = (key_columns[:1], IDENTITY, IDENTITY)



      match (key0 := key_columns[:1]):
         case ['GRAM'] | ['WORD']:
            if key0 == ['GRAM'] : constructor = Gram; initial_columns = ['gramtext', 'lang']
            else                : constructor = Word; initial_columns = ['gramtext', 'lang', 'wordnum']

            init_col_count = len(initial_columns)
            cols_plainSQL_SQLplain = (initial_columns,
                           lambda key_cols: [*key_cols[0]] + list(key_cols[1:]),
                           lambda key_cols: [constructor(*key_cols[:init_col_count])] + list(key_cols[init_col_count:]))
         case _:
            cols_plainSQL_SQLplain = (key_columns[:1], IDENTITY, IDENTITY)





len(init_cols)

240911    01:48:16    


   def InsertRow(self, row):
      # could track number, and then commit when hit 10000....
      row = list(self.Plainkey_SQLkey(row[:-1])) + [self.Store(row[-1])]
#      self.db.cursor.execute(f"INSERT INTO {self.name} \
#                               ({', '.join(self.columns)}) VALUES ({', '.join(['?']*len(self.columns))})", row)
      self.db.cursor.execute('INSERT INTO ' + self.name 
                + ' (' + ', '.join(self.columns) + ') VALUES (' + ', '.join(['?']*len(self.columns)) + ')', row)


240911    02:06:56    


   #TODO: shrink https://docs.python.org/3/library/sqlite3.html
   def RowIterator(self):
      iterator_cursor = self.db.new_cursor()
      iterator_cursor.execute(f'SELECT * FROM {self.name}')
      return iterator_cursor


   def Count(self):
      print('\ncounting', self.name)
      return self.db.cursor.execute(f'SELECT COUNT(*) FROM {self.name}').fetchone()[0]


def OBJECT_TABLE_TODO(obj, sql_table, single_value=False):
   if single_value or type(obj) in [set, list]:
      generator_source = [(obj, )]
      function = lambda x:[(x,)]
   elif type(obj) == dict:
      generator_source = obj.items()
      function = lambda x,y: [(x,y)]
   return {'generator source':generator_source, 'function':function, 'target tables':[sql_table], 'message': 'writing '}


def sql_db_activate(db_file):
   print('\nactivating database using Wiktionary db file:', db_file)
   SQL_DB('DB', db_file)

def WordCoglangs_Cognates(word, Coglangs):
#   word_poset = logged_WordCoglangs_Wordposet(word, Coglangs)
   return {'trunk': Word_Pastwardwordposet(word), 'tree': WordCoglangs_Wordposet(word, Coglangs)}


240911    02:53:13    

GLOBALCACHES = {}
def CACHIFY(function, name=None):
   function_name = function.__name__ if name is None else name
   global GLOBALCACHES
   GLOBALCACHES[function_name] = {}
   @functools.wraps(function)
   def cachified(*args):
      global GLOBALCACHES
      if args not in GLOBALCACHES[function_name]:
#         mprint('cache miss', function_name, args)
         GLOBALCACHES[function_name][args] = function(*args)
      return GLOBALCACHES[function_name][args] 
   return cachified


def WIPE_GLOBALCACHES(keys=None):
   global GLOBALCACHES
   if keys==None:
      GLOBALCACHES = {k:{} for k in GLOBALCACHES.keys()}
   else:
      for k in keys:
         GLOBALCACHES[k] = {} 
      

240911    03:00:07    

def Time(thunk, name, to_print=True):
   if to_print:
      print('>>>>>>  starting thunk', name)
   start = time.time()
   result = thunk()
   end = time.time()
   if to_print:
      print('>>>>>>  done with thunk', name)
      print(end-start)
      return result
   else:
      return (result, end-start)



def big_link_extractor(match):
   links = []
   for i, group in enumerate(groups := big_extractor(match)): 
      if i<len(groups)-1: 
         links.extend((from_ref, to_ref) for from_ref in group for to_ref in groups[i+1])
   return links

def big_ref_extractor(match):
   for group in big_extractor(match)): yield from group


def terminal_rgb_col(r, g, b):
   return "\033[38;2;{};{};{}m".format(r, g, b)

RED = (255, 80, 80) 
def TerminalColor(rgb, text):
   return terminal_rgb_col(*rgb) + text + '\033[0m'

def fun_nested_map(fun, xs):
   if type(xs) == list:
      return [fun_nested_map(fun, x) for x in xs]
   else:
      return fun(xs)

KIND = 'ref'
DIRECTION = 'pastward'

#def test_extractor(match):
#   return match[0]
test_extractor = 0

#make_regex(Primitive, 'Pr_with', lambda content: '(' in content and ')' not in content and not content.endswith('('), kind='text')
#make_regex('AND', 'Pr_nonemptywith', [Pr_nonempty, Pr_with])
#make_regex(Pattern, 'P_test', [Pr_nonemptywith, Pr_link, Pr_nonempty])
#make_regex(Pattern, 'P_test', [Pr_with, Pr_link, ])
#SECTION = 'Derived terms' 
#SECTION = 'Descendants' 


#make_regex(Primitive, 'P_test', lambda content: content['kind'] in {'columns'}, kind='reference')
#SECTION = 'Descendants' # essentially never happens apparently?
#SECTION = 'Derived terms' # basically always good? unsure. 

#make_regex(Primitive, 'P_test', lambda content: 'wordtexts' in content and any('/' in wordtext for wordtext in content['wordtexts']), kind='reference')
#make_regex(Primitive, 'P_test', lambda content: 'wordtexts' in content, kind='reference')
#SECTION = ''

make_regex(Primitive, 'P_test', lambda content: content['kind'] in {'confix'}, kind='reference')
SECTION = 'Etymology'


test_inf_obj = Inference_object(KIND, P_test, SECTION, test_extractor, DIRECTION)
G_inference_objects = [test_inf_obj]

def WordParsed_Inferences(word, parsed):
   return [inference for sensenum, sensedict in parsed.items() for inference in SenseDict_Inferences(Sense(word, sensenum), sensedict)]

def FUN(word, parsed):
   ref_results = WordParsed_Inferences(word, parsed)


   for ref_result in ref_results:
      header_text = Tokens_Text(ref_result['header tokens'])
      match_text = Tokens_Text(fully_flatten(ref_result['match']))
      assert match_text in header_text
      text = header_text.replace(match_text, TerminalColor((255,90,90), match_text))
      if len(ref_result['reflinks']) >0:# and not any(PIE_langcode_poset.strict_ulteriors[next_lang] == set() for next_lang in PIE_langcode_poset.immediate_ulteriors[word.lang]):
         print('in:', word)
#         print(fun_nested_map(lambda token: token['content'] if token['kind'] == 'text' else token['content']['text'], ref_result['match']))
         print(text)
#         print(ref_result['reflinks'])
#         print([w for w in ref_result['extractions']['content']['wordtexts'] if '/' in w])
   return []

to_do_1 = {'generator source':SQL_WORD_Parsed, 'function':FUN, 'target tables':[SQL_SANDBOX], 'message': '', 'limit':9000000, 'printing': False}

CALL([to_do_1, ], to_profile=False)


240912    20:36:32    

def Tests_handle(actions):
   match actions:
      case ['test']              : Run_sql_tests()
      case ['reset']             : ResetTests(ComputeTests())
      case ['previous']          : previousDB_compare(XMLfile_MostRecentDBfile(WiktionaryXMLfile, lookback=1))
      case ['previous', db_file] : previousDB_compare(db_file)
      case ['fulltest']          : __X__FullTest()
      case ['fullreset']         : __X__FullReset()

#previousDB_compare(XMLfile_MostRecentDBfile(WiktionaryXMLfile, lookback=1), given_tables=['DB._Tests'])


def Path_Stem(path): return '.'.join(path.split('/')[-1].split('.')[:-1])

def XMLpath_Stem(xml_path): 
   if xml_path.endswith('.xml') : return Path_Stem(xml_path)
   else                         : raise Exception(f'bad xml path {xml_path}')

def XMLpath_wiktionariesDir(xml_path): return WiktionariesFullDir + XMLpath_Stem(xml_path) + '/'

def Path_wiktionariesDir(xml_path): return WiktionariesFullDir + Path_Stem(xml_path) + '/'
def wiktionaryPath_fullPath(xml_path): return Path_wiktionariesDir(xml_path) + xml_path

def XMLfile_MostRecentDBfile(xml_file, lookback=0):
   index = 0
   for file in reversed(sorted(next(os.walk(XMLpath_wiktionariesDir(xml_file)))[2])):
      if file.startswith(XMLpath_Stem(xml_file)) and file.endswith('.db'):
         if index >= lookback : return Path_wiktionariesDir(xml_file) + file 
         else                 : index += 1

G_Wiktionary_db_file    = XMLfile_MostRecentDBfile(WiktionaryXMLfile)
G_WiktionaryXMLfullPath = wiktionaryPath_fullPath(WiktionaryXMLfile)
G_DB_Checkpoint_filename = Path_wiktionariesDir(WiktionaryXMLfile) + 'Checkpoint_' + XMLpath_Stem(WiktionaryXMLfile) + '.db'

def __X__NewWiktionaryDBfile(): 
   return XMLpath_wiktionariesDir(WiktionaryXMLfile) + XMLpath_Stem(WiktionaryXMLfile) + f'-{GetTimestamp()}.db' 

240913    05:21:55    

240913    03:47:35    
uh oh. 
ed _WORD_PastwardPoset row 72460 of 72486 or 100.0% done.
cached _WORD_PastwardPoset row 72470 of 72486 or 100.0% done.
cached _WORD_PastwardPoset row 72480 of 72486 or 100.0% done.
done adding all rows to cache for table _WORD_PastwardPoset
loaded tables: _WORD_PastwardPoset _WORD_PastwardPoset
equal:  False

printing obj_diff of the two results:
('change', [([handy, en]_1,)])
Traceback (most recent call last):
  File "/Users/tbt/radix/src/radix.py", line 129, in <module>
    if len(sys.argv) > 1: Tests_handle(sys.argv[1:])
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 66, in Tests_handle
  File "<string>", line 137, in previousDB_c

at some point make poset and wordposet suclasse of a superclas, or something along those lines

ideally would be able to do fulltest-like thing but starting later than begin...

cached _GRAM_Parsed row 13390 of 13400 or 99.9% done.
done adding all rows to cache for table _GRAM_Parsed
loaded tables: _GRAM_Parsed _GRAM_Parsed
equal:  False

printing obj_diff of the two results:
('change', [([wiki, xto],), 1, 'Etymology', 15, 'content', 'gramtexts', 0])

''
''

failed table _GRAM_Parsed
 None
Traceback (most recent call last):

240913    04:32:34    


240913    02:06:38    
xml = DB._Gramtitle_XML.Get('necromantia') 
parsed_article_dict = XML_Dict(xml)
for i in range(len(a)):
   if a[i] != b[i]:
      print(a[i], end='')

div2='}}'
for x in k.split('{{'):
   old = x.split(div2)[-1]
   new = x.rsplit(div2, 1)[-1]
   if old != new:
      print('')
      print(x)
      print(old)
      print(new)

a=''
b=''
for i in range(len(a)):
   print(i, a[i], b[i], a[i] == b[i])


ed _WORD_PastwardPoset row 72460 of 72486 or 100.0% done.
cached _WORD_PastwardPoset row 72470 of 72486 or 100.0% done.
cached _WORD_PastwardPoset row 72480 of 72486 or 100.0% done.
done adding all rows to cache for table _WORD_PastwardPoset
loaded tables: _WORD_PastwardPoset _WORD_PastwardPoset
equal:  False

printing obj_diff of the two results:
('change', [([handy, en]_1,)])
Traceback (most recent call last):
  File "/Users/tbt/radix/src/radix.py", line 129, in <module>
    if len(sys.argv) > 1: Tests_handle(sys.argv[1:])
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 66, in Tests_handle
  File "<string>", line 137, in previousDB_c

at some point make poset and wordposet suclasse of a superclas, or something along those lines

ideally would be able to do fulltest-like thing but starting later than begin...

cached _GRAM_Parsed row 13390 of 13400 or 99.9% done.
done adding all rows to cache for table _GRAM_Parsed
loaded tables: _GRAM_Parsed _GRAM_Parsed
equal:  False

printing obj_diff of the two results:
('change', [([wiki, xto],), 1, 'Etymology', 15, 'content', 'gramtexts', 0])

''
''

failed table _GRAM_Parsed
 None
Traceback (most recent call last):
gram = str_Gram('[wiki, xto]')
Gram_Parsed(gram)[1]['Etymology'][15]

bro what. 
>>> Gram_Parsed(gram)[1]['Etymology'][15]
{'kind': 'template', 'content': {'lang': 'os', 'gramtexts': [''], 'kind': 'cognate', 'text': '{{cog|os|}}'}}

the  (noncyrillic) changed to  (cyrillic)??
original is ... actually cyrillic version??
something to do with lua correction???


ne getting all rows for table _WORD_PastwardPoset
adding all rows to cache for table _WORD_PastwardPoset
done adding all rows to cache for table _WORD_PastwardPoset
loaded tables: _WORD_PastwardPoset _WORD_PastwardPoset
equal:  False

printing obj_diff of the two results:
('change', [([handy, en]_4,)])
Traceback (most recent call last):
  File "/Users/tbt/radix/src/radix.py", line 129, in <module>
    if len(sys.argv) > 1: Tests_handle(sys.argv[1:])
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 66, in Tests_handle
  File "<string>", line 139, in previousDB_compare
  File "<string>", line 153, in obj_diff
  File "<string>", line 92, in test_strings_compare
  File "<string>", line 90, in __repr__
TypeError: sequence item 0: expected str instance, Gram found
Traceback (most recent call last):
  File "/Users/tbt/radix/src/radix.py", line 129, in <module>
    if len(sys.argv) > 1: Tests_handle(sys.argv[1:])
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 67, in Tests_handle
  File "<string>", line 162, in __X__FullTest
  File "/Users/tbt/.pyenv/versions/3.11.0/lib/python3.11/subprocess.py", line 569, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: C

240913    05:08:41    

loaded tables: _WORD_PastwardPoset and _WORD_PastwardPoset
truth of the two tables _WORD_PastwardPoset and _WORD_PastwardPoset being equal: False



FAILED table _WORD_PastwardPoset


printing obj_diff of the two results:
('change', [([handy, en]_4,)])
printing difference of the two strings:
found no difference. start: [handy, en]] ...
('change', [([handy, en]_1,)])
printing difference of the two strings:
found no difference. start: [handy, en], [*handugaz, gem-pro], [hendig, ang], [handy, enm], [hondi, enm], [hand, en], [hand, enm], [hond, enm], [hand, ang], [*handu, gmw-pro], [* ...
('change', [([handy, en]_2,)])
printing difference of the two strings:
found no difference. start: [handy, en], [hand, en], [hand, enm], [hond, enm], [hand, ang], [*handu, gmw-pro], [*handuz, gem-pro]] ...
('change', [([handy, en]_3,)])
printing difference of the two strings:
found no difference. start: [handy, en]] ...


